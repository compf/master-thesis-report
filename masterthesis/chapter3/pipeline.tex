

\section{Pipeline}\label{sec:pipeline}
In order to find and refactor data clumps automatically, a particular sequence of steps \textbf{(pipeline)} has to be respected. Most steps of this sequence must be in a specific order because they rely on information extracted in a previous step. 

This section outlines the structure of the pipeline. Section \ref{sec:pipeline_structure} gives an overview of how the pipeline is implemented. 

Section \ref{sec:pipeline_exec_order} discusses an alternative but similar approach for the pipeline which is relevant if multiple data clumps are considered in the pipeline. 

Finally, section \ref{sec:pipeline_steps} describes each important pipeline steps and what considerations should be taken into account when developing tools for the respective pipeline step. 


\subsection{Structure of the pipeline}\label{sec:pipeline_structure}
Each step of the pipeline is performed by a  \textbf{handler}. A handler might handle one or more steps.  Each handler has information about what steps of the pipeline it supports and  a handler can be registered to a pipeline step if it supports that particular step.  

Since a service-based approach is used, a handler can be seen as a gateway to a different program or service that performs the specific functionality.  The handler and the respective service are tightly coupled. While a handler could deal with multiple services, it is not required by the design, and requires special care. Nevertheless, it recommendable to write the handler as abstract as possible.
\begin{figure}[ht!]
    \centering
    \includesvg[inkscapelatex=false,scale=0.9]{figures/chapter3/solver_gateway_service.svg}
    \caption{Visualization of communication to services for one pipeline step}
    \label{fig:solver_gateway_service_overview}
\end{figure}

Figure \ref{fig:solver_gateway_service_overview} illustrates how the service-based approach works for one step:

\begin{enumerate}
    \item The program developed in this master thesis \textbf{(main program)} executes one step (e.~g. finding a suitable class name)
    \item The handler registered to that step is used. It is provided with the current context containing the results from previous steps.
    \item The handler starts or employs a specific service. This service might be dependent on the current step or might be constant. The handler knows how to start or use the service and sends all relevant information to the service (e.~g. via files, network streams etc.). 
    \item The service processes the requests submitted by the handler. For instance, ChatGPT can suggest a suitable class name. The service might be outside of the control of the developer. For example, it might be programmed in a different programming language or the source code might not be available. At some point it will create a response and send it back to the handler. 
    \item The handler reads the response and processes it. It then creates a new context based on the previous context and the response from the service. 
    \item The new context is returned to the program and can be used for successive steps (e.~g. the suggested name can be used for creating a respective class) 
\end{enumerate}



A handler can also check whether it can be run in the given situation. For instance, a code obtaining handler can determine whether the project location does in fact exist or a validation handler for \textit{Gradle} can check whether the project is indeed \textit{Gradle}-based.

The program, therefore, acts as an orchestrator. It starts every service and controls them. The steps and services are temporally coupled. A step cannot be started unless all previous mandatory steps have completed. In most cases, if a step was not successful, further execution of the pipeline makes no sense, so it can be aborted. 

An alternative approach is called choreography. Here each step would call the next step without using the central main program. 

Both approaches have their advantages. In the choreography architecture, there is less coupling since a main program does not need to know which handler is associated with which step. Instead, each handler need to know the next (and only the next) step. This can become challenging if steps are optional because the gateway does not know what service to call next. Additionally, debugging the process is more challenging as there is no central point that coordinates the pipeline.

On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each handler is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but are accessed using the internet or the local network, which causes more overhead. \cite{orchestration_choreography}


\subsection{Pipeline execution order}\label{sec:pipeline_exec_order}

Another issue to analyze is the pipeline execution order. This should not be confused with the pipeline step execution order. The latter is simply the order of the different steps (e.~g. code obtaining before data clump detection). 

The former order deals with how the steps are executed with respect to each detected data clump. Two approaches are considered here:

In the first approach, each data clump is  further processed immediately after detection. This means after a data clump has been detected, it is immediately given a suggested name, a class is extracted if necessary, and the refactoring is executed. In the following, this approach will be referred to as \textit{one-data-clump approach}.

In the second approach, all data clumps are detected first. Then all, those data clumps are assigned a suitable name, and classes are extracted if necessary. At the end, each data clump is refactored. In the following, this approach will be referred to as \textit{one-step approach}.

As a result, in the \textit{one-data-clump approach}, a data clump is processed by one step of the pipeline and then by the next while in the \textit{one-step approach} the first step has to process all data clumps first before the data clumps can be processed by the succeeding steps.

There are arguments for and against both approaches. However, designing a system that supports both approaches is challenging and out of scope for this master thesis, so only one approach is used. 

The \textit{one-data-clump approach} leads to quicker intermediate results so that if the tool to be developed is stopped during execution, it is more likely that some data clumps are fixed so that some progress is being made.

Additionally, this approach is better for load balancing. Since many services might be used for refactoring data clumps and one data clump is processed at a time, each service gets a longer break before being used again. This is  especially important for services like ChatGPT, where many requests in a short time period can lead to outages or huge costs. 

On the other hand, the \textit{one-step approach} is easier to implement using the tools already existing. For instance, the \textit{DataClumpDoctor} detects all data clumps at a time and therefore is more compatible with the \textit{one-step approach}.

Moreover, configuration mistakes can be fixed more easily because in this approach, the software project will not be modified for some time (i.~e. data clumps will be detected or suitable names will be found). As a result, users of the tool are more likely to safely stop the tool if they discover a configuration mistakes. This might often occur in the very first moments after starting the tool.

Additionally, this approach can use the cache more efficiently. Since one service is being used  over a longer period of time,  relevant data for the service can be cached better in contrast to switching the service frequently  As a result, the performance can be increased. 