\section{Integration of Large Language Services}

Since \ac{LLM} like ChatGPT are a major part of this master thesis, the issue of integrating \ac{LLM} is another part of the concept. The following problems must be addressed
\begin{enumerate}
    \item How should an interface to a \ac{LLM} look like?
    \item How should the conversation with a \ac{LLM} be performed?
    \item How are messages from and to the model structured?
    \item Where should instructions be stored and processed so that they can be sent to a \ac{LLM}
\end{enumerate}
Issue 1 and 2 will be addressed in section \ref{sec:llm_interface}. The issue 2 3 will be discussed in section \ref{sec:llm_msg_structure}. Lastly, issue 4 will be dealt with in section \ref{llm_msg_storage}.
\subsection{An interface for large language models}\label{sec:llm_interface}

Since the market for large language model is constantly expanding in just a few years, designing a interface for communication is challenging. As a result, only the core functionality can be modeled by an interface in order to keep compatibility and ease extendability. 

An interface to a \ac{LLM} should support providing messages. These messages are provided by the user and can be a instruction (see  section \ref{llm_msg_storage}), data or any other relevant information.

Providing a message does not mean that the message is processed by the \ac{LLM} but is kept until further instruction. Thus, a user can prepare multiple messages before sending them to the model.

If the user decides to send the messages to the mode, another operation can be used. This operation sends the accumulated message to the large language model and waits for the response, so that the operation is synchronous. While an asynchronous approach would also be feasible, in most cases the data clump detection and refactoring process cannot proceed without the relevant information from the model so that waiting is tolerable. 

After sending and receiving the messages, the response from the model can be returned. Now the interface must deal with the messages it has accumulated. Since most models have no memory, the messages must be sent again if they are still relevant for future requests. However, storing and resending messages can cost more so that this should not be done always.

Therefore, the caller of the sending operation has the possibility to clear the previous messages after the \ac{LLM} has responded or keep them, and can therefore decide what to do.

\subsection{A message format for large language models} \label{sec:llm_msg_structure}
The structure of the messages to an \ac{LLM} is another issue to handle. Each model has its own requirements on how a request must be sent to it and how it will respond so that a general message structure must be developed. However there are similarities. Each model differentiates between requests by the user and the responses and represents the messages in a chronological manner, the most recent message is the message with the highest index. 

As a result, a simple message format can be an array of message object. Each message object is either an input or an output message. Input messages are generated by the user while output messages come from the model. A \ac{LLM} may have specific terms for input and output messages (e.~g. assistant and user), however using these simpler terms helps to generalize the problem.

A message object may contain multiple messages as it uses a string array. This is useful if multiple messages have a connection and need to be sent at the same time. For instance, if a user wants to transfer the file contents of a project to a \ac{LLM}, he can transmit each file within a a single message object. This not only helps to improve the performance a little bit but allows for easier management of messages since messages are grouped by request. 

\subsection{Storing instructions}\label{llm_msg_storage}

Another issue that arises while using \ac{LLM} is the management of instructions.

An instruction is a resource or artifact. Similarly to resources like textures, 3D models, sound data, images etc., they should be separated from the code \cite{separate_code_data}. As a result, separate text files for the instructions are better as they can be distributed and modified more efficiently, especially if other persons or entities create the instruction prompt.

An instruction is often not a single resource but a composition of many resources or other data. For instance, if an instruction contains an code example, it is reasonable to split the code example into a different file to reduce the size of a single file. This  also allows easier modification of the example with an IDE because combining instruction text and source code would lead to compiler errors.

As a result, an instruction resource may need to hold references to other files (e.~g. source code) or references to other data.
For the \ac{LLM}, the instruction should be complete such that it contains the whole instruction with the content of all referenced files and other information.

As a result, two perspectives need to be taken into consideration. From a user perspective, an instruction file should be as modular as possible as explained above. From the perspective of a large language model, an instruction needs to be complete. 

These two perspectives can be reconciled by a template model. The instruction file can be considered as a template. It does not contain the complete instruction that will be sent to a \ac{LLM} but a mixture of actual text and references.

When loading the instruction, all references must be correctly mapped with the correct content so that it can be sent to the model and be correctly interpreted. 

A reference to a raw string is in the format \enquote{\$\{id\}} where \enquote{id} is an identifier. When loading the template file a, string must be provided that replaces this reference.  A reference to a file is in the format \enquote{\%\{id\}}, so it starts with a percentage sign. On loading the template, a path to a file must be  provided and the content of that file replaces the reference. 

Listing \ref{lst:nstruction_template} illustrates an example instruction file which is also used in section \ref{sec:initial_experiments} The instruction prompts the model that code files will be provided (l.~1) and that all data clumps in those source code files need to be detected (l.~4). It also informs the model that examples of data clump will be provided (l. 6 and 11-12) and describes how the response by the \ac{LLM} should be structured (l~7-9). 

However, the examples and output format are not directly specified in the instruction file but are referenced. For instance the text \enquote{\%\{output\_format\}} will not be sent to the model but replaced by the actual output format that is stored somewhere else. The same applies to the examples. Also the specific programming language (e.~g. Java) is not directly defined by the instruction but will added when the instruction is sent to a large language model.

This allows for more flexibility since a single instruction files can be used for multiple programming languages and scenarios. However, it requires more configuration as outlined in section \ref{sec:config}.
\begin{lstlisting}[caption={Instruction file example}, label={lst:nstruction_template}, captionpos=b, numbers=left, ]
I will provide you one or more ${programming_language}
code files.
Find all data clumps in the respective files.

Examples of data clump are provided below.
Use the following JSON format for the output:
## JSON
%{output_format}

## Examples
%{examples}
\end{lstlisting}


\hfill
