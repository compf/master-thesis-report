\section{Integration of Large Language Models}

Since \acp{LLM} like ChatGPT are a major part of this master thesis, the issue of integrating \ac{LLM} is another part of the concept. The following problems must be addressed:
\begin{enumerate}
    \item How should an interface to a \ac{LLM} look like?
    \item How should the conversation with a \ac{LLM} be performed?
    \item How are messages from and to the model structured?
    \item Where should instructions be stored and processed so that they can be sent to a \ac{LLM}
\end{enumerate}
Issues 1 and 2 will be addressed in section \ref{sec:llm_interface}. The issues 2--3 will be discussed in section \ref{sec:llm_msg_structure}. Lastly, issue 4 will be dealt with in section \ref{llm_msg_storage}.
\subsection{An interface for Large Language Models}\label{sec:llm_interface}

Since the market for large language models is constantly expanding in just a few years, designing an interface for communication is challenging. As a result, only the core functionality can be modeled by an interface in order to keep compatibility and ease extendability. 

An interface to a \ac{LLM} should support providing messages. These messages are provided by the user and can be an instruction (see  section \ref{llm_msg_storage}), data or any other relevant information. After a message is stored,  no modifications of the message content can occur so that that necessary text transformation must occur earlier. 

Providing a message does not mean that the message is processed by the \ac{LLM} but is kept until further instruction. Thus, a user can prepare multiple messages before sending them to the model.

If the user decides to send the messages to the mode, another operation can be used. This operation sends the accumulated messages to the large language model and waits for the response, so that the operation is synchronous. While an asynchronous approach would also be feasible, in most cases the data clump detection and refactoring process cannot proceed without the relevant information from the model so that waiting is tolerable. 

After sending and receiving the messages, the response from the model can be returned. Now the interface must deal with the messages it has accumulated. Since most models have no memory, the messages must be sent again if they are still relevant for future requests. However, storing and resending messages can cost more so that this should not be done always.

Therefore, the caller of the sending operation has the possibility to clear the previous messages after the \ac{LLM} has responded or keep them and can therefore decide what to do.

\subsection{A message format for Large Language Models} \label{sec:llm_msg_structure}
The structure of the messages to an \ac{LLM} is another issue to handle. Each model has its own requirements on how a request must be sent to it and how it will respond so that a general message structure must be developed. However, there are similarities. Each model differentiates between requests by the user and the responses and represents the messages in a chronological manner, the most recent message is the message with the highest index. 

As a result, a simple message format can be an array of message object. Each message object is either a system message, an input message, or an output message. Input messages are generated by the user while output messages come from the model. System messages can be used to submit general instructions.  Each of these message types has a representation in an \ac{LLM} so the respective handler must perform a conversion. 

A message object may contain multiple messages. This is useful if multiple messages have a connection and need to be sent at the same time. For instance, if a user wants to transfer the file contents of a project to a \ac{LLM}, he can transmit each file within a single message object. This not only helps to improve the performance a little bit but allows for easier management of messages since messages are grouped by request. 

\subsection{Resources management}\label{llm_msg_storage}

Another issue that arises while using \ac{LLM} is the management of resources. Resources can either be static or dynamic. 

A static resource can be an instruction or additional context to be transmitted to an \ac{LLM}. A static resource is independent of the project to analyze and must already exist before the data clump refactoring process begins. For instance, examples of data clumps or the instruction to refactor data clumps do not use any information from the project to analyze but are generally written. 

 Similarly to resources like textures, 3D models, sound data, images etc., they should be separated from the code \cite{separate_code_data}. As a result, separate text files for the instructions are better as they can be distributed and modified more efficiently, especially if other persons or entities create the instruction prompt.

An instruction is often not a single resource but a composition of many resources or other data. For instance, if an instruction contains an code example, it is reasonable to split the code example into a different file to reduce the size of a single file. This  also allows easier modification of the example with an \ac{IDE} because combining instruction text and source code would lead to compiler errors.

As a result, an instruction resource may need to hold references to other files (e.~g. source code) or references to other data.
For the \ac{LLM}, the instruction should be complete such that it contains the whole instruction with the content of all referenced files and other information.

As a result, two perspectives need to be taken into consideration. From a user perspective, an instruction file should be as modular as possible as explained above. From the perspective of a large language model, an instruction needs to be complete. 

These two perspectives can be reconciled by a template model. The instruction file can be considered as a template. It does not contain the complete instruction that will be sent to a \ac{LLM} but a mixture of actual text and references.

When loading the instruction, all references must be correctly mapped with the correct content so that it can be sent to the model and be correctly interpreted. 

A reference to a raw string is in the format \enquote{\$\{id\}} where \enquote{id} is an identifier. When loading the template file, a string must be provided that replaces this reference.  A reference to a file is in the format \enquote{\%\{id\}}, so it starts with a percentage sign. On loading the template, a path to a file must be  provided and the content of that file replaces the reference. 

Listing \ref{lst:nstruction_template} illustrates an example instruction file.  The instruction prompts the model that code files will be provided (l.~1) and that all data clumps in those source code files need to be detected (l.~4). It also informs the model that examples of data clump will be provided (l. 6 and 11-12) and describes how the response by the \ac{LLM} should be structured (l~7-9). 

However, the examples and output format are not directly specified in the instruction file but are referenced. For instance, the text \enquote{\%\{output\_format\}} will not be sent to the model but replaced by the actual output format that is stored somewhere else. The same applies to the examples. Also the specific programming language (e.~g. Java) is not directly defined by the instruction but will added when the instruction is sent to a large language model.

This allows for more flexibility since a single instruction file can be used for multiple programming languages and scenarios. However, it requires more configuration as outlined in section \ref{sec:config}.
\begin{lstlisting}[caption={Instruction file example}, label={lst:nstruction_template}, captionpos=b, numbers=left, ]
I will provide you one or more ${programming_language}
code files.
Find all data clumps in the respective files.

Examples of data clump are provided below.
Use the following JSON format for the output:
## JSON
%{output_format}

## Examples
%{examples}
\end{lstlisting}


\hfill
\subsubsection{Dynamic resources}

In contrast, dynamic resources represent information from the analyzed project. In the end, they somewhat are representation of the context. However, directly transmitting the context is not useful, as it might contain unnecessary information, or some information must be gathered by combining information from multiple context layers. Therefore, it is the duty of the respective handler to use the existing context and generate messages that contain all relevant information. These messages are appended to the static resources at the end and then transmitted to the model. 

One issue that should be taken care of is that the dynamic resources and the static resources must be compatible for optimal or even usable results. The concept does not guarantee that as it would be unfeasible to implement. For instance, the static resource (instruction) can state that complete files are transmitted to the model. Nevertheless, a handler might only submit pieces of the source code or some other representation of the code. Hence,  this can lead to misunderstandings and can decrease the detection and refactoring quality. 
