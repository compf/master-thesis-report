\subsection{Context}\label{sec:context}
In section \ref{sec:pipeline_steps}, the several steps of the data clump pipeline are outlined. However, these steps do not include how the information generated by each step is stored and how successive steps might use this information.

Therefore, a concept to store intermediate information needs to be developed. Such information might be called \textbf{context}.

The context is a storage filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.

Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created. Also, a step can create multiple chained contexts if a step is capable of executing multiple steps. 

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false,scale=0.9]{figures/chapter3/context_pipeline.svg}
    \caption{Visualization of context pipeline}
    \label{fig:context_pipeline}
\end{figure}

The context can be represented as a linked list. Each node of the list represents a part of the context. 

Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a handler needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. While this can create overhead, the size of the linked list is bounded by the size of the pipeline so that the overhead is limited.  The overhead would only be relevant if the number of steps for refactoring data clumps would be substantially more, which is not to be expected. 

In figure \ref{fig:context_pipeline}, in the beginning, only the location of the project to analyze is stored, which is mandatory. The pipeline step executing the \textit{DataClumpDoctor} can use this context and append a new context. A name finding service like ChatGPT can then append a context with the names of the extracted classes. At the end, a service like IntelliJ can do the refactoring.



To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the \ac{AST} is available for a given source code file. 

Such context can only work if the context data is in a specific format, which can be generated by one step and might be correctly interpreted by each successive step. 