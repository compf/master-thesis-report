\section{Context}\label{sec:context}
In section \ref{sec:pipeline_steps}, the several steps of the data clump pipeline are outlined. However, these steps do not include how the information generated by each step is stored and how successive steps might use this information.

Therefore, a concept to store intermediate information needs to be developed. Such information might be called \textbf{context}.
\subsection{Context structure}
The context is a repository filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.The context can be represented as a linked list. Each node of the list represents a part of the context.
Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a handler needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. 


Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created. Also, a step can create multiple chained contexts if a step is capable of executing multiple steps. 

\begin{figure}
   \includesvg{figures/chapter3/context_pipeline_drawio.svg}
\caption{Exemplary lifecycles of the context: a) All steps are performed by an \ac{LLM}, b) Detection and filtering by another tool, c) Only name suggestion by \ac{LLM}}
\label{fig:context_lifecycle}
\end{figure}

Figure \ref{fig:context_lifecycle} illustrates how a different combination of handlers can affect the state of the context. The figure depicts a state diagram where rectangles visualize the most recent context and arrows indicate executed steps.  In the left part (a), the \ac{LLM} performs the detection of data clumps, and the refactoring (including all associated sub-tasks). Because all these task are performed by the same service, it would be unnecessary to save intermediate results. Therefore, the context is only updated once at the conclusion of the program. This final context does not have additional information but marks that the pipeline has been executed successfully.
 


In part b in the center of the figure, some parts of the pipeline are performed by the \textit{DataClumpDoctor} and other programs. For instance, data clumps are detected by the \textit{DataClumpDoctor} . As a result, a data clump detector context is created containing all detected data clumps. Afterwards, a filter removes non-important data clump and creates a new context of the same type that is linked to the previous context. If a later handler requires information about the detected data clump, it needs to find the first context that contains this information  beginning at the tail of the linked list. In this case it would be the filtered data clump context not the original data clump context. Here, also the context is not updated until the final step because the refactoring is performed via an \ac{LLM}. 

On the right side of the figure, the context is extended by a name finding context. This context includes information about a suitable identifier suggested by a \ac{LLM}. A manual refactoring tool (e.~g. IntelliJ), can use this context to perform the refactoring without further input by an \ac{LLM} so that the role of the model is limited here. 




To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the \ac{AST} is available for a given source code file. 


\subsection{Context serialization}

One advantage of developing a shared context is that it can be serialized  and de-serialized with less effort. At every step of the pipeline, the current context is serialized into a specific location. If for some reason, the pipeline could not be executed successfully or the user of the tool is not satisfied with the result, the program does not have to be re-run, but previous results can be re-used. This is particular important in the case of persistent handlers like the \textit{DataClumpDoctor}. Re-running this service multiple times under the same parameters would have no advantage but consume resources and time. By serializing the data clump detection results, the data can be retrieved again without executing the tool again.

Also for \ac{LLM}-related steps can this be useful because every call to an \ac{LLM} induces costs. On the other hand however, the creativity and randomness of such models can be an argument not to de-serialize such results but re-generate them so that multiple proposals can be created.

In the end, the issue whether to use context serialization or not should depend on the service and also should be configurable by the user of the tool. 

\subsection{Context information per step}
In the following, the context created or updated after each step will be explained:

\subsubsection{Code obtaining}
The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.

Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files. 

\subsubsection{File Filtering}
Developing a filtering context is more challenging. One major issue is that services suitable for data clump detection offer varying solutions for including and excluding specific files. For instance, PMD which is used by the \textit{DataClumpDoctor} requires that a rule set file is located at a specific location. This rule set file contains inclusion and exclusion rules using regular expressions. 

If files are sent to an \ac{LLM}, the filtering can be performed directly on the handler level, so that more sophisticated  filtering rules can be applied without being restricted to use regular expressions. 

Because the pipeline design separates the context from the handlers, the context has limited information about the handlers and does not know which handler will be executed later. Therefore, the filtering context must be very general to maximize compatibility with all possible handlers. 

Therefore, as a design decision, the file filtering context contain only file inclusion and exclusion rules that can be applied by subsequent handlers. This means that a more complex filtering rule (e.~g. by commit date), must be resolved to a list files to be included and excluded. 

It should also be noted that inclusion and exclusion rules might conflict, and these conflicts can be handled differently. In the case of PMD, if a file is included and excluded according to the rules, the inclusion prevails so that it is included. This makes it easier to only include a limited set of files and exclude a large number of other files. Vice versa, it is more challenging to exclude a very limited amount of files while including more files. The file filtering context does not attempt to resolve these issues. 

\begin{comment}
\subsubsection{Extraction of AST}
\end{comment}
\subsubsection{Data clump detection and filtering}

The format described in section \ref{sec:data_clump_format} can be used to store the detected data clump. While this format is relatively new, it contains all relevant information for storing data clump information and is extendable. The same format can be used for filtering too as all irrelevant data clumps can be removed easily. As a result, this context is an example of a context that might appear multiple times in the linked list. 

\subsubsection{Name finding}
To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database. 

Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.

\subsubsection{Class path choosing}

At the class extraction step, the created class must be saved somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project. 

For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located. 

Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used. 

As an alternative, the complete class body could be stored instead of saving it directly to a file. This might be advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step. 

For this master thesis, the first approach is used. The type-name--key is mapped to a specific path which is chosen at the name suggestion step although it could also be chosen at the class extraction step. 

 
\subsubsection{Validation}

The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass). 

In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases, modern build tools like \textit{Gradle} already acquire these data so they can be easily obtained. These error information can then be used to correct errors. 




