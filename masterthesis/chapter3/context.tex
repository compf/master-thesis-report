\subsection{Context}\label{sec:context}
In section \ref{sec:pipeline_steps}, the several steps of the data clump pipeline are outlined. However, these steps do not include how the information generated by each step is stored and how successive steps might use this information.

Therefore, a concept to store intermediate information needs to be developed. Such information might be called \textbf{context}.

The context is a repository filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.The context can be represented as a linked list. Each node of the list represents a part of the context.
Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a handler needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. 


Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created. Also, a step can create multiple chained contexts if a step is capable of executing multiple steps. 

\begin{figure}
   \includesvg{figures/chapter3/context_pipeline_drawio.svg}
\caption{Exemplary lifecycles of the context: a) All steps are performed by an \ac{LLM}, b) Detection and filtering by another tool, c) Only name suggestion by \ac{LLM}}
\label{fig:context_lifecycle}
\end{figure}

Figure \ref{fig:context_lifecycle} illustrates how a different combination of handlers can affect the state of the context. The figure depicts a state diagram where rectangles visualize the most recent context and arrows indicate executed steps.  In the left part (a), the \ac{LLM} performs the detection of data clumps, and the refactoring (including all associated sub-tasks). Because all these task are performed by the same service, it would be unnecessary to save intermediate results. Therefore, the context is only updated once at the conclusion of the program. This final context does not have additional information but marks that the pipeline has been executed successfully.
 


In part b in the center of the figure, some parts of the pipeline are performed by the \textit{DataClumpDoctor} and other programs. For instance, data clumps are detected by the \textit{DataClumpDoctor} . As a result, a data clump detector context is created containing all detected data clumps. Afterwards, a filter removes non-important data clump and creates a new context of the same type that is linked to the previous context. If a later handler requires information about the detected data clump, it needs to find the first context that contains this information  beginning at the tail of the linked list. In this case it would be the filtered data clump context not the original data clump context. Here, also the context is not updated until the final step because the refactoring is performed via an \ac{LLM}. 

On the right side of the figure, the context is extended by a name finding context. This context includes information about a suitable identifier suggested by a \ac{LLM}. A manual refactoring tool (e.~g. IntelliJ), can use this context to perform the refactoring without further input by an \ac{LLM} so that the role of the model is limited here. 




To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the \ac{AST} is available for a given source code file. 

Such a context can only work if the context data is in a specific format, which can be generated by one step and might be correctly interpreted by each successive step. 

\subsection{Context serialization}

One advantage of developing a shared context is that it can be serialized  and de-serialized with less effort. At every step of the pipeline, the current context is serialized into a specific location. If for some reason, the pipeline could not be executed successfully or the user of the tool is not satisfied with the result, the program does not have to be re-run, but previous results can be re-used. This is particular important in the case of persistent handlers like the \textit{DataClumpDoctor}. Re-running this service multiple times under the same parameters would have no advantage but consume resources and time. By serializing the data clump detection results, the data can be retrieved again without executing the tool again.

Also for \ac{LLM}-related steps can this be useful because every call to an \ac{LLM} induces costs. On the other hand however, the creativity and randomness of such models can be an argument not to de-serialize such results but re-generate them so that multiple proposals can be created.

In the end, the issue whether to use context serialization or not should depend on the service and also should be configurable by the user of the tool. 

\subsection{Context information per step}
In the following, the context created or updated after each step will be explained:

\subsubsection{Code obtaining}
The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.

Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files. 

\subsubsection{Filtering}
After filtering the files, the context can be a new path with all files considered relevant, requiring a deletion or moving file system operation. If a list of files is used, all irrelevant file paths can be deleted, which might be faster.



With this approach, the filtering context does not filter the files itself but provides the relevant information to the next contexts. 

Since the filtering context does not know the next context, it cannot know how filtering can be performed. Some data clump detection tools allow filtering, some cannot do this easily. The disadvantage of this approach needs to be stressed explicitly. The filtering context is a very primitive context. In most cases, it only loads data from configuration files, which might be counter-intuitive. 

\begin{comment}
\subsubsection{Extraction of AST}
\end{comment}
\subsubsection{Data clump detection and filtering}

The format described in section \ref{sec:data_clump_format} can be used to store the detected data clump. While this format is relatively new, it contains all relevant information for storing data clump information and is extendable. However, there are other reasons for other data formats. For instance,  the data clump type context format might be too detailed, which can lead to storage or performance issues. 

\subsubsection{Name finding}
To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database. 

Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.

\subsubsection{Class extraction}

After a class extraction step, the created class can be stored somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project. 

For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located. 

Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used. 

As an alternative, the complete class body could be stored instead of saving it directly to a file. This might be advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step. 

One might also argue that choosing the extracted class location should be a separate step. 
However, too many steps can be counterproductive as they increase the complexity and cause the individual steps to have too few responsibilities.

Therefore, the extractor determines the class location, saves the class content, and stores the data in its context. 


\subsubsection{Refactoring}
The context for the refactoring step  can be empty. While in previous steps, information has been gathered and analyzed, the refactoring step does not produce new information but applies the obtained information to refactor data clumps. 

One could argue that the context should contain information about the location of the refactored source code. This is only useful if the refactored source code should not be stored at the same location as the original source code.  However, the code obtaining step (see \ref{sec:code_obtaining}
) could handle this part by copying the project files to a specific location so this step is not necessary. 

\subsubsection{Validation}

The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass). 

In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases, modern build tools like \textit{Gradle} already acquire these data so they can be easily obtained. 




