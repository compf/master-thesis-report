\section{Configuration and usage of the main program}\label{sec:config}
This section outlines how the main program can be configured and used.

\subsubsection{Configuration}
An important aspect for the usability of the tool is the possibility to configure the tool for the user's need. 

Since the goal of the tool is to allow the combination of multiple services and other tools in order to find and refactor data clumps, the user must be able to define which handler deals with which step  as outlined in section \ref{sec:pipeline}.



The configuration is provided by a \ac{JSON} file which must exists when the tool is started. Using a configuration file over  command line arguments is preferred because of the modular design which includes multiple steps and objects. Using a command line argument system  requires designing a command line syntax that can handle this amount of configuration while remaining readable. This is deemed too complex for the purpose of this master's thesis. 


Listing \ref{lst:config} shows an example configuration file:
  \begin{figure} [ht!]
			\lstinputlisting
			[caption={ Example configuration file},
			label={lst:config}, basicstyle=\footnotesize
			captionpos=b, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
			{figures/chapter4/config.json}
		\end{figure}




In the beginning, the programming language is defined (l.~1). 

Then, the user can define objects. An object can be used by a handler to perform sub-steps, but declaring an object in \ac{JSON} does not necessarily mean that it is used. 

Here, two objects object is declared (l.4). The \textit{AbstractLanguageModel} represents the interface to the \ac{LLM}. In this case, it is an instance of the type \enquote{ChatGPTInterface} (l.5). The object is given arguments via the \enquote{args} attribute (l. 6). In this case, model name and temperatures are given (l. 7-8).  This instance is stored internally and can be queried for.   

The second object (l. 11) is a metric that is used by a filter. This metric does not need any arguments.





Then, the pipeline is defined (ll. 13-30). The pipeline uses four handlers. 

The first handler (ll. 14-16) uses the \textit{DataClumpDoctor} to detect data clumps. 


The second handler (ll. 17-22) is executed during the data clump filtering step. It is provided with the name of the aforementioned metric and a rank threshold. This means that the filter would return exactly one data clump with the largest size. 


Afterwards, the name finding step is connected to a handler that uses an \ac{LLM} for name finding (ll. 24-26).

Last but not least, the refactoring step is linked to the IntelliJ plugin that can refactor data clumps (ll. 27-29)). 



It should be noted that the order of the steps in the configuration  does not matter because the execution order is constant and, in most cases, each step requires the context of a previous step. Hence, parallel execution or vice-versa execution of steps is not possible. Only in the case of usage finding and name finding would a parallel execution make sense because none of these steps is dependent on the other.  However, this exception is not implemented.

Whenever a handler needs the services of another object, it has two possibilities.
\begin{enumerate}
    \item The handler knows the exact concrete type of the object it needs. For instance, handlers that filter files or data clumps can be given the names of filters and metrics that they should use. The handler can receive the respective object by querying the known name. 

    \item The handler does not know the concrete type but requests access to a service. In this example, a handler does not need to know that ChatGPT is used as the \ac{LLM}. It can request the object by querying for the abstract class name \enquote{AbstractLanguageModel}. This represents the classical dependency injection approach as the handler does not need to care about the specific model, but only that the \ac{API} contract is respected. The disadvantage of this design strategy is that only one  concrete type for each abstract type can be defined. If someone wants to use an \ac{LLM} for detection, and another model for refactoring, the current design would not allow this. 
\end{enumerate}



\subsubsection{Running the main program}


If a valid configuration file has been created, the main program can be started via \textit{npm run run <config> <project-location>} where \textit{config} is the location of the configuration file and \textit{project-location} is the path to the project to be analyzed. The order of these arguments can be swapped. 

With these arguments given, the following steps are executed:

\begin{enumerate}
    \item The main program parses the configuration file and initializes the objects with the given arguments. It also assigns the handlers to the respective steps.
    \item  The validity of the pipeline is checked. For instance, it is tested whether all required programs are installed.
    \item An initial code obtaining context is generated.
    \item The pipeline is executed. For each executed pipeline step:
    \begin{enumerate}
        \item Checks whether there is a serialized context for this step and whether the handler accepts that this context is sufficient. For instance, data clumps could have been detected beforehand so that detecting them again would serve no purpose. If there is an existing context, the handler does not need to be executed again.
        \item If none of these conditions are met, the handler is executed.
        \item the context returned by the handler is used as the new context.
        \item If supported by the respective context, it is serialized.
    \end{enumerate}
    \item The main program exits. The running time of each step is serialized for evaluation purposes. 
\end{enumerate}

It can be seen that due to the modular design, the algorithm for the main program is fairly small. However, it performs a critical part of the refactoring process as it uses the responses from each handler to decide how to continue with the execution of the program. 

Appendix \ref{app:uml_diagram} describes important classes of the tool and illustrates via an \ac{UML} diagram how they interact with each other. 