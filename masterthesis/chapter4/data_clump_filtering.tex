\section{Fltering approaches for files and data clumps}
\label{sec:data_clump_filtering}

As noted in section \ref{sec:pipeline_steps}, filtering can be used to reduce the data size of the refactoring process and only refactor those data clumps that are worthy to refactor. It is important to note that the filtering process happens at two stages.

In the first stage, no data clumps have been detected yet, and only file paths are known. Therefore, only a limited set of information is available. 

In the second phase, data clumps have already  been detected given the (possibly shrunk) set of files. Now, information about the data clumps can be used  to decide which data clump will be refactored in later phases.

In both phases, the same type of filtering objects are used because  a separate design for file filtering objects and data clump filtering objects would be over-engineering. A file filtering object could work with data clumps becaus ethe have an associated file path, however because a data clump could have two different file paths, the filtering object must take care of that issue. Conversely, a data clump filtering object might work for files only in some circumstances where the file path is sufficient. 

Hence, each filtering objects must indicate whether it supports file paths, data clumps or both, so that the filtering objects can be correctly used. 

In this section, file paths and data clumps are generalized as \textbf{items}.

Two concepts to  reduce the number of items can be distinguished.
\subsection{Ranking} \label{sec:metrics}
In the ranking approach, each data clump is scored using a metric. A higher score is better than a lower score.  Then, the data clumps are sorted based upon their score in a descending order. Finally, the first  \textit{n} data clumps are retained while the remaining data clumps are filtered out. With this ranking approach only important data clumps are retained. The importance of a data clump is determined by a metric, and the \enquote{survival} of a data clump depends on how many other data clumps with better scores exists.

In the following, the implemented metrics are discussed:

\subsubsection{Size of a data clump}

The size of a data clump means the number of variables associated with it. For instance, a data clump with the variables \textit{x}, \textit{y}, and \textit{z} has the size 3. 

Prioritizing data clumps with large sizes can be beneficial because removing them reduces the code size more significantly which helps to improve readability. Additionally, they are more likely to be perceived as a data clump because it is easier to notice two methods that share more than three parameters in contrast to methods sharing only three parameters. 

On the other hand,  it can become more difficult to find a suitable name for the extracted class the more parameters are shared because the probability of a common topic can be lower. 

\subsubsection{ Data clump occurrence}

The occurrence metric describes how often a type name key of a data clump appears throughout the software project to be analyzed. A high value is a strong indicator that refactoring is necessary because it demonstrates how strongly the code size can be reduced. Additionally, this metric is hard to measure manually because no developer will have an overview on the whole project and can count this occurrence making it more difficult to realize that a data clump has a high occurrence.  

On the other hand, refactoring these data clumps require that more lines of the project must be changes which needs to be verified and tested. 

\subsubsection{Affected Files}

The number of files affected by a data clump is somewhat  similar to the occurrence metric. However, in this metric each file is counted exactly once. If a data clump occurs in one file ten times and in another file 5 times, the occurrence metric would count all occurrences while the affected files metric would return two as only those both files are affected. 

Especially, if complete files are transmitted to an \ac{LLM}, this metric should be inverted meaning that low values should be ranked higher than higher scores. An \ac{LLM} only has a limited context window and the more files are transmitted the higher are the chances for errors.  

\subsubsection{Update time}

The time or frequency of when a particular file is updated can also be a metric to ascertain whether a refactoring is warranted. These information can be obtained from \ac{VCS} like Git as they are part of commits.

Files that have been recently updated can have new data clumps that should be refactored. These data clumps can be refactored more easily because it is more improbable that they have manifested so that other parts of the project or outside project depend on the existence of the data clump. 

On the other hand, files that were untouched for longer times might also contain data clumps that exist but are not refactored. One reason can be that developers are not familiar with these source files and are afraid to change them. Depending on the concrete type of data clumps, they can nevertheless be refactored by automatic tools so that developers do not need to read this unknown source code.


\subsubsection{Combining metrics}

One special metric is dependent on other metrics because it only creates a weighted sum of other metrics therefore allowing multiple metrics to be combined. 

\subsection{Filtering}
In the filtering approach, a binary condition is tested on each data clump individually. if the data clump does not meet the criterion, it is removed from further consideration. hence, each data clump is analyzed independently. Theoretically, this could mean that all data clumps are filtered out if the filtering conditions are too lax and apply to all data clumps. 

These filters can be combined by logical operation (and, or), so that more complex filtering rules are possible. 

The following subsections presents some  criteria for filtering data clumps
\subsubsection{Ignore abstract methods}

Abstract methods or non-implemented methods in interfaces only describe a contract without functionality which must be implemented by derived classes. Therefore, the signature of such operations is essential because it describes the contract which must be obliged to by derived classes.

Hence, changing the signature of abstract methods is often avoided as derived classes or classes implementing the interface must be modified too. These classes may reside outside the scope of the abstract class or interface. Therefore, refactoring the signature of abstract methods has a higher chance of errors because the derived classes may not be known or cannot be changed. 

Hence, it can be useful to exclude abstract methods for purpose of data clump refactoring to prevent these issues. 


\subsection{Combining metrics and filters}

The differentiation between filters and metrics can be relaxed by using metrics as a filters, and filters as metrics thereby enabling the construction complex filtering methods. 

In the first case, filters employ the metrics described in section \ref{sec:metrics}, a comparison operator, and a threshold value. If the metric applied to a data clump compared with the threshold value using the comparison function returns true, the data clump remains, otherwise it is filtered out. For instance, the metric can be the data clump size, comparison function can be the greater-than function, and the threshold value can be three. In this example only data clumps with more than three variables will remain for further consideration. 

Vice versa, a filter can be used as a metric. If a filter returns true on a specific item, the metric returns a different value than it would if the filter returns false. As result, the binary classification by a filter is still employed, but items that do not fulfill the criteria of the filter still has a chance not be filtered out. 

