\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Concept}\label{chapter_conception}
\endgroup
This chapter deals with the concept of the program. It provides information about the processing pipeline (\ref{chapter:pipeline} and how the different parts of that pipelines can be modelled in a way that allows easy extension. 


\hfill
\section{Pipeline}\label{chapter:pipeline}
In order to find and refactor data clumps automatically, a particular sequence of steps has to be respected. Most steps of this sequence are required to be in a specific order because they rely on information extracted in a previous step, or the quality of the results (which might be needed by subsequent steps) would get worse. In the following subsections, these steps will be outlined:

\subsection{Detecting data clumps}\label{subsec:chap3_data_clump_detection}
It is easily agreeable that data clumps can only be fixed if they have been found previously. Therefore, one of the first steps of the pipeline must be the detection of data clumps. 

The data clump detection process itself can be further divided into sub-steps.
\subsubsection{Filtering}\label{subsub:filtering_files}
First of all, all files of the programming language need to be detected. Usually, there are specific file extensions (e.g. \textit{.java}) for a source code file which simplifies the finding of files. It also for software project to have a root folder that contains all source code files. Therefore, one can recursively iterate through all files and directories of a given root folder that have a matching extension. 

However, not every source code file might be analyzed for data clumps. For instance, parts of the projects must not be refactored, or a refactorization would be too time-consuming. As outlined in chapter \ref{sec:introduction}, the cost for detection of data clumps might be too high. Therefore, filtering rules might be necessary to reduce the size of the relevant files for the subsequent steps.  

\subsubsection{Extraction of AST}
Then,  the source code needs to be simplified as comments, the method bodies, and other language parts are not relevant for detecting data clumps. This could be achieved by extracting a subset of the \ac{AST} from the source code which includes all the relevant information to identify data clumps.
\subsubsection{Similarity detection}

The next step is about finding pairs of method parameters and pairs of fields that are identical or at least similar. For this, the identifier and the  data type can be compared. Unequal variable names might not rule out a similarity between two variables. For instance, typos may happen, or synonyms may be used. Also, the type might be different. For instance, the data type \textit{double} can be seen as a superset of the datatype \textit{int} because every 32-bit integer can be converted to a \textit{double}.

From this, a graph can be constructed that visualizes the relationship of the several variables. A node represents a parameter or field value, while an edge exists if and only if a relationship between two variables is detected. 

\subsubsection{Data clump detection}
After similar variables have been detected, it must be determined whether a cluster of variables that are deemed similar is in fact a data clump. Here, the rules from section \ref{sec:data_clump_def} can be used. However, defining one's own rules can lead to better results, so flexibility is important, too. 

\subsection{Data clump filtering or prioritization} \label{subsub:filtering_data_clumps}
In section \ref{subsub:filtering_files}, a filtering by files is performed. However, even if many data clumps were detected, they do not need always to be refactored. One can decide to ignore certain data clumps and refactor them later or even never. 

There can be many reasons for this approach. For instance, parts of the source code that are infrequently updated might be too risky for refactoring since any automatic tool might cause bugs or invalid code that is hard to fix because the developers are not familiar with this part of the source code. On the contrary, a part of the source code that is updated more frequently might be in an intermittent state so that refactoring is currently not recommendable. Also, as noted in section  \ref{subsub:filtering_files}, if a service like ChatGPT is used, the cost of transferring many source code files with data clumps might be too high. 

In case of field data clumps, we also have to consider the worst-case, namely that  at least one of the data clump fields is public. This is considered bad design as fields should always be encapsulated by getters and setters \cite{5680918}. Nevertheless, it is still prevalent \cite{5076631}.

A subsequent refactoring of these fields is more difficult because they might be used by external programs or libraries which are beyond control of the developer, which might induce bugs or non-compilable software 

One can argue that refactoring should never affect the public interface since the issues mentioned above can occur \cite{10.1145/1352678.1352681}.

On the other hand, one can argue that those changes are still important because they can improve readability and the source code may be in early stages so that such breaking changes are still feasible. 

Therefore, it can be suggested to filter out certain data clumps that are considered not worthy of refactoring.

\subsection{Name finding}\label{subsec:chap3_data_clump_name_finding}
The next step of the pipeline is to find a suitable identifier of the extracted class.

There are some criteria for this identifier. It should be a valid name in the respective programming language so that, in general, only alphanumerical characters may be chosen. It should not be  in conflict with other identifiers as it may lead to bugs or compiler errors, and it should be readable and, as best as feasible, represent the fields of the class.

Hence, the name finding is a complex process that requires domain knowledge and creativity if the resulting name is to be accepted. 
Since English is the predominant language of of identifiers, it will be assumed that only English identifiers will be used. 


\subsection{Class Extraction}\label{subsec:chap3_data_class_extraction})
After a class name has been found, a respective class can be added to the source code. This class will be used as a replacement  for the data clump variables or parameters. 

In contrast to the previous step of the pipeline, this process can be automated more straightforwardly because it does not require creativity, and a general template can be used, that is:
\begin{itemize}
    \item Create a private field for each variable of the data clump
    \item Create a public getter for each variable of the data clump. This getter may be named get\textit{Var} where \textit{Var} is replaced by the capitalized name of the variable. The method returns the value of the respective field
    \item Similarly, a setter set\textit{Var} may be created that has no return type, accepts a new value as a parameter, and sets the respective field. 
    \item A constructor may be created that initializes the fields with provided values or with default values. 
\end{itemize}

Additionally, methods for converting a object to a string or creating a hash code may be useful, but these methods are usually not required for the refactoring of data clumps. 


\subsection{Refactoring}
After a class has been extracted, the next step is to refactor the source code to remove the data clumps. Here, again, we need to differentiate between method parameter data clumps as this has implications on the access scope. 

In case of a field data clump, the following transformations need to be applied.

\begin{enumerate}
    \item For each class in which the same field data clump is detected, the data clump variables are replaced by an object of the extracted class
    \item A getter of that object is created that has the same access level as the highest access level of the data clump items
    \item If a data clump item is read, it should be replaced by a call of the getter to the extracted class object chained by a call to the respective egtter of the data clump field
\end{enumerate}



In case of a method data clump, the general approach is similar to the case of field data clumps. The data clump items are replaced by an object of the extracted class, and all references to the data clump items are updated as well. Here, the object of the extracted class is not a getter because it is only locally defined. Additionally, the calls to the refactored method must be updated. Instead of providing the arguments for the data clump items directly to the method, an intermediate object can be created that is filled with those values and this object is provided to the method. 



\subsection{Example}
Listing \ref{lst:math_stuff_java} will be used as the foundation to describe a detailed approach for fixing data clumps.

Looking at the parameters \textit{x}, \textit{y}, and \textit{z}, there are multiple suggestions for a class name. The most trivial one is a simple concatenation  (e.g. \textit{XYZ} or \textit{X\_Y\_Z}). However, this name is often not suitable as it can be hard to read and to understand its meaning.

A more suitable approach requires domain knowledge. It is common knowledge that the terms \textit{x}, \textit{y}, and \textit{z} are terms used in math to describe coordinates. Using this information, a fitting name could be \textit{Coordinate}. Simple methods would not have discovered this name as artificial or human intelligence is required. 

\subsubsection{Class Extraction} (see \ref{subsec:chap3_data_class_extraction}))

Since a suitable class name has been found, the following class can be created with the information obtained previously. 

Listing \ref{lst:coordinate_java} shows an example implementation of a coordinate class. This class contains fields, getters, and setters, for all method parameters. There is also a constructor for initialization. This respects Fowler's opinion that the extracted class should not be a mere data class but a functional class that can be easily extended and modified.








\hfill
