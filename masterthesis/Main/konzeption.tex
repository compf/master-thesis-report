\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Concept}\label{chapter_conception}
\endgroup
This chapter deals with  the concept of the program. It provides information about the processing pipeline (\ref{chapter:pipeline} and how the different parts of that pipelines can be modelled in a way that allows easy extension. 


\hfill
\section{Pipeline}\label{chapter:pipeline}
In order to find and refactor data clumps automatically, a certain sequence of steps has to be respected. Most steps of this sequence are required to be in a specific order because they rely information extracted in a previous step or the quality of the results (which might me needed by subsequent steps) would get worse. In the following subsections, these steps will be outlined:

\subsection{Detecting data clumps}\label{subsec:chap3_data_clump_detection}
It is easily agreeable that data clumps cannot be fixed unless they have been found previously. Therefore, one of the first step of the pipeline must be the detection of data clumps. 

The data clump detection process itself can be further divided into sub steps.
\subsubsection{Filtering}\label{subsub:filtering_files}
First of all, all files of the programming language needs do detected. Usually, there are specific file extensions (e.g. \textit{.java}) for a source code file which simplifies the finding of files. It also for software project to have a root folder that contains all source code files. Therefore, one can recursively iterate through all files and directories of a given root folder that have a matching extension. 

However, not every source code file might be analyzed for data clumps. For instance, parts of the projects must not be refactored or a refactorization would be too time-consuming. As outlined in chapter \ref{sec:introduction}, the cost for detection of data clumps might be too high. Therefore, filtering rules might be necessary to reduce the size of the relevant files for the subsequent steps.  

\subsubsection{Extraction of AST}
Then,  the source code needs to be simplified as comments, the method bodies, and other language parts are not relevant for detecting data clumps. This could be achieved by extracting a subset of the \ac{AST} from the source code which includes all the relevant information. to identify data clumps.
\subsubsection{Similarity detection}

The next step is about finding pairs of method parameters and pairs of m´fields that are identical or at least similar.  For this, the identifier and the  data type  can be compared. Unequal variable names might not rule out a similarity between two variables. For instance, typos may happen or synonyms may be used. Also the type might be different. For instance, the data type \textit{double} can be seen as a super set of the datatype \textit{int} because every 32-bit integer can be converted tó a \textit{double}.

From this, a graph can be constructed that visualizes the relationship of the several variables. A node represent a parameter or field value while an edge exists if and only if a relationship between two variables was detected. 

\subsubsection{Data clump detection}
After similar variables have been been detected, it must be determined whether a cluster of variables that are deemed similar, are in fact a data clump. Here, the rules from section \ref{sec:data_clump_def} can be used. However, defining own rules can lead to better results so that  here flexibility is important too. 

\subsection{Data clump filtering or prioritization}
In section \ref{subsub:filtering_files}, a filtering by files is performed. However, even if many data clumps were detected, they do not need always to be refactored. One can decide to ignore certain data clumps and refactor them later or even never. 

There can be many reasons for this approach. For instance, parts of the source code that are infrequently updated might be too risky for refactoring since any automatic tool might cause bugs or invalid code that is hard to fix because the developers are not familiar with this part of the source code. On the contrary, a part of the source code that is updated more frequently might be in an intermittent state so that refactoring is currently not recommendable. Also as noted in section  \ref{subsub:filtering_files}, if a service like ChatGPT is used, the cost of transferring many source code files with data clumps might be too high. 

Therefore, it can be suggested to filter out certain data clumps that are considered not worthy of refactoring.

\subsection{Name finding}\label{subsec:chap3_data_clump_name_finding}
The next step of the pipeline is to find a suitable identifier of the extracted class.

There are some criteria for this identifier. It should be a valid name in the respective programming language so that in general only alphanumerical characters may be chosen. It should not be  in conflict with other identifiers as it may lead to bugs or compiler errors, and it should be readable and as best as feasible represent the fields of the class.

Hence, the name finding is a complex process that requires domain knowledge and creativity if the resulting name is to be accepted. . 
Since English is the predominate language of of identifiers, it will be assumed that only English identifiers will be used. 


\subsection{Class Extraction}\label{subsec:chap3_data_class_extraction})
After a class name has been found, a respective class can be added to the source code. This class will be used as a replacement  for the data clump variables or parameters. 

In contrast to the previous step of the pipeline, this process can be automated more straightforwardly because it does not require creativity and a general template can be used, that is:
\begin{itemize}
    \item Create a private field for each variable of the data clump
    \item Create a public getter for each variable of the data clump. This getter may be named get\textit{Var} where \textit{Var} is replaced by the capitalized name of the variable. The method returns the value of the respective field
    \item Similarly, a setter set\textit{Var} may be created that has no return type, accepts a new value as a parameter, and sets the respective field. 
    \item A constructor may be created that initialized the fields with provided values or with default values. 
\end{itemize}

Additionally, methods for converting a object to a string or creating a hash code may be useful, but these methods are usually not required for the refactoring of data clumps. 

\subsection{Example}
Listing \ref{lst:math_stuff_java} will be used a the foundation to describe a detailed approach for fixing data clumps.

Looking at the parameters \textit{x}, \textit{y}, and \textit{z}, there are multiple suggestions for a class name. The most trivial one is a simple concatenation  (e.g. \textit{XYZ} or \textit{X\_Y\_Z}). However, this name is often not suitable as it can be hard to read and to understand its meaning.

A more suitable approach requires domain knowledgwe. It is common knowledge, that the terms \textit{x}, \textit{y}, and \textit{z} are terms used in math to describe coordinates. using this information, a s fitting name could be \textit{Coordinate}. Simple methods would not have discovered this name as artificial or human intelligence is required. 

\subsubsection{Class Extraction} (see \ref{subsec:chap3_data_class_extraction}))

Since a suitable class name has been found, the following class can be created with the information obtained previously. 

Listing \ref{lst:coordinate_java} shows an example implemtation of a coordinate class. This class contains fields, getters, and setters, for all method parameters. There is also a constructor for initialization.  




\hfill
