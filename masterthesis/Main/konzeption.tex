


\hfill






In the following, the context created or updated after each step will be explained:

\subsubsection{Code obtaining}
The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.

Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files. 

\subsubsection{Filtering}
After filtering the files, the context can be a new path with all files considered relevant, requiring a deletion or moving file system operation. If a list of files is used, all irrelevant file paths can be deleted, which might be faster.



With this approach, the filtering context does not filter the files itself but provides the relevant information to the next contexts. 

Since the filtering context does not know the next context, it cannot know how filtering can be performed. Some data clump detection tools allow filtering, some cannot do this easily. The disadvantage of this approach needs to be stressed explicitly. The filtering context is a very primitive context. In most cases, it only loads data from configuration files, which might be counter-intuitive. 

\begin{comment}
\subsubsection{Extraction of AST}
\end{comment}
\subsubsection{Data clump detection and filtering}

The format described in section \ref{sec:data_clump_format} can be used to store the detected data clump. While this format is relatively new, it contains all relevant information for storing data clump information and is extendable. However, there are other reasons for other data formats. For instance,  the data clump type context format might be too detailed, which can lead to storage or performance issues. 

\subsubsection{Name finding}
To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database. 

Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.

\subsubsection{Class extraction}

After a class extraction step, the created class can be stored somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project. 

For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located. 

Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used. 

As an alternative, the complete class body could be stored instead of saving it directly to a file. This might be advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step. 

One might also argue that choosing the extracted class location should be a separate step. 
However, too many steps can be counterproductive as they increase the complexity and cause the individual steps to have too few responsibilities.

Therefore, the extractor determines the class location, saves the class content, and stores the data in its context. 


\subsubsection{Refactoring}
The context for the refactoring step  can be empty. While in previous steps, information has been gathered and analyzed, the refactoring step does not produce new information but applies the obtained information to refactor data clumps. 

One could argue that the context should contain information about the location of the refactored source code. This is only useful if the refactored source code should not be stored at the same location as the original source code.  However, the code obtaining step (see \ref{sec:code_obtaining}
) could handle this part by copying the project files to a specific location so this step is not necessary. 

\subsubsection{Validation}

The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass). 

In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases, modern build tools like \textit{Gradle} already acquire these data so they can be easily obtained. 





