\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Concept}\label{chapter:conception}
\endgroup
This chapter deals with the concept of the program. It provides information about the processing pipeline (\ref{sec:pipeline} and how the different parts of that pipeline can be modeled in a way that allows easy extension. This is expanded by the context that stores the data of each step (section (\ref{sec:context}). Next different approaches for executing the pipeline are discussed in section \ref{sec:pipeline_exec_order}. Then, the integration of \ac{LLM} like ChatGPT will be analyzed. 


\hfill
\section{Pipeline}\label{sec:pipeline}
In order to find and refactor data clumps automatically, a particular sequence of steps \textbf{(pipeline)} has to be respected. Most steps of this sequence must be in a specific order because they rely on information extracted in a previous step, or the quality of the results (which might be needed by subsequent steps) would worsen.

Each step of the pipeline is performed by a  \textbf{Handler}. A handler might handle one or more steps.  Each handler has information about what steps of the pipeline it supports and  a handler can be registered to a pipeline step if it supports the particular step.  

Since a service-based approach is used, a handler can be seen as a gateway to a different program or service that performs the specific functionality.  The handler and the respective service are tightly coupled. While a handler could deal with multiple services, it is not required by the design, and requires special care. Nevertheless, it recommendable to write the handler as abstract as possible.
\begin{figure}
    \centering
    \includesvg[inkscapelatex=false,scale=0.9]{figures/chapter3/solver_gateway_service.svg}
    \caption{Visualization of communication to services for one pipeline step}
    \label{fig:solver_gateway_service_overview}
\end{figure}

Figure \ref{fig:solver_gateway_service_overview} illustrates how the service-based approach works for one step:

\begin{enumerate}
    \item The program developed in this master thesis \textbf{(main program)} executes one step (e.g. finding a suitable class name)
    \item The handler registered to that step is used. It is provided with the current context containing the results from previous steps.
    \item The handler starts or employs a specific service. This service might be dependent on the current step or might be constant. The gateway knows how to start or use the service and sends all relevant information to the service (e.g. via files, network streams etc.) 
    \item The service processes the requests submitted by the handler. For instance, ChatGPT can suggest a suitable class name. The service might be outside of the control of the developer. For example, it might be programmed in a different programming language or the source code might not be available. At some point it will create a response and send it back to the handler. 
    \item The handler reads the response and processed it. It then creates a new context based on the previous context and the response from the service. 
    \item The new context is returned to the program and can be used for successive steps (e.~g. the suggested name can be used for creating a respective class) 
\end{enumerate}



Some parts of the pipeline are mandatory because finding and refactoring data clumps would be defeated if these steps were not executed. For instance, obtaining the source code of the project to analyze, finding data clumps, finding a suitable name, and refactoring can be regarded as mandatory.



A handler can also check whether it can be run in the given situation. For instance, a code obtaining handler can determine whether the project location does in fact exist or a validation handler for \textit{Gradle} can check whether the project is indeed \textit{Gradle}-based.

The program, therefore, acts as an orchestrator. It starts every service and controls them. The steps and services are temporally coupled. A step cannot be started unless all previous mandatory steps have completed. In most cases, if a step was not successful, further execution of the pipeline makes no sense, so it can be aborted. 

An alternative approach is called choreography. Here each step would call the next step without using the central main program. 

Both approaches have their advantages. In the choreography architecture, there is less coupling since a main program does not need to know which gateway handles which step. Instead, each gateway need to know the next (and only the next) step. This can become challenging if steps are optional because the gateway does not know what service to call next. Additionally, debugging the process is more challenging as there is no central point that coordinates the pipeline.

On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each gateway is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but are accessed using the internet or the local network, which causes more overhead. \cite{orchestration_choreography}

\section{Pipeline steps}\label{sec:pipeline_steps}
In the following subsections, the steps of the pipeline will be outlined. For each step, there is a discussion on why this step is noteworthy to consider and what points are important when implementing the step.
\subsubsection{Code obtaining}\label{sec:code_obtaining}
Data clumps can only be found if access to the source code is given. This requires that the source code is available in some location and that the location is known.

In most cases, this is trivial since a developer needing to find and fix data clumps should know where the source code is located.

However, there might be more complex cases. For instance, the source code could be located on a \ac{VCS} or must be downloaded elsewhere so that the newest changes can be considered. Therefore, defining a specific step for obtaining the source code before executing the rest of the pipeline is useful. 
\subsubsection{Detecting data clumps}\label{subsec:chap3_data_clump_detection}
It is easily agreeable that data clumps can only be fixed if found previously. Therefore, one of the first steps of the pipeline must be the detection of data clumps. 

The data clump detection process itself can be further divided into the sub-steps filtering, AST generation, similiarity detection, data-clump detection, and data clump filtering.
\subsubsection{Filtering}\label{subsub:filtering_files}
First of all, all programming language files need to be detected. Usually, there are specific file extensions (e.g., \textit{.java}) for a source code file, simplifying the finding of files. It is also common for software projects to have a root folder that contains all source code files. Therefore, one can recursively iterate through all files and directories of a given root folder with a matching extension. 

However, not every source code file might be analyzed for data clumps. For instance, parts of the projects must not be refactored, or a re-factorization would be too time-consuming. As outlined in chapter \ref{sec:introduction}, the cost of detecting data clumps might need to be lowered. Therefore, filtering rules might be necessary to reduce the size of the relevant files for the subsequent steps.  

\subsubsection{Extraction of AST}
Then,  the source code needs to be simplified as comments, the method bodies, and other language parts are irrelevant for detecting data clumps. This could be achieved by extracting a subset of the \ac{AST} from the source code, which includes all the relevant information to identify data clumps.
\subsubsection{Similarity detection}

The next step is finding pairs of method parameters and pairs of identical or at least similar fields. For this, the identifier and the data type can be compared. Unequal variable names might not rule out a similarity between two variables. For instance, typos may happen, or synonyms may be used. Also, the type might be different. For instance, the data type \textit{double} can be seen as a super-set of the datatype \textit{int} because every 32-bit integer can be converted to a \textit{double}.

From this, a graph can be constructed that visualizes the relationship of the several variables. A node represents a parameter or field value, while an edge exists if and only if a relationship between two variables is detected. 

\subsubsection{Data clump detection}
After similar variables have been detected, it must be determined whether a cluster of variables that are deemed similar is, in fact, a data clump. Here, the rules from section \ref{sec:data_clump_def} can be used. However, defining one's own rules can lead to better results, so flexibility is essential, too. 

\subsubsection{Data clump filtering or prioritization} \label{subsub:filtering_data_clumps}
In section \ref{subsub:filtering_files}, a filtering by files is performed. However, even if many data clumps were detected, they do not always need to be refactored. One can decide to ignore certain data clumps and refactor them later or even never. 

There can be many reasons for this approach. For instance, parts of the source code that are infrequently updated might be too risky for refactoring since any automatic tool might cause bugs or invalid code that is hard to fix because the developers are unfamiliar with this part of the source code. On the contrary, a part of the source code that is updated more frequently might be in an intermittent state, so that refactoring is currently not recommendable. Also, as noted in section  \ref{subsub:filtering_files}, if a service like ChatGPT is used, the cost of transferring many source code files with data clumps might be too high. 

In the case of field data clumps, we also have to consider the worst case, namely that at least one of the data clump fields is public. This is regarded as a bad design as fields should always be encapsulated by getters and setters \cite{5680918}. Nevertheless, it is still prevalent \cite{5076631}.

A subsequent refactoring of these fields is more complicated than that of less accessible fields because they might be used by external programs or libraries that are beyond the control of the developer, which might induce bugs or non-compilable software. 

One can argue that refactoring should never affect the public interface since the issues as mentioned earlier can occur \cite{10.1145/1352678.1352681}.

On the other hand, one can argue that those changes are still significant because they can improve readability, and the source code may be in the early stages to make such breaking changes feasible. 

Therefore, filtering out certain data clumps considered not worthy of refactoring can be suggested.

There are several methods to filter out data clumps which will be discussed in section \ref{sec:data_clump_filtering}

\subsubsection{Name finding}\label{subsec:chap3_data_clump_name_finding}
The next step of the pipeline is to find a suitable identifier of the extracted class.

There are some criteria for this identifier. It should be a valid name in the respective programming language so that, in general, only alphanumerical characters may be chosen. It should not conflict with other identifiers as it may lead to bugs or compiler errors, and it should be readable and, as best as feasible, represent the fields of the class.

Hence, the name finding is a complex process requiring domain knowledge and creativity if the resulting name is accepted. 
Since English is the predominant language of identifiers, it will be assumed that only English identifiers will be used. 

\subsubsection{Reference finding}
 Having detected a data clump, there is still information missing to refactor it. In particular, successful re-factorization requires that every method call, method definition, variable usage or variable definition that is connected to the data clump is updated in a later step. These so-called \textbf{references} need to be found so that they can be updated.

Since the identifier of the data clump variables (or methods) is known, one could search for all occurrences of that identifier. This, however, is not enough because identifiers have a scope. A variable with the name \textit{var} can be declared as a field, as a parameter of a method or inside a block (e.~g. if-branch). All these names can refer to different variables although they have the same identifier. As a result, a textural replacement approach will not work consistently.

Therefore, additional tools will be needed to parse the source code and use the rules of the programming language to find where the data clump items are indeed used.
\subsubsection{Class Extraction}\label{subsec:chap3_data_class_extraction}
After a class name has been found, a respective class can be added to the source code. This class will be used as a replacement for the data clump variables or parameters. 

In contrast to the name finding step of the pipeline, this process can be automated more straightforwardly because it does not require creativity and a general template can be used, that is:
\begin{itemize}
    \item Create a private field for each variable of the data clump
    \item Create a public getter for each variable of the data clump. This getter may be named \textit{getVar} where \textit{Var} is replaced by the capitalized name of the variable. The method returns the value of the respective field
    \item Similarly, a setter \textit{setVar} may be created that has no return type, accepts a new value as a parameter, and sets the respective field. 
    \item A constructor that initializes the fields with provided or default values may be created. 
\end{itemize}

Additionally, methods for converting an object to a string, checking the equality of two objects, or creating a hash code may be useful, but these methods are usually optional for refactoring data clumps. 


\subsubsection{Refactoring}
After extracting a class, the next step is to refactor the source code to remove the data clumps. Here, again, one needs to differentiate between method parameter data clumps as this impacts the access scope. 

In case of a field data clump, the following transformations need to be applied.

\begin{enumerate}
    \item For each class in which the same field data clump is detected, the data clump variables are replaced by an object of the extracted class
    \item A getter of that object is created that has the same access level as the highest access level of the data clump items
    \item If a data clump item is read, it must be replaced by a call of the respective getter of the extracted class
    \item Similarly, if a data clump item is written to, the assignment must be replaced by a setter
\end{enumerate}



In the case of a method data clump, the general approach is similar to the case of field data clumps. An object of the extracted class replaces the data clump items, and all references to the data clump items are also updated. Here, the object of the extracted class is not a getter because it is only locally defined. Additionally, the calls to the refactored method must be updated. Instead of providing the arguments for the data clump items directly to the method, an intermediate object can be created that is filled with those values, and this object is provided to the method. 

\subsubsection{Validation}

At the conclusion of the refactoring process, it is not certain that everything is perfect. This applies to manual refactoring and also automatic refactoring. For instance, there might be an undetected error, the refactoring leads to ambiguous method signatures, some reference are not up-to-date etc.

As a result, it is critical to test the project after refactoring so make sure that all requirements are still fulfilled. Testing includes checking the project for build errors (e.~g. invalid syntax, missing dependencies etc.) and unit test, where the functionality of the project is tested. Since those unit tests are supposed to run quick, they should be included in the validation check. 

Many build systems like \textit{Gradle} or \textit{Maven} support simple commands to run all unit tests and build the project in a consistent manner.  Therefore, one can run such a command and check the exit code or output to determine whether the project compiles and passes unit tests.

Disadvantageously, this may only be effective if the project is correctly configured with a build system. This might not always be the case, and it is not trivial to construct an interface for a general build solution. 

\subsubsection{Evaluation}

As another optional step, one might evaluate the effectiveness and efficiency of the data clump refactoring. For instance, one could execute the data clump detection process again to determine how many data clumps are fixed in comparison to the first iteration. 

Another metric for the evaluation could be the running time. A substantial running time of the refactoring can slow down the development process and cause the program not to be used again for future iterations. It could also be useful to anaylze the running time for each individual step so that handlers that require too much time or resources can be detected and later not be used. 

\subsubsection{Committing}

After all changes are made,they need are still local and not part of the \ac{VCS}. As a result, they must be committed and pushed so that every developer of the software project has access to the changes by the refactoring. Hence, it can be viewed as the reverse of the code obtaining step. 

In this step, a suitable commit message should be chosen. This could also be done via ChatGPT. Also a manual message based on the changed files might be sufficient. Further issues that might arise is whether a new branch should be created for the refactoring and how humans or other automatic tools (Continuous integration continuous development) should be used to validate the refactoring.  
\section{Context}\label{sec:context}
In section \ref{sec:pipeline_steps}, the several steps of the data clump pipeline are outlined. However, these steps do not include how the information generated by each step is stored and how successive steps might use this information.

Therefore, a concept to store intermediate information needs to be developed. Such information might be called \textbf{context}.

The context is a storage filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.

Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created. Also, a step can create multiple chained contexts if a step is capable of executing multiple steps. 

\begin{figure}
    \centering
    \includesvg[inkscapelatex=false,scale=0.9]{figures/chapter3/context_pipeline.svg}
    \caption{Visualization of context pipeline}
    \label{fig:context_pipeline}
\end{figure}

The context can be represented as a linked list. Each node of the list represents a part of the context. 

Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a handler needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. While this can create overhead, the size of the linked list is bounded by the size of the pipeline so that the overhead is limited.  The overhead would only be relevant if the number of steps for refactoring data clumps would be substantially more, which is not to be expected. 

In figure \ref{fig:context_pipeline}, in the beginning, only the location of the project to analyze is stored, which is mandatory. The pipeline step executing the \textit{DataClumpDoctor} can use this context and append a new context. A name finding service like ChatGPT can then append a context with the names of the extracted classes. At the end, a service like IntelliJ can do the refactoring.



To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the \ac{AST} is available for a given source code file. 

Such context can only work if the context data is in a specific format, which can be generated by one step and might be correctly interpreted by each successive step. 

In the following, the context created or updated after each step will be explained:

\subsubsection{Code obtaining}
The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.

Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files. 

\subsubsection{Filtering}
After filtering the files, the context can be a new path with all files considered relevant, requiring a deletion or moving file system operation. If a list of files is used, all irrelevant file paths can be deleted, which might be faster.

In most cases, however, the list of \textit{inclusion globs} and \textit{exclusion globs} are useful to be part of the filtering context. Globs are filenames that contain wildcard pattenr (e.~g. \textbf{*}). Each inclusion glob specifies which file must be included, and each exclusion glob specifies files that must be excluded. For instance, consider \textit{includeGlobs=[\enquote{*.java}]} and \textit{excludeGlobs=\enquote{*Main.java}}. In this case, all Java files are included, if however a file ends with \enquote{Main.java} it is excluded.

With this approach, the filtering context does not filter the files itself but provides the relevant information to the next contexts. 

Since the filtering context does not know the next context, it cannot know how filtering can be performed. Some data clump detection tools allow filtering, some cannot do this easily. The disadvantage of this approach needs to be stressed explicitly. The filtering context is a very primitive context. In most cases, it only loads data from configuration files, which might be counter-intuitive. 

\begin{comment}
\subsubsection{Extraction of AST}
\end{comment}
\subsubsection{Data clump detection and filtering}

The format described in section \ref{sec:data_clump_format} can be used to store the detected data clump. While this format is relatively new, it contains all relevant information for storing data clump information and is extendable. However, there are other reasons for other data formats. For instance,  the data clump type context format might be too detailed, which can lead to storage or performance issues. 

\subsubsection{Name finding}
To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database. 

Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.

\subsubsection{Class extraction}

After a class extraction step, the created class can be stored somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project. 

For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located. 

Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used. 

As an alternative, the complete class body could be stored instead of saving it directly to a file. This might be advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step. 

One might also argue that choosing the extracted class location should be a separate step. 
However, too many steps can be counterproductive as they increase the complexity and cause the individual steps to have too few responsibilities.

Therefore, the extractor determines the class location, saves the class content, and stores the data in its context. 


\subsubsection{Refactoring}
The context for the refactoring step  can be empty. While in previous steps, information has been gathered and analyzed, the refactoring step does not produce new information but applies the obtained information to refactor data clumps. 

One could argue that the context should contain information about the location of the refactored source code. This is only useful if the refactored source code should not be stored at the same location as the original source code.  However, the code obtaining step (see \ref{sec:code_obtaining}
) could handle this part by copying the project files to a specific location so this step is not necessary. 

\subsubsection{Validation}

The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass). 

In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases, modern build tools like \textit{Gradle} already acquire these data so they can be easily obtained. 
\begin{comment}

\section{Pipeline execution example}
Listing \ref{lst:math_stuff_java} will be used as the foundation to describe a detailed approach for fixing data clumps.

Looking at the parameters \textit{x}, \textit{y}, and \textit{z}, there are multiple suggestions for a class name. The most trivial one is a simple concatenation  (e.g., \textit{XYZ} or \textit{X\_Y\_Z}). However, this name is often not suitable as it can be hard to read and to understand its meaning.

A more suitable approach requires domain knowledge. It is common knowledge that the terms \textit{x}, \textit{y}, and \textit{z} are terms used in math to describe coordinates. Using this information, a fitting name could be \textit{Coordinate}. Simple methods would not have discovered this name as artificial or human intelligence is required. 



Since a suitable class name has been found, the following class can be created using previously obtained information. 

Listing \ref{lst:coordinate_java} shows an example implementation of a coordinate class. This class contains fields, getters, and setters for all method parameters. There is also a constructor for initialization. This respects Fowler's opinion that the extracted class should not be a mere data class but a functional class that can be easily extended and modified.


\end{comment}
\section{Pipeline execution order}\label{sec:pipeline_exec_order}

Another issue to analyze is the pipeline execution order. This should not be confused with the pipeline step execution order. The latter is simply the order of the different steps (e.~g. code obtaining before data clump detection). 

The former order deals with how the steps are executed with respect to each detected data clump. Two approaches are considered here:

In the first approach, each data clump is  further processed immediately after detection. This means after a data clump has been detected, it is immediately given a suggested name, a class is extracted if necessary, and the refactoring is executed. In the following, this approach will be referred to as \textit{one-data-clump approach}

In the second approach, all data clumps are detected first. Then all, those data clumps are assigned a suitable name, and classes are extracted if necessary. At the end, each data clump is refactored. In the following, this approach will be referred to as \textit{one-step approach}

As a result, in the \textit{one-data-clump approach}, a data clump is processed by one step of the pipeline and then by the next while in the \textit{one-step approach} the first step has to process all data clumps first before the data clumps can be processed by the succeeding steps.

There are arguments for and against both approaches. However, designing a system that supports both approaches is challenging and out of scope for this master thesis, so only one approach is used. 

The \textit{one-data-clump approach} leads to quicker intermediate results so that if the tool to be developed is stopped during execution, it is more likely that some data clumps are fixed so that some progress is being made.

Additionally, this approach is better for load balancing. Since many services might be used for refactoring data clumps and one data clump is processed at a time, each service gets a longer break before being used again. This is  especially important for services like ChatGPT, where many requests in a short time period can lead to outages or huge costs. 

On the other hand, the \textit{one-step approach} is easier to implement using the tools already existing. For instance, the \textit{DataClumpDoctor} detects all data clumps at a time and therefore is more compatible with the \textit{one-step approach}.

Moreover, configuration mistakes can be fixed more easily because in this approach, the software project will not be modified for some time (i.~e. data clumps will be detected or suitable names will be found). As a result, users of the tool are more likely to safely stop the tool if they discover a configuration mistakes. This might often occur in the very first moments after starting the tool.

Additionally, this approach can use the cache more efficiently. Since one service is being used  over a longer period of time,  relevant data for the service can be cached better in contrast to switching the service frequently  As a result, the performance can be increased. 


\section{Integration of Large Language Services}

Since \ac{LLM} like ChatGPT are a major part of this master thesis, the issue of integrating \ac{LLM} is another part of the concept. The following problems must be addressed
\begin{enumerate}
    \item How should an interface to a \ac{LLM} look like?
    \item How should the conversation with a \ac{LLM} be performed?
    \item How are messages from and to the model structured?
    \item Where should instructions be stored and processed so that they can be sent to a \ac{LLM}
\end{enumerate}
Issue 1 and 2 will be addressed in section \ref{sec:llm_interface}. The issue 2 3 will be discussed in section \ref{sec:llm_msg_structure}. Lastly, issue 4 will be dealt with in section \ref{llm_msg_storage}.
\subsubsection{An interface for large language models}\label{sec:llm_interface}

Since the market for large language model is constantly expanding in just a few years, designing a interface for communication is challenging. As a result, only the core functionality can be modeled by an interface in order to keep compatibility and ease extendability. 

An interface to a \ac{LLM} should support providing messages. These messages are provided by the user and can be a instruction (see  section \ref{llm_msg_storage}), data or any other relevant information.

Providing a message does not mean that the message is processed by the \ac{LLM} but is kept until further instruction. Thus, a user can prepare multiple messages before sending them to the model.

If the user decides to send the messages to the mode, another operation can be used. This operation sends the accumulated message to the large language model and waits for the response, so that the operation is synchronous. While an asynchronous approach would also be feasible, in most cases the data clump detection and refactoring process cannot proceed without the relevant information from the model so that waiting is tolerable. 

After sending and receiving the messages, the response from the model can be returned. Now the interface must deal with the messages it has accumulated. Since most models have no memory, the messages must be sent again if they are still relevant for future requests. However, storing and resending messages can cost more so that this should not be done always.

Therefore, the caller of the sending operation has the possibility to clear the previous messages after the \ac{LLM} has responded or keep them, and can therefore decide what to do.

\subsubsection{A message format for large language models} \label{sec:llm_msg_structure}
The structure of the messages to an \ac{LLM} is another issue to handle. Each model has its own requirements on how a request must be sent to it and how it will respond so that a general message structure must be developed. However there are similarities. Each model differentiates between requests by the user and the responses and represents the messages in a chronological manner, the most recent message is the message with the highest index. 

As a result, a simple message format can be an array of message object. Each message object is either an input or an output message. Input messages are generated by the user while output messages come from the model. A \ac{LLM} may have specific terms for input and output messages (e.~g. assistant and user), however using these simpler terms helps to generalize the problem.

A message object may contain multiple messages as it uses a string array. This is useful if multiple messages have a connection and need to be sent at the same time. For instance, if a user wants to transfer the file contents of a project to a \ac{LLM}, he can transmit each file within a a single message object. This not only helps to improve the performance a little bit but allows for easier management of messages since messages are grouped by request. 

\subsubsection{Storing instructions}\label{llm_msg_storage}

Another issue that arises while using \ac{LLM} is the management of instructions.

An instruction is a resource or artifact. Similarly to resources like textures, 3D models, sound data, images etc., they should be separated from the code \cite{separate_code_data}. As a result, separate text files for the instructions are better as they can be distributed and modified more efficiently, especially if other persons or entities create the instruction prompt.

An instruction is often not a single resource but a composition of many resources or other data. For instance, if an instruction contains an code example, it is reasonable to split the code example into a different file to reduce the size of a single file. This  also allows easier modification of the example with an IDE because combining instruction text and source code would lead to compiler errors.

As a result, an instruction resource may need to hold references to other files (e.~g. source code) or references to other data.
For the \ac{LLM}, the instruction should be complete such that it contains the whole instruction with the content of all referenced files and other information.

As a result, two perspectives need to be taken into consideration. From a user perspective, an instruction file should be as modular as possible as explained above. From the perspective of a large language model, an instruction needs to be complete. 

These two perspectives can be reconciled by a template model. The instruction file can be considered as a template. It does not contain the complete instruction that will be sent to a \ac{LLM} but a mixture of actual text and references.

When loading the instruction, all references must be correctly mapped with the correct content so that it can be sent to the model and be correctly interpreted. 

A reference to a raw string is in the format \enquote{\$\{id\}} where \enquote{id} is an identifier. When loading the template file a, string must be provided that replaces this reference.  A reference to a file is in the format \enquote{\%\{id\}}, so it starts with a percentage sign. On loading the template, a path to a file must be  provided and the content of that file replaces the reference. 

Listing \ref{lst:nstruction_template} illustrates an example instruction file which is also used in section \ref{sec:initial_experiments} The instruction prompts the model that code files will be provided (l.~1) and that all data clumps in those source code files need to be detected (l.~4). It also informs the model that examples of data clump will be provided (l. 6 and 11-12) and describes how the response by the \ac{LLM} should be structured (l~7-9). 

However, the examples and output format are not directly specified in the instruction file but are referenced. For instance the text \enquote{\%\{output\_format\}} will not be sent to the model but replaced by the actual output format that is stored somewhere else. The same applies to the examples. Also the specific programming language (e.~g. Java) is not directly defined by the instruction but will added when the instruction is sent to a large language model.

This allows for more flexibility since a single instruction files can be used for multiple programming languages and scenarios. However, it requires more configuration as outlined in section \ref{sec:config}.
\begin{lstlisting}[caption={Instruction file example}, label={lst:nstruction_template}, captionpos=b, numbers=left, ]
I will provide you one or more ${programming_language}
code files.
Find all data clumps in the respective files.

Examples of data clump are provided below.
Use the following JSON format for the output:
## JSON
%{output_format}

## Examples
%{examples}
\end{lstlisting}


\hfill
