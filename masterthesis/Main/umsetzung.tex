
\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Implementation}\label{chapter:implementation}
\endgroup

\section{Large language model usage}
A significant part of this master thesis is to include \ac{LLM} such as ChatGPT in the data clump refactoring pipeline. 

To allow the use of a \ac{LLM}, some considerations need to be made. This includes \enquote{prompt engineering}, \enquote{input engineering}\enquote{usage reduction}, and   \enquote{output parsing}. These steps will be outlined in the following:



\subsection{Prompt engineering}\label{sec:prompt_engineering}


In order to use a \ac{LLM} more effectively, the prompts and queries need to be modeled in a specific manner so that they are interpreted correctly. While deviating from these manners may still produce correct results, it nevertheless increases the risk of wrong results. The following tips were derived from the OpenAI documentation \cite{ChatGPT_url} and hence apply only to ChatGPT. Generally speaking. However, many recommendations will work for other \ac{LLM} too because they will help to make the prompt more clearly and prevent misunderstanding:

\subsubsection{Separate instruction and input}
Many queries to \ac{LLM} include an instruction and an input. For instance, a query to find and refactor data clumps could  provide the source code containing the possible data clumps \textbf{(input)}. The instruction could be the query \enquote{Find and refactor all data clumps in this source code}. 

OpenAI recommends that the instructions and input be separated as distinctive as possible. It suggests enclosing the input in a block of \textit{"""} or \textit{\#\#\#} to mark what the input and what the instruction is clearly.

\subsubsection{Provide detailed context and how the model should respond}

When generating a reply to a query, a language model will use the available context to process the query and generate an output that satisfies the user's need. This means that every bit of relevant information can help the language model to generate a better response.

On the contrary, providing irrelevant information can increase the chance of wrong responses, so the creator of a query must always consider what to include in a query and what not. 

In the context of data clump refactoring, the query should include the content of the source code and the programming language. However, files that cannot have data clumps (e.g., configuration files) should not be included.

An instruction for refactoring data clumps should state that only the refactored source code files should be returned without providing explanatory texts or other information, as they can hinder the parsing of the output. 

When using the \enquote{gpt-4-1106-preview} or \enquote{gpt-3.5-turbo-1106} model of the OpenAI-\ac{API}, developers can force the model to response in \ac{JSON}. hence, the output can be made more predictable and easier to control. A request using this mode must include the term \enquote{\ac{JSON}}. It should however be noted, that the precise structure of the \ac{JSON} returned may still differ from the request. 

\begin{comment}
\subsection{Cost reduction}

Many \ac{LLM} are not for free but must be paid based on usage or other factors. Even if a \ac{LLM} is free, there are many restrictions for the data to be processed by the model, so special care needs to be taken to reduce the data size as well as possible.  The cost might already be reduced by following the steps in section \ref{sec:prompt_engineering}. Nevertheless, there are other factors that will be outlined:
\end{comment}
\subsection{Prompt engineering for ChatGPT}

As outlined in  section\ref{sec:llm_challenges}, using \ac{LLM} can result in some challenges for refactoring code. Therefore, experiments are needed to test which input can lead to good results for finding or refactoring data clumps or both. These experiments should not be seen as a replacement for the full evaluation discussed in chapter \ref{chapter:eval}, but a mandatory prerequisite to save costs since the full evaluations needs to be performed on a larger set of projects.

The input project is a modified version of the source code discussed in \ref{sec:data_clump_def} which is fairly small but include at least three data clumps. Table \ref{tbl:javaTest_data_clumps} lists all data clumps exists in this project. The names in the \textit{From} or \textit{To} column are in the format \textit{className.methodName} or \textit{className} depending on whether it is a field-to-field \textit{(f2f} or parameter-to-parameter \textit{(p2p)} data clump

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c|c}
         Id &From & To & Type & Data clump variables  \\\hline
         1 & MathStuff.printLength & MathStuff.printSum & p2p & x,y,z\\\hline
        2 & MathStuff.printSum & MathSum.printMax & p2p & x,y,z\\\hline
        3 & MathStuff.printLength & MathSum.printMax & p2p & x,y,z\\\hline
        4 & MathStuff & MathUser & f2f & sign, mantissa, exponent\\\hline
        5 & Library & MathUser & f2f & sign, mantissa, exponent\\\hline
        6 & MathStuff & Library & f2f & sign, mantissa, exponent\\\hline
    \end{tabular}
    \caption{All data clumps in the test projects}
    \label{tbl:javaTest_data_clumps}
\end{table}

In this section, the results of some of these experiments will be discussed.  As repeatedly noted, these experiments are not always reproducible and therefore are only to some extent helpful. In all cases, ChatGPT is asked to output in \ac{JSON} to have a consistent format
\subsubsection{Finding data clumps without giving detailed instructions}

At first, it is useful whether ChatGPT can find data clumps without giving a specific definition. This means it is not asked to differentiate between parameters-to-parameters data clumps and field-to-field data clump, and the output format specifies the two endpoints (e.~g. \textit{from} or \textit{to}) of a data clump as \textit{item1} and \textit{item2}.

Nevertheless, the results are fairly good. ChatGPT ignores the inheritance between \textit{MathStuff} and \textit{BetterMathStuff} and does not output a data clump for each method in \textit{MathStuff}


\subsubsection{Providing examples of data clumps to ChatGPT}

Another approach for including ChatGPT for data clump refactoring is example-based. Multiple examples of data clumps and how they can be refactored are presented to ChatGPT. In order to avoid any bias, each type of data clump is equally represented and field identifiers or parameters field identifiers are unique across the whole example. This results in a larger request size and thusly more costs


\section{Implementation of steps}
While in section \ref{sec:pipeline} a general approach to find and refactor data clumps is given, the concrete implementation with the various tools is left out. In the following subsections, the implementations with the tools are described. In the case of \textit{code obtaining}, \textit{filtering}, only basic approaches are used so that they will not be detailed further. 


Detecting data clumps is a significant part of the pipeline. The tools used for this pipeline step are \textit{DataClumpDoctor}, \textit{ChatGPT} and \textit{IntelliJ}.
\subsection{Data clump doctor}

The  \textit{Data Clump Doctor} is NodeJS command line tool developed by Nils Baumgartner in preparation of this master thesis. It employs \textit{PMD} to find all classes, methods, and fields in a Java project to generate an \ac{AST}. This syntax tree is saved as a \ac{JSON}. 

In a second step, the generated \ac{AST} can be loaded again to find data clumps. This multi-step approach results in better performance since the detection of data clump requires many nested for-loops so that any reduction of data size has a measurable effect on the performance.

All detected data clumps are reported in the format specified in section \ref{sec:data_clump_format}.

Since the general tool is developed in Typescript, the \textit{Data Clump Doctor} can easily be integrated.

\subsection{ChatGPT}
ChatGPT is another approach to detect data clumps as it can process code easily and report data clumps in any format the user wants. It also supports many programming languages that other tools do not provide.

However, ChatGPT has a limited context size, so that processing large projects is  either too costly or simply not possible.

Additionally, giving ChatGPT the right instructions to find data clumps can be challenging. While ChatGPT can define and find some data clumps without further context, it is better to give it a precise definition to work with. The following definition leads to good result, however it cannot be guaranteed that this will work forever:


A data clump exists
\begin{enumerate}
   
   \item if at least three fields also exists in another class
   \item if at least three fields also exists as method parameters in some method
    \item if two methods have at least three common parameters
\end{enumerate}


\subsection{IntelliJ}
The Program Structure Interface provided by IntelliJ is an \ac{API} to analyze projects that can be loaded by IntelliJ. As a result, the various classes, methods etc. can be obtained which allows for the detection of data clumps. Like the ChatGPT approach, it can be easily extended to refactor the data clumps.

In order to use this API, an instance of IntelliJ must be started. To reduce loading times and improve the performance, IntelliJ can be started in a headless mode so that no GUI is initialized. Nevertheless, IntelliJ requires many resources and much overhead so that  the initialization  needs some time.

Another problem during the development of the tool is the issue of correctly loading projects. While a project that has no IntelliJ meta data files can be loaded, finding references of a method or field can lead to undefined behavior. Sometimes all references are correctly found, sometimes only a subset of the references are found, and sometimes no even no references are detected. The reasons for this are difficult to determine and the documentation is scarce, so the the PSI approach seems to be only suitable for projects created via IntelliJ or correctly initialized by IntelliJ with the required meta data. 

