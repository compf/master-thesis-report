
\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Implementation}\label{chapter:implementation}
\endgroup
In this chapter, the the design choices and considerations discussed in chapter \ref{chapter:conception} will be implemented. Firstly, in section \ref{sec:config}, the configuration of the tool is explained. After that experiments are performed in order to find out how ChatGPT can be integrated for data clump finding ( see section \ref{sec:prompt_engineering_impl}). In section \ref{sec:step_impl} an overview will be given which combination of tools can be used together to find and refactor data clumps and the limitation and strengths of each combination will be outlined. 
\section{Configuration}\label{sec:config}
An important aspect for the usability of the tool is the possibility to configure the tool for the user's need. 

Since the goal of the tool is to allow the combination of multiple services and other tools in order to find and refactor data clumps, the user must be able to define which handler deals with which step (see section \ref{sec:pipeline}).

The configuration is provided by a \ac{JSON} file whose location needs to be provided to the tool via a command line argument. It might be argued that providing the configuration directly via the command line is better suited than a separate configuration file because they do not require the creations of files and are easier for users who start the tool just once. Nevertheless, configuration files are persistent and especially \ac{JSON} can be more easily structured so that they are easier to understand. As a result, only \ac{JSON} files will be used  for the configuration. 


Listing \ref{lst:config} shows an example configuration file:
  \begin{figure} [htbp!]
			\lstinputlisting
			[caption={ Example configuration file},
			label={lst:config},
			captionpos=b, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
			{figures/chapter4/config.json}
		\end{figure}


In the beginning, the programming language is defined (l.~1). Then, the user can specify which handler should handle which step (l.~3-26). In this example, there are two steps. 

The first step (l.~4-9) establishes the project location and therefore the code obtaining context.

The second step (l.~ 10-26), deals with the detection of data clumps. Here a handler is used that allows detection and refactoring of data clumps via ChatGPT. This specific handler requires sub-handlers that perform intermediate task. For instance, the \textit{SimpleInstructionHandler} loads an instruction from a file path. The \textit{PairOfFileContent} handler submits pairs of file contents to ChatGPT (see section \ref{sec:initial_experiments}).

It should be noted that the order of the handlers in the configuration  does not matter because the execution order is constant and in most cases, each step requires the context of a previous step so that parallel execution or vice-versa execution of steps is not possible. Only in the case of usage finding and name finding would a parallel execution make sense because none of these steps is dependent on the other.  However, this exception is not implemented.

Not all relevant objects are part of the pipeline. Some are outside of the pipeline and can be referenced by all handlers. Here, the large language model \ac{API} is initialized with ChatGPT(l.~ 28).

Each object that appears in the \ac{JSON} is instantiated using dependency injection. This means that the objects initially are registered at a central location but not fully instantiated until they are really needed. The general approach is as follows:
\begin{enumerate}
    \item The main program loads the \ac{JSON} and iterates over all objects
    \item Each object has a category (e.~g. \textit{LanguageModelInterface} (see l.~ 28)) and a name (e.~g. ChatGPTInterface) 
    \item The name of the object and it paremters are registered at the central dependency manager under the category given
    \item If a handler or other object needs another object of a specific category, it can ask the central manager. If the respective object has  yet not been created, it will be created. Otherwise, the already instantiated object will be returned. 
\end{enumerate}
With this approach, a looser coupling can be achieved. The main program and the handlers does not need to know the exact details of the configuration but provide and receive them from a central dependency manager. Only the handler-specific configuration (e.~g. loading the project location  for the \textit{CodeObtaining} handler is still required and must be performed by each handler. These parameters are stored by the dependency manager. 
\section{Data clump filtering approaches}
\label{sec:data_clump_filtering}

As noted in section \ref{sec:pipeline_steps}, data clump filtering can be used to reduce the data size of the refactoring process and only refactor those data clumps that are worthy to refactor. Two concepts to  reduce the number of data clumps can be distinguished.

In the filtering approach, a binary condition is tested on each data clump individually. if the data clump does not meet the criterion, it is removed from further consideration. For instance, all data clumps that have only three variables can be filtered out. 
The following list presents some criteria for filtering data clumps
\begin{enumerate}
    \item Ignore interfaces or abstract classes as there is a risk that the refactoring will be incomplete because the source code of derived classes or classes implementing the interface is not available. 

    \item Choose randomly with a given probability whether to filter out a data clump
    \item Ignore data clumps with specific annotation such as \enquote{@injected} because they are filled via dependency injection thereby complicating refactoring.
 
\end{enumerate}

In the ranking approach, each data clump is scored using a metric. Then, the data clumps are sorted based upon their score in a descending order. Finally, the first  \textit{n} data clumps are retained while the remaining data clumps are filtered out. With this ranking approach only important data clumps are retained. The importance of a data clump is determined by a metric, and the \enquote{survival} of a data clump depends on whether other data clumps with better scores exists.

In the following list, some metrics for scoring a data clump are explained

\begin{enumerate}

    \item The number of occurrences of data clump. For instance, if there is a data clump with the variables \textit{x}, \textit{y}, and  \textit{z}, it is counted how many methods have these parameters and how many classes have these fields

     \item The number of data clump items. For instance, the \enquote{xyz}-data-clump has three data clump items.
     
    \item The number of affected files. Every file that is affected by a data clump. This includes the location of the data clump and also the files where methods and fields that are part of the data clump are references because these files must also be changed if the data clump is refactored. 
    
\end{enumerate}

Note that any metric mentioned can also be used by a bianry filter. For instance, one can filter out data clumps that have only three items.
Whether any filter or metric method is suitable for a project is an open research topic which this master thesis does not attempt to solve. 






\label{sec:implementation_tools}





\section{Extent of large language model integration}

A major part of this master thesis is the question how well \ac{LLM} can be integrated into the data clump  detection and refactoring process. Several extends to such integration exists which will be discussed in the following subsections. 

\subsubsection{No Large language model integration}

The most basic approach to refactor data clumps automatically is by using no \ac{LLM} as if they do not exists. This is the only working approach if the use of  \ac{LLM} is forbidden or too costly. The tools described in section \ref{sec:implementation_tools} are used to perform the detection and refactoring of data clumps. 

The only major issue is the name finding step. As outlined in section \ref{sec:pipeline_steps}, the name finding step usually requires creativity. Without a \ac{LLM}, this poses more challenges.

The simplest approach would be a simple concatenation of the data clump item identifiers. For instance, the data clump items \textit{x}, \textit{y}, and \textit{z} can be concatenated to  the identifier \textit{XYZ} or \textit{X\_Y\_Z}. The resulting names are valid but they do not give information about the purpose of the class so that their usefulness is limited.

Another approach is to use more extensive \ac{NLP} techniques that classify groups of word into a category. This master thesis however will not deal with this issue because it is a complex issues on its own.

\subsubsection{Limited large language model integration}

If a \ac{LLM} can be used but costs or resources are a significant concern, a limited inclusion approach is useful. In this scenario, the \ac{LLM} only performs small tasks like suggesting names for an extracted class, filtering detected data clumps etc. In each case, the model is not provided with the  complete source code but only specific data. so that the data size to be transmitted is more limited for larger projects. The issue here is that large language models have no context about the source code so that their suggestion can have worse quality. 

\subsubsection{Extensive large language model integration }

In this approach, a major subset of the pipeline is performed by a \ac{LLM}. For instance, the detection of data clump can be performed by a large language model, but the refactoring is performed by a traditional method. 

This results in higher costs and resource usages as the software project must be transmitted to the \ac{LLM} but uses the creativity and extensibility of a \ac{LLM}

\subsubsection{Complete integration of large language models}

In this method, the large language model performs most steps of the pipeline. it should be noted that some steps like code obtaining or file filtering are too trivial or too technical to be performed by a \ac{LLM} so that some smaller parts of the pipeline should be executed with other tools. 

This method uses the full creativity and adaptability of a \ac{LLM}, so that data clump detection are seamlessly combined. Nevertheless, the costs and resource consumption must be considered. In the case of a one-shot-prompting, the model detects and refactors the data clumps in one step. As already noted, this can lead to worse results because to many steps are combined. However, the costs are similar to the  extensive integration approach because the source code needs to be transmitted only once. 

In a multi-shot approach, the model is asked to find data clumps and only then asked to refactor them. This requires that the source code be sent multiple times as a \ac{LLM} is often stateless. 

\section{Prompt engineering ChatGPT for data clump detection and refactoring}\label{sec:prompt_engineering_impl}

As outlined in  section \ref{sec:llm_challenges}, using \ac{LLM} can result in some challenges for refactoring code. Therefore, experiments are needed to test which input can lead to good results for finding or refactoring data clumps or both. These experiments should not be seen as a replacement for the full evaluation discussed in chapter \ref{chapter:eval}, but a mandatory prerequisite to save costs since the full evaluations needs to be performed on a more numerous set of larger projects.

The following scenarios of ChatGPT are addressed:
\begin{enumerate}
    \item ChatGPT only finds data clumps and report them in the format specified in section \ref{sec:data_clump_format}. The refactoring can be done by another tool.
    \item ChatGPT performs the refactoring. It is given the data clumps detected by other tools. 
    \item ChatGPT searches for and refactors immediately all data clumps
\end{enumerate}

The scenarios reflect the idea of ChatGPT being a service that provides functionality for one or more steps in the pipeline. It is not always necessary for ChatGPT to perform all steps (i.~e.  data clump detection and refactoring). For instance, data clump detection and refactoring can be done with one query, but such a general query can lead to worse  results. Splitting the query into a detection and refactoring part can improve the result(few-shots), however this requires multiple transmission and therefore higher costs. 

On the other hand, reading from and outputting into the format specified in section \ref{sec:data_clump_format} can be problematic as it may be difficult to instruct ChatGPT to understand the format correctly. This, however, is necessary to guarantee compatibility with other tools and services.  As an alternative, ChatGPT could be instructed to use a different format and the conversion happens in the handler. Nevertheless, a format must be chosen that ChatGPT can interpret. 

The input project is a modified version of the source code discussed in \ref{sec:data_clump_def} which is fairly small but include at least three data clumps. Table \ref{tbl:javaTest_data_clumps} lists all data clumps which exists in this project. The names in the \textit{From} or \textit{To} column are in the format \textit{className.methodName} or \textit{className} depending on whether it is a field-to-field \textit{(f2f} or parameter-to-parameter \textit{(p2p)} data clump.

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c|c}
         Id &From & To & Type & Data clump variables  \\\hline
         1 & MathStuff.printLength & MathStuff.printSum & p2p & x,y,z\\\hline
        2 & MathStuff.printSum & MathSum.printMax & p2p & x,y,z\\\hline
        3 & MathStuff.printLength & MathSum.printMax & p2p & x,y,z\\\hline
        4 & MathStuff & MathUser & f2f & sign, mantissa, exponent\\\hline
        5 & Library & MathUser & f2f & sign, mantissa, exponent\\\hline
        6 & MathStuff & Library & f2f & sign, mantissa, exponent\\\hline
    \end{tabular}
    \caption{All data clumps in the test projects}
    \label{tbl:javaTest_data_clumps}
\end{table}

Integrating large language models into the pipeline requires special attention. As the \ac{LLM} is a special service that resides outside of the domain of the pipeline, all required information must be submitted to it and any context not provided to the model is not available for consideration. The way such context is worded or submitted to the model has a great impact on the quality of the results.

Additionally, there are many models available that have been trained on general or fine-tuned  tasks. Selecting the right model for a specific task is not trivial. 

These parameters can be subsumed as the configuration of the \ac{LLM} as they should be changeable by the user. 

Therefore, experiments should be conducted to ascertain what model is useful for data clump detection and refactoring and how the information for the model is structured.  

Four groups of configuration options can be distinguished:
\begin{enumerate}
    \item The prompt including the manner the term \enquote{data clumps} is conveyed to the model
    \item How to process the source code of the project under analysis to the \ac{LLM}
    \item The manner the LLM is prompted to output its results
    \item model specific parameters like the choice of the model itself and parameters like the temperature
\end{enumerate}

The first three options are inter-related. Changing the type of input the LLM is confronted with or the output format requires a change of the prompt in order for the LLM to understand what to do. The template system discussed in section \ref{sec:llm_msg_structure} simplifies the prompt building because the prompts for explaining the input format, output format, and the other relevant context can be separated.

The several configuration options are as follows















\begin{comment}
In this section, the results of some of these experiments will be discussed.  As repeatedly noted, these experiments are not always reproducible and therefore are only to some extent helpful. In all cases, ChatGPT is asked to output in \ac{JSON} to have a consistent format.

In each experiment, different parameters of the input are adjusted. Each these parameters has an influence of the quality of the output:

\begin{itemize}
    \item Instruction
    \item Model
    \item Data format
    \item Data size
    \item Temperature
\end{itemize}
\end{comment}
\subsubsection{Instruction}\label{sec:llm_instruction}
The instruction is a main part of a prompt to ChatGPT or other \ac{LLM}. It tells the  \ac{LLM} what to do and how to respond. As a result, varying the instruction is an important part of prompt engineering. \newline



Initially, it is useful whether ChatGPT can find data clumps without giving a specific definition. This means ChatGPT needs to deduce the definition of a data clump from its trained models which might not be the same as the definition used in section \ref{sec:data_clump_def}. For instance, data clumps are defined in this master thesis to have at least three common variables while other sources require at least four common variables. 
\begin{comment}
Nevertheless, the results are fairly good. ChatGPT ignores the inheritance between \textit{MathStuff} and \textit{BetterMathStuff} and does not output a data clump for each method in \textit{MathStuff}
\end{comment}


In another approach, ChatGPT is provided a specific definition of data clumps and asked to apply this definition on the project. Therefore, a precise definition is elementary for this approach.

The following query was used:
A data clump exists if
\begin{enumerate}
\item  Two methods (in the same or in different classes) have at least 3 common parameters
    and one of those methods does not override the other, 
\item At least three fields in a class are common with the parameters of a method (in the same or in a different class), or 
\item Two different classes have at least three common fields
\end{enumerate}


Another approach for including ChatGPT for data clump refactoring is example-based. Multiple examples of data clumps and how they can be refactored are presented to ChatGPT. In order to avoid any bias, each type of data clump is equally represented and field identifiers or parameters field identifiers are unique across the whole example. This results in a larger request size and thusly more costs. Nevertheless, if larger projects were to be used the size of the examples becomes negligible so that this approach has its reasons. The examples provided to ChatGPT can be found in the digital appendix under the path \enquote{data\_clump\_examples.java} 

\subsubsection{Model}
The model is a parameter for ChatGPT that controls the quality, performance, and cost of processing a query and responding to it. 

OpenAI provides multiple models for user of its API. Each  ChatGPT model  is based on the same original model but is fine-tuned to optimize for a specific task. For this master thesis, only  the  models \enquote{gpt-4-1106-preview} and \enquote{gpt-3.5-turbo-1106} are relevant. Both models support a \ac{JSON}-only mode, which means that any response by the model will be valid \ac{JSON} instead of natural language, so it easier to parse and process. \cite{chatgpt_new_models}

The model {gpt-4-1106-preview} is also called \textit{GPT-4 Turbo}. It can process 128,000 tokens at once and has a knowledge cutoff of April 2023. \cite{chatgpt_new_models}

The model \enquote{gpt-3.5-turbo-1106} has a context window size of 16,000. The knowledge cutoff is September 2021. \cite{chatgpt_new_models}


In general, GPT-4-Turbo has a better quality because of the larger context window size and more up-to-date information. While the more recent cutoff date might not be that important for data clump detection and refactoring, the larger context window size means that larger amount of code can be transmitted to ChatGPT at once. So, larger projects can be refactored more easily.  \cite{chatgpt_new_models}

On the other hand, the costs per 1000 tokens is  \$0.003 for GPT-3-Turbo whereas the costs for GPT-4 is \$0.01 for 1000 tokens. As a result, the GPT-4-Turbo is thrice as expensive as GPT-3 which can be a reason to use GPT-3. \cite{chatgpt_new_models}

Additionally, there are  stricter limits for using GPT-4. For instance, only 500,000 tokens can be sent per day, and 150,000 tokens can be sent per minute. In contrast, GPT-3-Turbo has  lower values for token per minutes but no  token per day limit.  ¸\cite{chatgpt_limits}

\subsubsection{Output format}
 Similar to the input format, a model can also be configured to format its output. If the content of full files is provided, the model can  apply the required refactoring on the file and return the refactored files. This however requires transmitting the full file content nearly twice which induces costs and resource usage. It can lead to small errors if parts of the source code that have nothing to do with data clumps are changed even though there was no reason to do so. The \ac{LLM} being a probabilistic model cannot prevent such small mistakes. Additionally some \acs{LLM} have a maximal output size which prevents them from outputting large files or many small files.

 An alternative approach is similar to the related input approach. The model is instructed only to return those lines that have been changed. These changes can be applied to the file by a pipeline handler without the help of the \ac{LLM}. However, the model must provide some context of where the changes should be applied. For instance, the model can return the line number for the changes. This can cause problems if the  line number must be determined by the model because the line number might be not up-to-date if previous changes created new new-lines, or the LLM starts counting the lines at the wrong location.

 Alternatively, the LLM could return the old content of each line. Then every occurrence of the old content can be replaced by the new content. Based on heuristic assumption that if the text \textit{t} appears multiple times and is suggested to be changed by the \ac{LLM}, it can be replaced everywhere in this file. 

 Combining both approaches further helps to increase the accuracy, as the handler can check whether the content at the line number matches the old content returned by the LLM.  
 

\subsubsection{Data format}

After creating a instruction, ChatGPT cannot do much because it does not have access to the source code which must be provided to ChatGPT. Therefore, the next part of the prompt is some representation of the source code

The two approaches tested are providing the complete source code and providing the \ac{AST}.

Providing the complete source code is the most natural manner to give ChatGPT the relevant information. Since ChatGPT is language-agnostic, it can deal with many programming languages and therefore is able to process the source code files to find data clumps. However, source code files can be large as they can contain comments, method bodies or other parts of the programming language that are not relevant for detecting data clumps. This leads to higher costs.

On the other hand, the \ac{AST} can be provided. The \ac{AST} only contains a reduced structure of the source code and will therefore cost not as much. However, the \ac{AST} is usually not available but must be produced by other tools or ChatGPT (which means the source code has to be sent to the \ac{LLM} nevertheless). Additionally, there is no fixed format for the \ac{AST} which might complicate parsing and processing the \ac{AST} or the results by ChatGPT.


In case ChatGPT should also perform the refactoring, the \ac{AST} becomes not as useful. Refactoring data clumps always include updating method calls or variable usages which might not be available in the \ac{AST}. One could submit the \ac{AST} and the source code so that ChatGPT uses the \ac{AST} for detecting and the source code for refactoring, but this creates more costs and whether it may worsen the quality is an open question as ChatGPT needs to process more data and create a connection between source code and \ac{AST}.
\subsubsection{Data size}

The size of the data is the amount of data that is submitted during a single request. Here three approaches are tested.

In the first approach, the full project is submitted to ChatGPT to find data clump while in the second pairs of files are transmitted which each pair of file being a separate independent request.

The advantage of the first approach is that only one transaction needs to be done, and ChatGPT can find many data clumps in one shot. ChatGPT can therefore better understand the structure and provide better suggested name or find data clumps where the identifier names of the data clump variables are not identical. However,  a major issue is that many projects are too large, and therefore ChatGPT cannot provide the advantages just described.

In the second approach, pair of files are transmitted to ChatGPT with an instruction mentioned in \ref{sec:llm_instruction}
This means that every request only contains the instruction and the content of the two files. Therefore, ChatGPT can only find data clumps only in those two files and in no others as it has no memory of previous prompts. One might think that this approach is cheaper, however, if one still wants to find data clump in the whole software project, he has to to transmit $\binom{n}{2}=0.5*n*(n-1)$ where $n$ is the number of relevant files. Therefore, the cost can be much higher than transmitting each file only once as in the first approach.

Additionally, each file could be sent again separated in order to find data clumps that exists in a single file and might not be detected if two files are provided so that the total number of files would be $0.5*n*(n-1)+n$.


Another approach is not to sent full file contents but only code snippets that contain source code. Each snippet of code so provided is related to a data clump. It can be the method declaration of a data clump an associated method call, a field definition, or a variable usage. By providing only these lines, many irrelevant lines of code can be omitted. The caveat of this idea that the data clumps  and their usage must be known already in order to find those snippets of code. Therefore, this method only works in limited circumstances.

\subsubsection{Temperature}
As outlined in section \ref{sec:chatgpt}, the temperature is a significant parameter that controls the randomness of the \ac{LLM} output. A high temperature indicates that the response by ChatGPT is more random and unpredictable, while a low temperature results in more consistent responses.

Hence, a higher temperature can help ChatGPT to be more creative so that it may find data clumps that it would usually not detect. However, it can also lead to a lower specificity because it may flag wrong methods or fields as part of a data clump because of the increased unpredictability. 

As a result, adjusting the temperature parameter is a significant part for the small experiment. Here, two values will be tested. A low temperature of 0.1 and a hight value of 0.9. While these are two extremas and not a middle ground, they can give a good approximation of whether thelow or high temperatures are better for data clump detection. 

\begin{comment}
\subsection{Requesting follow-ups}

As outlined in section \ref{sec:chain of thought},  one prompt might no be enough to refactor data clumps. This might also apply for the simpler task of detecting data clumps.

Instead of expecting that a single request will detect all data clumps, another approach would be to repeat the same requests multiple times and aggregate the results. These follow-up prompts result in ChatGPT to re-analyze the code so that more data clumps can be found. 

The issue  with follow-up prompts is how often should a prompt be repeated. Each prompt repeat requires that the conversation history to be sent again because ChatGPT is stateless. This leads to higher resource usage and costs. There are multiple approach to implement follow up.

\begin{description}
\item[Full project follow-up] The whole project (all relevant files) is sent to  ChatGPT again with an instruction to find more data clumps. As a result, more data is sent per  request while the number of request can be reduced. As a  result, ChatGPt might find additional data clumps that it has not found during the previous prompts.
\item [Tuple-based follow-up] A tuple of two files in the project (i.~e. the content of those files) is sent again to ChatGPT with the instruction to find more data clumps. If the goal is to data clumps between tuples of all files, $0.5*n*(n-1)$ file tuples need to uploaded so that many requests will needed. However, this can help ChatGPT to focus on a smaller part of the software project to analyze in order to find more data clumps.
\end{description}
\end{comment}

\subsection{Results of the experiments}\label{sec:initial_experiments}

The results of the experiments are now described. Each parameter configuration is tested three times. While more repetitions would be better for more valid results, this is nevertheless a small experiment whose goal is to find good and bad ChatGPT configurations. 

The results are evaluated by comparing them to a ground truth. In the case of the data clump detection-only experiment, it is determined how often a detected data clump appears in the ground truth (specificity) and how often a ground truth data clump is detected (sensitivity).

In the data clump detection and refactoring experiment (3), the comparison to a ground truth is more difficult because ChatGPT has more flexibility with regard to naming, formatting, or other refactoring activities. To overcome this, a heuristic is used that count errors. Initially it is tested whether the refactored source compiles. If it does not compile, ChatGPT has made a huge error and the comparison fails.

If it does compile, for each method and class, the best fitting method or class from the ground truth is determined and then tested how close it is to the ground truth. For instance, each refactored method should now have only one parameter. it is also tested whether all (two) extracted classes exists and whether  they have all required fields. This approach has some disadvantages. Especially if ChatGPT decides to refactor more than the data clumps, the finding of equivalent method or classes is more error-prone and can lead to more negative results. 

As the the size of the experiment is limited, only a qualitative analysis of the results is performed. The full results of the experiments are available int the digital appendix. 

The following are the main conclusion of the experiment:
\begin{itemize}
\item \textbf{Regarding data clump detection}
\begin{itemize}
    \item GPT-4 leads to much better results than GPT-3
    \item GPT-4 and GPT-
    3 have better results if examples are provided instead of a data clump definition. However, the sensitivity of GPT-3 is better if definitions are used
    \item A high temperature is better for GPT-3. A high temperature for GPT-4 leads to reduced specificity but increased sensitivity. 
    \item While some results are very good(e-~g. 100 percent), the median and mean is often worse and seldom reach 50 percent. 
    \item Providing pairs of file leads to better result, the improvement is nevertheless not as strong as expected. 
    \item The daily limit of token for ChatGPT is a major issue during the experiments because it was frequently reached and therefore, the experiment  must be conducted over a longer period of time. 
\end{itemize}
  \item \textbf{Regarding detecting and refactoring} \begin{itemize}
       \item ChatGPT sometimes produces invalid code. In most cases it forgets to add all used constructors
    \item ChatGPT can mix up the two distinct data clumps and creates only one extracted class. Nevertheless, it may also refactor all data clumps, so that the extracted class contains only the fields of one data clump but is used for both data clumps. This leads to compiler errors.
       \item In other cases, the code of the extracted classes is not returned
       \item Sometimes it performs no refactoring but returns the original code
       \item Non-refactored files are sometimes not returned. This is acceptable as this decreased the costs
       \item The combination of providing data clump definitions and ChatGPT-3.5 leads almost always to non-compilable code.
       \item A lower temperature leads to better results
 
   \end{itemize}
\end{itemize}

As already noted, the results should be interpreted cautiously because the project size is small. Nevertheless, some observations can be made:

\begin{itemize}
    \item Use examples for data clump as the instruction format as they seem to produce better results
    \item Use a high temperature which allows for more creativity and apparent better results
    \item If possible (given the higher costs) use GPT-4
    \item Whether all files or pair of files should be transferred is difficult to say and the impact is difficult to determine. 
    \item The accuracy of ChatGPT detecting and replacing data clumps at the same time is better than expected. However, there is a risk of creating non-compilable source code
    \item finding data clumps only, leads to worse accuracy
\end{itemize}
These result can be explained that detecting and refactoring data clumps is more \enquote{natural} task than just detecting data clumps. No specific format needs to be used that ChatGPT must understand and apply. This shows that combining ChatGPT with other services can work but the best results are obtained by performing the complete detection and refactoring process by ChatGPT alone. 

It is interesting that providing a data clump definition worsens the results. On explanation for this observation could be that the definition of this master thesis conflicts with other definitions known by ChatGPT or that the definition is not precise enough.

\section{Implementation of tools (excluding large language models} \label{sec:step_impl}

\subsubsection{Data Clump Doctor}

The data clump doctor introduced in section \ref{sec:data_clump_doctor} is used for detecting data clumps. As it is developed in Typescript and uses the data clump type context format discussed in section \ref{sec:data_clump_format}, it can be easily integrated into the pipeline so that the handler directly calls the relevant methods of the data clump doctor to detect data clumps. 

\subsubsection{IntellIJ}

For refactoring data clumps, the  \ac{API} from IntelliJ mentioned in section \ref{sec:psi} can be used because it allows for easy modification of the source code that does not result in faulty code. 


\begin{comment}The reasons for this are difficult to determine and the documentation is scarce, so the the PSI approach seems to be only suitable for projects created via IntelliJ or correctly initialized by IntelliJ with the required meta data. Gradle and maven projects are therefore not suitable for the full refactoring step. 
\end{comment}


The basic approach for refactoring is based on modifying references. A  reference is an information about the usage of an element. Element in this context could mean a class, a method, or a variable. Four kinds of references are relevant for data clump refactoring.

\begin{enumerate}
     \item A field is declared
    \item A variable (field or method parameter) is read from or assigned to
   
    \item A method is declared or overridden
    \item A method is called
\end{enumerate}
In many cases , IntelliJ can find all references automatically and categorize them.

However, this does not always happen. A project can be loaded wrongly so that  finding references of a method or field can lead to undefined behavior. Sometimes all references are correctly found, sometimes only a subset of the references are found, and sometimes no even no references are detected. This could be explained by invalid caches or unsupported projects although it is difficult to determine whether there are reference errors as no ground truth exists.

To solve this issue, two approaches can be considered.

In the first approach,, the \ac{LSP} as described in section \ref{sec:lsp} is used to find all references of the methods, fields, and variables that are part of the data clump. For instance, the \ac{LSP} server finds all references of a method parameter that is part of a data clump. This approach represents the pipeline idea that many tools need to work together to achieve a common goal.

The second approach uses IntelliJ alone. Since not all references can be detected, the syntax tree must be traversed to find all occurrences where a relevant reference is used. For method parameters, the search can be shortened because their scope is very limited. 

Whether such manual search is faster than the combined use of \ac{LSP} and IntelliJ is difficult to determine. On the one hand, can external services be faster because they are implemented better (i.~e. more sophisticated algorithms). On the other hand, starting two services leads to more overhead. 



Regardless of how the reference issue is solved, IntelliJ can now perform the refactoring. Depending of the category of a reference , IntelliJ needs to perform distinct step. 
\begin{enumerate}
    \item In case of a declared field, the field can be deleted because it is part of a field data clump. Now it can be determined whether the class contains already the new field that replaces the fields of the data clump.
    \item If a method is declared, IntelliJ can modify the signature of the method. This can be the original method or an overridden one. IntelliJ needs to remove the parameters that are part of the data clump and add a new method parameter which replaces the method parameters of the data clump. 
    \item If a variable is used it can be replaced by a getter or setter call. For instance if the variable \textit{var} is read, and the name of the extracted class is \textit{Object}, any reading of the variable can be replaced by  \textit{object.getVar()} where \textit{object} is a variable of type \textit{Object} and \textit{getVar} is the getter of \textit{var}. Similarly, an assignment can be replaces by the setter.
    \item If a method is used, several substeps are needed.
    \begin{enumerate}
        \item First, for each argument provided in a method call it is determined whether the argument is connected to a data clump variable (i.~e. it provides a value to a parameter that is part of a data clump) 
        \item The position of those arguments is stored and a reference to the argument is stored for further processing.
        \item Since the extracted class is known and existing, a matching constructor is determined to support all arguments to the data clump variables of the method call. 
        \item For each argument to a data clump variable, the argument is inserted into the matching position of the constructor, and the argument is deleted in the original method call. 
        \item the constructor call is added at the position of the method  where the parameter of the extracted class is expected. 
        
    \end{enumerate}
    
\end{enumerate}

This approach requires some coordination. For instance, the order of operation is important. Method and field declarations must be updated first because they are needed to perform the refactoring of variable usages and method calls successfully. 

Another aspect, where the order of the operation matters, is the hierarchy in the abstract syntax tree. Consider the assignemt \textit{x=x+1}. In an abstract syntax tree, the reading of the value \textit{x} and the addition of 1 is executed first. It is also at a deeper level of the tree than the assignment. If the assignment were to be replaced by a setter, the syntax tree of the reading expression can be out of sync because it is not linked to the original assignment. Therefore, it is vital to refactor parts of a code with higher depth in the syntax tree before parts with lower depths. 

\begin{comment}
\subsection{ChatGPT}
ChatGPT is another approach to detect data clumps as it can process code easily and report data clumps in any format the user wants. It also supports many programming languages that other tools do not provide.

However, ChatGPT has a limited context size, so processing large projects is  either too costly or simply not possible.

Giving ChatGPT the right instructions to find data clumps can also be challenging. While ChatGPT can define and find some data clumps without further context, it is better to give it a precise definition to work with. The following definition leads to good result, however it cannot be guaranteed that this will work forever:


A data clump exists
\begin{enumerate}
   
   \item if at least three fields also exists in another class
   \item if at least three fields also exists as method parameters in some method
    \item if two methods have at least three common parameters
\end{enumerate}
\end{comment}