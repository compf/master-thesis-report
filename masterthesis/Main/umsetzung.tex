
\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Implementation}\label{chapter:implementation}
\endgroup














\section{Extent of large language model integration}

A major part of this master thesis is the question how well \ac{LLM} can be integrated into the data clump  detection and refactoring process. Several extends to such integration exists which will be discussed in the following subsections. 

\subsubsection{No Large language model integration}

The most basic approach to refactor data clumps automatically is by using no \ac{LLM} as if they do not exists. This is the only working approach if the use of  \ac{LLM} is forbidden or too costly. The tools described in section \ref{sec:implementation_tools} are used to perform the detection and refactoring of data clumps. 

The only major issue is the name finding step. As outlined in section \ref{sec:pipeline_steps}, the name finding step usually requires creativity. Without a \ac{LLM}, this poses more challenges.

The simplest approach would be a simple concatenation of the data clump item identifiers. For instance, the data clump items \textit{x}, \textit{y}, and \textit{z} can be concatenated to  the identifier \textit{XYZ} or \textit{X\_Y\_Z}. The resulting names are valid but they do not give information about the purpose of the class so that their usefulness is limited.

Another approach is to use more extensive \ac{NLP} techniques that classify groups of word into a category. This master thesis however will not deal with this issue because it is a complex issues on its own.

\subsubsection{Limited large language model integration}

If a \ac{LLM} can be used but costs or resources are a significant concern, a limited inclusion approach is useful. In this scenario, the \ac{LLM} only performs small tasks like suggesting names for an extracted class, filtering detected data clumps etc. In each case, the model is not provided with the  complete source code but only specific data. so that the data size to be transmitted is more limited for larger projects. The issue here is that large language models have no context about the source code so that their suggestion can have worse quality. 

\subsubsection{Extensive large language model integration }

In this approach, a major subset of the pipeline is performed by a \ac{LLM}. For instance, the detection of data clump can be performed by a large language model, but the refactoring is performed by a traditional method. 

This results in higher costs and resource usages as the software project must be transmitted to the \ac{LLM} but uses the creativity and extensibility of a \ac{LLM}

\subsubsection{Complete integration of large language models}

In this method, the large language model performs most steps of the pipeline. it should be noted that some steps like code obtaining or file filtering are too trivial or too technical to be performed by a \ac{LLM} so that some smaller parts of the pipeline should be executed with other tools. 

This method uses the full creativity and adaptability of a \ac{LLM}, so that data clump detection are seamlessly combined. Nevertheless, the costs and resource consumption must be considered. In the case of a one-shot-prompting, the model detects and refactors the data clumps in one step. As already noted, this can lead to worse results because to many steps are combined. However, the costs are similar to the  extensive integration approach because the source code needs to be transmitted only once. 

In a multi-shot approach, the model is asked to find data clumps and only then asked to refactor them. This requires that the source code be sent multiple times as a \ac{LLM} is often stateless. 

\section{Prompt engineering ChatGPT for data clump detection and refactoring}\label{sec:prompt_engineering_impl}

As outlined in  section \ref{sec:llm_challenges}, using \ac{LLM} can result in some challenges for refactoring code. Therefore, experiments are needed to test which input can lead to good results for finding or refactoring data clumps or both. These experiments should not be seen as a replacement for the full evaluation discussed in chapter \ref{chapter:eval}, but a mandatory prerequisite to save costs since the full evaluations needs to be performed on a more numerous set of larger projects.

The following scenarios of ChatGPT are addressed:
\begin{enumerate}
    \item ChatGPT only finds data clumps and report them. The refactoring can be done by another tool.
    \item ChatGPT performs the refactoring. It is given the data clumps detected by other tools. 
    \item ChatGPT searches for and refactors immediately all data clumps
\end{enumerate}

The scenarios reflect the idea of ChatGPT being a service that provides functionality for one or more steps in the pipeline. It is not always necessary for ChatGPT to perform all steps (i.~e.  data clump detection and refactoring). For instance, data clump detection and refactoring can be done with one query, but such a general query can lead to worse  results. Splitting the query into a detection and refactoring part can improve the result(few-shots), however this requires multiple transmission and therefore higher costs. 

On the other hand, reading from and outputting into the format specified in section \ref{sec:data_clump_format} can be problematic as it may be difficult to instruct ChatGPT to understand the format correctly. This, however, is necessary to guarantee compatibility with other tools and services.  As an alternative, ChatGPT could be instructed to use a different format and the conversion happens in the handler. Nevertheless, a format must be chosen that ChatGPT can interpret. 

The input project is a modified version of the source code discussed in \ref{sec:data_clump_def} which is fairly small but include at least three data clumps. Table \ref{tbl:javaTest_data_clumps} lists all data clumps which exists in this project. The names in the \textit{From} or \textit{To} column are in the format \textit{className.methodName} or \textit{className} depending on whether it is a field-to-field \textit{(f2f} or parameter-to-parameter \textit{(p2p)} data clump.

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c|c}
         Id &From & To & Type & Data clump variables  \\\hline
         1 & MathStuff.printLength & MathStuff.printSum & p2p & x,y,z\\\hline
        2 & MathStuff.printSum & MathSum.printMax & p2p & x,y,z\\\hline
        3 & MathStuff.printLength & MathSum.printMax & p2p & x,y,z\\\hline
        4 & MathStuff & MathUser & f2f & sign, mantissa, exponent\\\hline
        5 & Library & MathUser & f2f & sign, mantissa, exponent\\\hline
        6 & MathStuff & Library & f2f & sign, mantissa, exponent\\\hline
    \end{tabular}
    \caption{All data clumps in the test projects}
    \label{tbl:javaTest_data_clumps}
\end{table}

Integrating large language models into the pipeline requires special attention. As the \ac{LLM} is a special service that resides outside of the domain of the pipeline, all required information must be submitted to it and any context not provided to the model is not available for consideration. The way such context is worded or submitted to the model has a great impact on the quality of the results.

Additionally, there are many models available that have been trained on general or fine-tuned  tasks. Selecting the right model for a specific task is not trivial. 

These parameters can be subsumed as the configuration of the \ac{LLM} as they should be changeable by the user. 

Therefore, experiments should be conducted to ascertain what model is useful for data clump detection and refactoring and how the information for the model is structured.  

Four groups of configuration options can be distinguished:
\begin{enumerate}
    \item The prompt including the manner the term \enquote{data clumps} is conveyed to the model
    \item How to process the source code of the project under analysis to the \ac{LLM}
    \item The manner the LLM is prompted to output its results
    \item model specific parameters like the choice of the model itself and parameters like the temperature
\end{enumerate}

The first three options are inter-related. Changing the type of input the LLM is confronted with or the output format requires a change of the prompt in order for the LLM to understand what to do. The template system discussed in section \ref{sec:llm_msg_structure} simplifies the prompt building because the prompts for explaining the input format, output format, and the other relevant context can be separated.

The several configuration options are as follows















\begin{comment}
In this section, the results of some of these experiments will be discussed.  As repeatedly noted, these experiments are not always reproducible and therefore are only to some extent helpful. In all cases, ChatGPT is asked to output in \ac{JSON} to have a consistent format.

In each experiment, different parameters of the input are adjusted. Each these parameters has an influence of the quality of the output:

\begin{itemize}
    \item Instruction
    \item Model
    \item Data format
    \item Data size
    \item Temperature
\end{itemize}
\end{comment}
\subsubsection{Instruction}\label{sec:llm_instruction}
The instruction is a main part of a prompt to ChatGPT or other \ac{LLM}. It tells the  \ac{LLM} what to do and how to respond. As a result, varying the instruction is an important part of prompt engineering. \newline



Initially, it is useful whether ChatGPT can find data clumps without giving a specific definition. This means ChatGPT needs to deduce the definition of a data clump from its trained models which might not be the same as the definition used in section \ref{sec:data_clump_def}. For instance, data clumps are defined in this master thesis to have at least three common variables while other sources require at least four common variables. 
\begin{comment}
Nevertheless, the results are fairly good. ChatGPT ignores the inheritance between \textit{MathStuff} and \textit{BetterMathStuff} and does not output a data clump for each method in \textit{MathStuff}
\end{comment}


In another approach, ChatGPT is provided a specific definition of data clumps and asked to apply this definition on the project. Therefore, a precise definition is elementary for this approach.

The following query was used:
A data clump exists if
\begin{enumerate}
\item  Two methods (in the same or in different classes) have at least 3 common parameters
    and one of those methods does not override the other, 
\item At least three fields in a class are common with the parameters of a method (in the same or in a different class), or 
\item Two different classes have at least three common fields
\end{enumerate}


Another approach for including ChatGPT for data clump refactoring is example-based. Multiple examples of data clumps and how they can be refactored are presented to ChatGPT. In order to avoid any bias, each type of data clump is equally represented and field identifiers or parameters field identifiers are unique across the whole example. This results in a larger request size and thusly more costs. Nevertheless, if larger projects were to be used the size of the examples becomes negligible so that this approach has its reasons. The examples provided to ChatGPT can be found in the digital appendix under the path \enquote{data\_clump\_examples.java} 

\subsubsection{Model}
The model is a parameter for ChatGPT that controls the quality, performance, and cost of processing a query and responding to it. 

OpenAI provides multiple models for user of its API. Each  ChatGPT model  is based on the same original model but is fine-tuned to optimize for a specific task. For this master thesis, only  the  models \enquote{gpt-4-1106-preview} and \enquote{gpt-3.5-turbo-1106} are relevant. Both models support a \ac{JSON}-only mode, which means that any response by the model will be valid \ac{JSON} instead of natural language, so it easier to parse and process. \cite{chatgpt_new_models}

The model {gpt-4-1106-preview} is also called \textit{GPT-4 Turbo}. It can process 128,000 tokens at once and has a knowledge cutoff of April 2023. \cite{chatgpt_new_models}

The model \enquote{gpt-3.5-turbo-1106} has a context window size of 16,000. The knowledge cutoff is September 2021. \cite{chatgpt_new_models}


In general, GPT-4-Turbo has a better quality because of the larger context window size and more up-to-date information. While the more recent cutoff date might not be that important for data clump detection and refactoring, the larger context window size means that larger amount of code can be transmitted to ChatGPT at once. So, larger projects can be refactored more easily.  \cite{chatgpt_new_models}

On the other hand, the costs per 1000 tokens is  \$0.003 for GPT-3-Turbo whereas the costs for GPT-4 is \$0.01 for 1000 tokens. As a result, the GPT-4-Turbo is thrice as expensive as GPT-3 which can be a reason to use GPT-3. \cite{chatgpt_new_models}

Additionally, there are  stricter limits for using GPT-4. For instance, only 500,000 tokens can be sent per day, and 150,000 tokens can be sent per minute. In contrast, GPT-3-Turbo has  lower values for token per minutes but no  token per day limit.  Â¸\cite{chatgpt_limits}

\subsubsection{Output format}
 Similar to the input format, a model can also be configured to format its output. If the content of full files is provided, the model can  apply the required refactoring on the file and return the refactored files. This however requires transmitting the full file content nearly twice which induces costs and resource usage. It can lead to small errors if parts of the source code that have nothing to do with data clumps are changed even though there was no reason to do so. The \ac{LLM} being a probabilistic model cannot prevent such small mistakes. Additionally some \acs{LLM} have a maximal output size which prevents them from outputting large files or many small files.

 An alternative approach is similar to the related input approach. The model is instructed only to return those lines that have been changed. These changes can be applied to the file by a pipeline handler without the help of the \ac{LLM}. However, the model must provide some context of where the changes should be applied. For instance, the model can return the line number for the changes. This can cause problems if the  line number must be determined by the model because the line number might be not up-to-date if previous changes created new new-lines, or the LLM starts counting the lines at the wrong location.

 Alternatively, the LLM could return the old content of each line. Then every occurrence of the old content can be replaced by the new content. Based on heuristic assumption that if the text \textit{t} appears multiple times and is suggested to be changed by the \ac{LLM}, it can be replaced everywhere in this file. 

 Combining both approaches further helps to increase the accuracy, as the handler can check whether the content at the line number matches the old content returned by the LLM.  
 

\subsubsection{Data format}

After creating a instruction, ChatGPT cannot do much because it does not have access to the source code which must be provided to ChatGPT. Therefore, the next part of the prompt is some representation of the source code

The two approaches tested are providing the complete source code and providing the \ac{AST}.

Providing the complete source code is the most natural manner to give ChatGPT the relevant information. Since ChatGPT is language-agnostic, it can deal with many programming languages and therefore is able to process the source code files to find data clumps. However, source code files can be large as they can contain comments, method bodies or other parts of the programming language that are not relevant for detecting data clumps. This leads to higher costs.

On the other hand, the \ac{AST} can be provided. The \ac{AST} only contains a reduced structure of the source code and will therefore cost not as much. However, the \ac{AST} is usually not available but must be produced by other tools or ChatGPT (which means the source code has to be sent to the \ac{LLM} nevertheless). Additionally, there is no fixed format for the \ac{AST} which might complicate parsing and processing the \ac{AST} or the results by ChatGPT.


In case ChatGPT should also perform the refactoring, the \ac{AST} becomes not as useful. Refactoring data clumps always include updating method calls or variable usages which might not be available in the \ac{AST}. One could submit the \ac{AST} and the source code so that ChatGPT uses the \ac{AST} for detecting and the source code for refactoring, but this creates more costs and whether it may worsen the quality is an open question as ChatGPT needs to process more data and create a connection between source code and \ac{AST}.
\subsubsection{Data size}

The size of the data is the amount of data that is submitted during a single request. Here three approaches are tested.

In the first approach, the full project is submitted to ChatGPT to find data clump while in the second pairs of files are transmitted which each pair of file being a separate independent request.

The advantage of the first approach is that only one transaction needs to be done, and ChatGPT can find many data clumps in one shot. ChatGPT can therefore better understand the structure and provide better suggested name or find data clumps where the identifier names of the data clump variables are not identical. However,  a major issue is that many projects are too large, and therefore ChatGPT cannot provide the advantages just described.

In the second approach, pair of files are transmitted to ChatGPT with an instruction mentioned in \ref{sec:llm_instruction}
This means that every request only contains the instruction and the content of the two files. Therefore, ChatGPT can only find data clumps only in those two files and in no others as it has no memory of previous prompts. One might think that this approach is cheaper, however, if one still wants to find data clump in the whole software project, he has to to transmit $\binom{n}{2}=0.5*n*(n-1)$ where $n$ is the number of relevant files. Therefore, the cost can be much higher than transmitting each file only once as in the first approach.

Additionally, each file could be sent again separated in order to find data clumps that exists in a single file and might not be detected if two files are provided so that the total number of files would be $0.5*n*(n-1)+n$.


Another approach is not to sent full file contents but only code snippets that contain source code. Each snippet of code so provided is related to a data clump. It can be the method declaration of a data clump an associated method call, a field definition, or a variable usage. By providing only these lines, many irrelevant lines of code can be omitted. The caveat of this idea that the data clumps  and their usage must be known already in order to find those snippets of code. Therefore, this method only works in limited circumstances.

\subsubsection{Temperature}
As outlined in section \ref{sec:chatgpt}, the temperature is a significant parameter that controls the randomness of the \ac{LLM} output. A high temperature indicates that the response by ChatGPT is more random and unpredictable, while a low temperature results in more consistent responses.

Hence, a higher temperature can help ChatGPT to be more creative so that it may find data clumps that it would usually not detect. However, it can also lead to a lower specificity because it may flag wrong methods or fields as part of a data clump because of the increased unpredictability. 

As a result, adjusting the temperature parameter is a significant part for the small experiment. Here, two values will be tested. A low temperature of 0.1 and a hight value of 0.9. While these are two extremas and not a middle ground, they can give a good approximation of whether thelow or high temperatures are better for data clump detection. 

\begin{comment}
\subsection{Requesting follow-ups}

As outlined in section \ref{sec:chain of thought},  one prompt might no be enough to refactor data clumps. This might also apply for the simpler task of detecting data clumps.

Instead of expecting that a single request will detect all data clumps, another approach would be to repeat the same requests multiple times and aggregate the results. These follow-up prompts result in ChatGPT to re-analyze the code so that more data clumps can be found. 

The issue  with follow-up prompts is how often should a prompt be repeated. Each prompt repeat requires that the conversation history to be sent again because ChatGPT is stateless. This leads to higher resource usage and costs. There are multiple approach to implement follow up.

\begin{description}
\item[Full project follow-up] The whole project (all relevant files) is sent to  ChatGPT again with an instruction to find more data clumps. As a result, more data is sent per  request while the number of request can be reduced. As a  result, ChatGPt might find additional data clumps that it has not found during the previous prompts.
\item [Tuple-based follow-up] A tuple of two files in the project (i.~e. the content of those files) is sent again to ChatGPT with the instruction to find more data clumps. If the goal is to data clumps between tuples of all files, $0.5*n*(n-1)$ file tuples need to uploaded so that many requests will needed. However, this can help ChatGPT to focus on a smaller part of the software project to analyze in order to find more data clumps.
\end{description}
\end{comment}

\subsection{Results of the experiments}\label{sec:initial_experiments}

The results of the experiments are now described. Each parameter configuration is tested three times. While more repetitions would be better for more valid results, this is nevertheless a small experiment whose goal is to find good and bad ChatGPT configurations. 

The results are evaluated by comparing them to a ground truth. In the case of the data clump detection-only experiment, it is determined how often a detected data clump appears in the ground truth (specificity) and how often a ground truth data clump is detected (sensitivity).

In the data clump detection and refactoring experiment (3), the comparison to a ground truth is more difficult because ChatGPT has more flexibility with regard to naming, formatting, or other refactoring activities. To overcome this, a heuristic is used that count errors. Initially it is tested whether the refactored source compiles. If it does not compile, ChatGPT has made a huge error and the comparison fails.

If it does compile, for each method and class, the best fitting method or class from the ground truth is determined and then tested how close it is to the ground truth. For instance, each refactored method should now have only one parameter. it is also tested whether all (two) extracted classes exists and whether  they have all required fields. This approach has some disadvantages. Especially if ChatGPT decides to refactor more than the data clumps, the finding of equivalent method or classes is more error-prone and can lead to more negative results. 

As the the size of the experiment is limited, only a qualitative analysis of the results is performed. The full results of the experiments are available int the digital appendix. 

The following are the main conclusion of the experiment:
\begin{itemize}
\item \textbf{Regarding data clump detection}
\begin{itemize}
    \item GPT-4 leads to much better results than GPT-3
    \item GPT-4 and GPT-
    3 have better results if examples are provided instead of a data clump definition. However, the sensitivity of GPT-3 is better if definitions are used
    \item A high temperature is better for GPT-3. A high temperature for GPT-4 leads to reduced specificity but increased sensitivity. 
    \item While some results are very good(e-~g. 100 percent), the median and mean is often worse and seldom reach 50 percent. 
    \item Providing pairs of file leads to better result, the improvement is nevertheless not as strong as expected. 
    \item The daily limit of token for ChatGPT is a major issue during the experiments because it was frequently reached and therefore, the experiment  must be conducted over a longer period of time. 
\end{itemize}
  \item \textbf{Regarding detecting and refactoring} \begin{itemize}
       \item ChatGPT sometimes produces invalid code. In most cases it forgets to add all used constructors
    \item ChatGPT can mix up the two distinct data clumps and creates only one extracted class. Nevertheless, it may also refactor all data clumps, so that the extracted class contains only the fields of one data clump but is used for both data clumps. This leads to compiler errors.
       \item In other cases, the code of the extracted classes is not returned
       \item Sometimes it performs no refactoring but returns the original code
       \item Non-refactored files are sometimes not returned. This is acceptable as this decreased the costs
       \item The combination of providing data clump definitions and ChatGPT-3.5 leads almost always to non-compilable code.
       \item A lower temperature leads to better results
 
   \end{itemize}
\end{itemize}

As already noted, the results should be interpreted cautiously because the project size is small. Nevertheless, some observations can be made:

\begin{itemize}
    \item Use examples for data clump as the instruction format as they seem to produce better results
    \item Use a high temperature which allows for more creativity and apparent better results
    \item If possible (given the higher costs) use GPT-4
    \item Whether all files or pair of files should be transferred is difficult to say and the impact is difficult to determine. 
    \item The accuracy of ChatGPT detecting and replacing data clumps at the same time is better than expected. However, there is a risk of creating non-compilable source code
    \item finding data clumps only, leads to worse accuracy
\end{itemize}
These result can be explained that detecting and refactoring data clumps is more \enquote{natural} task than just detecting data clumps. No specific format needs to be used that ChatGPT must understand and apply. This shows that combining ChatGPT with other services can work but the best results are obtained by performing the complete detection and refactoring process by ChatGPT alone. 

It is interesting that providing a data clump definition worsens the results. On explanation for this observation could be that the definition of this master thesis conflicts with other definitions known by ChatGPT or that the definition is not precise enough.


\begin{comment}
\subsection{ChatGPT}
ChatGPT is another approach to detect data clumps as it can process code easily and report data clumps in any format the user wants. It also supports many programming languages that other tools do not provide.

However, ChatGPT has a limited context size, so processing large projects is  either too costly or simply not possible.

Giving ChatGPT the right instructions to find data clumps can also be challenging. While ChatGPT can define and find some data clumps without further context, it is better to give it a precise definition to work with. The following definition leads to good result, however it cannot be guaranteed that this will work forever:


A data clump exists
\begin{enumerate}
   
   \item if at least three fields also exists in another class
   \item if at least three fields also exists as method parameters in some method
    \item if two methods have at least three common parameters
\end{enumerate}
\end{comment}