
In this chapter, the full-scale evaluation is discussed.
\section{Goal of the evaluation}
While the small-scale evaluation has shown that \acs{LLM} can be detect and refactor data clumps, the real usability is still hard to measure. Only two projects were tested and only a statistical analysis of the sensitivity, specificity and accuracy  is performed so that other factors are not considered.

Others factors are more subjective and cannot be tested with the method described in section \ref{sec:initial_experiments}. For instance, while data clumps can be detected with the DataClumpDoctor, each detected data clump might not be a data clump for every developer. For instance, developers may disagree about the required number of data clump items (three in this master thesis). Additionally, developers who have knowledge of the structure of a project can better determine whether a group of variables  constitutes a data clump. In some cases, they have written the method or class themselves and have introduced the data clump on purpose because they could not find a proper name for the extracted class, or the disadvantages discussed in section \ref{sec:data_clump_not_refactor} outweigh the advantages of introducing a code smell. 

Even if developers agree that a detected data clump constitutes a data clump  in their opinion, there could be arguments against refactoring them. These reasons include the points  discussed in section \ref{sec:data_clump_not_refactor}.Additionally, there are more grounds that are not specific to data clumps but code smells in general. For instance, the location of the code smell could be currently under heavy editing so that it might not be certain whether the methods or classes will exist in the long-term. On the other hand, parts of the source code that have been untouched for longer times, could be too risky to refactor as the knowledge on how the source code behaves or is structured is limited, so that developers would not refactor them even if it might make sense. 

As a result, asking the opinion and ideas of developers on whether data clump refactoring is justified and can be performed by a \ac{LLM} is the basis of this evaluation .
\section{Evaluation setup}
To facilitate the evaluation, GitHub projects that uses Java  were selected. These were forked and analyzed by the DataClumpDetector. a particularly selected data clump was then refactored by a combination of tools. For each GitHub project, a pull request was created to as the contributors  of the project to merge the refactoring in their project. The contributors were asked to fill out a survey form and submit any feedback relating to the refactoring which was collected. 

\subsection{GitHub project selection}
The projects are selected from the trending page of GitHub. This page list GitHub projects that have gained attention in a specific time period. For instance, they were forked comparatively often or received more stars in comparison, but the exact criteria is difficult to determine. Each project was tested on the following criteria
\begin{enumerate}
    \item Whether the project contains at least  10,000 \ac{LOC} of Java
        \item Whether the project uses has at least 100 stars
    \item Whether the last closed pull request is newer than 30 days

\end{enumerate}

With the first two criteria,  only larger projects are considered. This increases heuristically the chance of having a higher number of data clumps, and also the chance of getting more developers to respond to the survey as larger projects tend to be maintained by more people. The third criteria also influences the number of contributors and their willingness to work on the project. Only if pull request are frequently considered and closed (which does not necessarily mean that they are merged), the project is considered active enough. 


\subsection{Criteria for selecting data clumps}

For each selected project, one data clump was chosen. For each data clump detected by the \textit{DataClumpDoctor} the following scores were calculated:
\begin{itemize}
    \item The number of occurrences of data clump. For instance, if there is a data clump with the variables \textit{x}, \textit{y}, and  \textit{z}, it is counted how many methods have these parameters and how many classes have these fields

     \item The number of data clump items. For instance, the \enquote{xyz}-data-clump has three data clump items.
     
    \item The number of affected files. Every file that is affected by a data clump. This includes the location of the data clump and also the files where methods and fields that are part of the data clump are references because these files must also be changed if the data clump is refactored. 
    
\end{itemize}

From these scores, for each data clump a weighted sum was calculated. For the first two metrics, either the weight is 100 or 1. In these metrics, data clumps that occur more often or are larger are scored better so that they are more likely to be refactored because the code size can be reduced more strongly.  For the last metric, the weights were -100 or 0. A zero or negative weight was chosen because a large number of affected files can be obstacle for  refactoring as many areas of the codebase may need to be changed. If an \ac{LLM} performs the refactoring, all those files must be transmitted to the model which increases costs and resource usage. Even if \ac{LLM} is involved only marginally, it is more likely that the less files are changed the more contributors on GitHub are willing to give feedback.

For each combination of the weights described above, the five most-scored data clumps were manually reviewed. After that, one particular data clumps was  chosen from all combination. The criteria for selecting this data clump was more subjective as it is difficult to determine which data clump would be  the most relevant to refactor. The following considerations influence the filtering process.

\begin{itemize}
    \item Avoid data clumps that affect abstract classes or interfaces as they should not be changed.
    \item Avoid data clumps that only affect source code for unit testing. While refactoring source code for tests is very important too, data clumps in the main code are regarded more important for the purpose of this thesis.
    \item Whether the combination of the parameters would make logically sense. for instance, are the fields or parameters in similar domains or are they used together. 
    \item In case of fields, potentials issues that occur if dependency injection is used because moving these fields require special attention that is outside the scope of this master thesis
\end{itemize}
All these criteria are more guidelines than strict requirement to allow flexibility 
After considering all of these criteria, one final data clump was selected.

\subsection{Assignment to a category}

After a data clump is selected, the next step is to assign the project to a category to determine the extent \acs{LLM} are used to find and refactor data clumps.  The following six categories were used as each category is subdivided for each LLM. 

\begin{itemize}
    \item ChatGPT-4 / Codellama performs the detection and refactoring
    \item ChatGPT / Codellama performs the refactoring but the detected data clumps via the \textit{DataClumpDoctor}
    \item ChatGPT / Codellama only suggest a suitable name, and the refactoring is performed via \ac{PSI}
\end{itemize}

The following considerations influences the decision which category to assign each data clump:
\begin{itemize}
   \item	The number of affected files. The more affected files, the more likely only the name suggestion category is used. 
\item	The existence of special codestyle guidelines, for instance naming convention, documentation requirements etc. these should be handled better by a LLM
\item	The existence of other code smells that could be fixed by the LLM even though it is not their primary task.
\item Balancing: Each category should be represented equally in the pull request
\end{itemize}

After the analysis and assignment phase concludes, a refactoring phase is performed. If an \ac{LLM} is asked to execute the actual refactoring, the experiment is conducted at least ten times with varying temperatures and the definition-based instruction as this offers a suitable balance between using the limited context window and the quality of the results. The result of each experiment was saved without modifying the actual project, and a final candidate for refactoring was chosen. 

Based on the suggestion by the LLM, the refactoring was applied by inserting the refactoring suggestions at the correct position manually. This intermediate state was tested on whether it contains build errors.  