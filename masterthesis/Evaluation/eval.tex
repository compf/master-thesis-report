
In this chapter, the full-scale evaluation is discussed.
\section{Goal of the evaluation}
While the small-scale evaluation has shown that \acs{LLM} can be detect and refactor data clumps, the real usability is still hard to measure. Only two projects were tested and only a statistical analysis of the sensitivity, specificity and accuracy  is performed so that other factors are not considered.

Others factors are more subjective and cannot be tested  easily.  For instance, while data clumps can be detected with the DataClumpDoctor, each detected data clump might not be a data clump for every developer. For instance, developers may disagree about the required number of data clump items (three in this master thesis). Additionally, developers who have knowledge of the structure of a project can better determine whether a group of variables  constitutes a data clump. In some cases, they have written the method or class themselves and have introduced the data clump on purpose because they could not find a proper name for the extracted class, or the disadvantages discussed in section \ref{sec:data_clump_not_refactor} outweigh the advantages of introducing a code smell. 

Even if developers agree that a detected data clump constitutes a data clump  in their opinion, there could be arguments against refactoring them. These reasons include the points  discussed in section \ref{sec:data_clump_not_refactor}.Additionally, there are more grounds that are not specific to data clumps but code smells in general. For instance, the location of the code smell could be currently under heavy editing so that it might not be certain whether the methods or classes will exist in the long-term. On the other hand, parts of the source code that have been untouched for longer times, could be too risky to refactor as the knowledge on how the source code behaves or is structured is limited, so that developers would not refactor them even if it might make sense. 

As a result, asking the opinion and ideas of developers on whether data clump refactoring is justified and can be performed by a \ac{LLM} is the basis of this evaluation .
\section{Evaluation setup}
To facilitate the evaluation, GitHub projects that uses Java  were selected. These were forked and analyzed by the DataClumpDetector. a particularly selected data clump was then refactored by a combination of tools. For each GitHub project, a pull request was created to as the contributors  of the project to merge the refactoring in their project. The contributors were asked to fill out a survey form and submit any feedback relating to the refactoring which was collected. 

\subsection{GitHub project selection}
The projects are selected from the trending page of GitHub. This page list GitHub projects that have gained attention in a specific time period. For instance, they were forked comparatively often or received more stars in comparison, but the exact criteria is difficult to determine. Each project was tested on the following criteria
\begin{enumerate}
    \item Whether the project contains at least  10,000 \ac{LOC} of Java
        \item Whether the project uses has at least 100 stars
    \item Whether the last closed pull request is newer than 30 days

\end{enumerate}

With the first two criteria,  only larger projects are considered. This increases heuristically the chance of having a higher number of data clumps, and also the chance of getting more developers to respond to the survey as larger projects tend to be maintained by more people. The third criteria also influences the number of contributors and their willingness to work on the project. Only if pull request are frequently considered and closed (which does not necessarily mean that they are merged), the project is considered active enough. 


\subsection{Criteria for selecting data clumps}

For each selected project, one data clump was chosen. Two selection approaches were used. 
In the first approach, a data clump was chosen based on a weighted combination of metrics to identify an \enquote{important} data clump. These metrics are:

\begin{itemize}
    \item The number of occurrences of data clump. For instance, if there is a data clump with the variables \textit{x}, \textit{y}, and  \textit{z}, it is counted how many methods have these parameters and how many classes have these fields

     \item The number of data clump items. For instance, the \enquote{xyz}-data-clump has three data clump items.
     
    \item The number of affected files. Every file that is affected by a data clump. This includes the location of the data clump and also the files where methods and fields that are part of the data clump are references because these files must also be changed if the data clump is refactored. 
    
\end{itemize}

From these scores, for each data clump a weighted sum was calculated. For the first two metrics, either the weight is 100 or 1. In these metrics, data clumps that occur more often or are larger are scored better so that they are more likely to be refactored because the code size can be reduced more strongly.  For the last metric, the weights were -100 or 0. A zero or negative weight was chosen because a large number of affected files can be obstacle for  refactoring as many areas of the codebase may need to be changed. If an \ac{LLM} performs the refactoring, all those files must be transmitted to the model which increases costs and resource usage. Even if \ac{LLM} is involved only marginally, it is more likely that the less files are changed the more contributors on GitHub are willing to give feedback.

For each combination of the weights described above, the five most-scored data clumps were manually reviewed. After that, one particular data clumps was  chosen from all combination. The criteria for selecting this data clump was more subjective as it is difficult to determine which data clump would be  the most relevant to refactor. The following considerations influence the filtering process.

\begin{itemize}
    \item Avoid data clumps that affect abstract classes or interfaces as they should not be changed.
    \item Avoid data clumps that only affect source code for unit testing. While refactoring source code for tests is very important too, data clumps in the main code are regarded more important for the purpose of this thesis.
    \item Whether the combination of the parameters would make logically sense. for instance, are the fields or parameters in similar domains or are they used together. 
    \item In case of fields, potentials issues that occur if dependency injection is used because moving these fields require special attention that is outside the scope of this master thesis
\end{itemize}
All these criteria are more guidelines than strict requirement to allow flexibility 
After considering all of these criteria, one final data clump was selected.

\subsection{Assignment to a category}

After a data clump is selected, the next step is to assign the project to a category to determine the extent \acs{LLM} are used to find and refactor data clumps. 

Two categories are to be distinguished which are explained in the following subsections:

\subsubsection{Full refactoring by ChatGPT}
In this method, ChatGPT performs the refactoring completely. Because transmitting whole GitHub projects would infeasible, the DataClumpDoctor was used to detect the previously selected data clump and obtain all lines of interests. A neighborhood of 5 was used so that 5 lines below and 5 lines above each location of interest was transmitted. 

Then, ChatGPT is instructed to refactor all data clumps in the provided lines of interest. This instruction was repeated at least ten times, in each time the context of ChatGPT was cleared so it didn't know its previous answers. 

From these ten proposal, one proposal was chosen that describes refactoring data clumps most acurately. For instance, the extracted class is valid, most usages of the data clump items are updated and all method signatures are refactored (if applicable). Generating multiple proposals in necessary because not every proposal will be correct.

\subsubsection{Refactoring by IntelliJ}

The second approach for refactoring was via IntelliJ. In this case, ChatGPT only suggest as suitable name for the extracted class, but is otherwise not involved in the refactoring. Instead, IntelliJ, performs all refactoring in the manner described in section  \ref{sec:intellij_refactoring} is applied. This results in a very consistent refactoring without any creativity. Hence, this refactoring needs only to be executed once and the first proposal can be selected immediately. 

After selecting a proposal, the proposal is applied and saved on a separate branch.

Afterwards, the proposal might not be fully correct. For instance, there might none-updated method calls, missing semicolons etc. An additional problem occurs if codestyle tools like SpotBugs or Checkstyle are employed. If the refactoring by the \ac{LLM} does not conform to the required codestyle, the code might not compile because the developers of the project force a certain style. All these are issues that can still arise and require additional handling. Therefore, a manual correction step is performed. The project is  manually changed in such a way that it fully compiles. However, no creative refactoring is performed. For instance, if one part of the source code was not refactored, it was refactored like another part regardless of whether another refactoring might have make more sense. This reduces human intervention to a minimum and ensures that the creative part of the refactoring is done by the \ac{LLM}. 

As soon as this manual refactoring finished and the program compiles, the changes were squashed into one commit and a pull request was created in the respective repository. In this pull request, the maintainers were described the purpose of this pull request and  the definition of data clumps used in this master thesis. They were asked to give feedback by filling out a feedback form or by giving feedback via GitHub comments under the respective pull request. It was explicitly stressed that rejecting the pull request would not be perceived negatively. 

The feedback by the form and comments were collected and evaluated as described in subsection ?
