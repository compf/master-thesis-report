\begingroup
\renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
\renewcommand{\clearpage}{}
\chapter{Konzeption}\label{chapter_conception}
\endgroup
Im Folgenden wird ein Konzept für die Implementierung des Tools vorgestellt, das die Ziele der Bachelorarbeit erfüllen soll. Um diese Ziele zu erreichen, müssen zuerst die Dateien geparst werden, was in Kapitel \ref{chapter:parsing} besprochen wird. Im darauffolgenden Kapitel \ref{chapter:metric_conception} wird erläutert, wie die extrahierten Informationen von den Metriken verarbeitet werden. 


\hfill
\section{Parsing von Dateien}\label{chapter:parsing}
Eine Quellcodedatei besteht zunächst aus reinem Text und bietet daher nur indirekt Einblick in die innere Struktur. Damit eine Bewertung der Dokumentationsqualität möglich ist, müssen bestimmte Strukturen aus einer Datei gelesen werden, die für die Dokumentation wichtig sind. Andere Strukturen können dabei ignoriert werden. Für die Bewertung der Dokumentation sind nur wenige Bestandteile relevant. Beispielsweise sind alle Conditional-Branches (wie z.~B.  For-Schleifen und If-Verzweigungen) und viele andere Komponenten in Methodenrümpfen nicht relevant, da diese nur mit normalen Kommentaren und nicht mit Javadoc kommentiert werden (sie werden dennoch unstrukturiert als Zeichenkette gespeichert, damit Metriken diese Information eventuell nutzen können). Aus diesem Grund müssen die notwendigen Informationen extrahiert werden. Zudem ist es ein Ziel der Arbeit, eine Erweiterbarkeit auf andere objektorientierte Programmiersprachen zu ermöglichen. Daher müssen die Informationen in ein abstraktes Format gebracht werden, welches eine gute Annäherung für die meisten objektorientierten Programmiersprachen ist. Beispielsweise unterscheiden sich die Zugriffsmodifizierer vieler Programmiersprachen, sodass eine einheitliche Schnittstelle schwer umsetzbar ist. Daher enthält die abstrakte Repräsentation nur Informationen darüber, ob eine Komponente als öffentlich markiert ist. Dies ist sinnvoll, da öffentliche Komponenten als Teil der öffentlichen Schnittstelle eher dokumentiert werden sollten als nicht öffentliche und eine weiter gehende Differenzierung kaum Vorteile bietet. In einigen Programmiersprachen wie z. B. Python gibt es keine expliziten öffentlichen Komponenten, jedoch existieren de facto Standards für Bezeichner, sodass beispielsweise private Komponenten zwei Unterstriche als Präfix haben.

Außerdem werden in der abstrakten Repräsentation die Vererbung etwas vereinfacht dargestellt, indem nicht zwischen Basisklassen und Schnittstellen unterschieden wird, da es auch hier Unterschiede zwischen Programmiersprachen gibt. Zudem werden Konstruktoren als Methoden mit den Namen \enquote{constructor} und Schnittstellen als Klassen repräsentiert, da auch hier eine zu feine Spezifikation nicht notwendig sein wird.  

  

In anderen Fällen gibt es jedoch viele Gemeinsamkeiten zwischen objektorientierten Programmiersprachen. So gibt es in  allen relevanten Sprachen Klassen, Methoden und Felder, die alle einen Namen haben. Des Weiteren haben Methoden und Felder einen (Rückgabe-)Typen und Methoden besitzen Parameter, die ihrerseits durch einen Namen und einen Typen definiert sind. Einige Sprachen sind zwar nicht stark typisiert, jedoch kann für nicht bekannte Datentypen ein Alias wie \enquote{Any} oder  \enquote{Object} verwendet werden.  Zudem sind viele Komponenten hierarchisch. In den meisten Sprachen können beispielsweise Klassen andere Klassen enthalten, sodass diese abstrakte Struktur diese Tatsache berücksichtigen müsste. 

Um dennoch sprachspezifische Funktionen anbieten zu können, besitzt jede Komponente ein Feld mit dem Typen \textit{ComponentMetaInformation}, das wie oben erwähnt die Information enthält, ob eine Komponente als öffentlich angesehen werden soll. Dieser Typ, welches eine Schnittstelle ist, kann von einer Klasse implementiert werden, um Parser für andere Programmiersprachen die Möglichkeit zu geben, zusätzliche sprachspezifische Informationen zu speichern. Beim Java-Parser wird diese Funktion beispielsweise genutzt, um zu speichern, welche \enquote{checked} Ausnahmen eine Methode werfen kann, sodass später ein Vergleich mit dem Javadoc-Block möglich ist. Die Schnittstelle enthält nur die Anforderung, eine \textit{isPublic}-Methode anzubieten und kann daher für viele andere objektorientierte Programmiersprachen Informationen speichern, die für einige Metriken eventuell nützlich sind. 



\input{figures/chapter3/parsing_object_diagram}

Abbildung \ref{fig:python_java_comp} veranschaulicht, wie eine einfache Datei in die Objektstruktur umgewandelt werden kann. Dabei werden eine einfache Java-Datei und eine semantisch äquivalente Python-Datei als Beispiel verwendet, um zu zeigen, dass aus beiden Sprachen eine gleiche Objektstruktur erzeugt werden kann. Dabei wurden zur Übersichtlichkeit einige nicht relevanten Attribute entfernt.



Das Programm in beiden Sprachen besteht aus einer öffentlichen Klasse \textit{Main} und einer privaten Methode \textit{test}, die einen Parameter \textit{a} als Ganzzahl erhält und eine Ganzzahl zurückgibt. Die höchste Hierarchieebene ist immer ein \textit{FileComponent}. Diese Datei enthält hier genau ein Kind namens \textit{classObj}, könnte aber in anderen Fällen auch mehrere Kinder (wie z.~B. Klassen enthalten). Die Klasse besitzt zudem einen Verweis auf ihren Elternteil. Außerdem enthält das \textit{classObj} eine Referenz auf Metainformationen, die hier nur angeben, dass die Klasse öffentlich ist. In diesen Metainformationen könnten auch andere relativ sprachspezifische Informationen definiert werden, falls sie für Metriken relevant sein könnten. Die Klasse enthält wiederum genau die Methode als einziges Kind. Die Methode hat ebenfalls einen Verweis auf die Metainformationen, welche die Methode als privat markieren. Außerdem hat die Methode einen \textit{returnType} und einen Verweis auf die Liste der Parameter, die wiederum aus einem Namen und einem Datentyp bestehen. Das Feld \textit{comment}, welches einen Verweis auf den strukturierten Kommentar liefert, wird in dieser Abbildung nicht dargestellt, da es bei den Dateien keine Dokumentation gibt und es somit den Wert \textit{null} hat.

Die Beziehungen der Klassen, die für das Parsing zuständig sind, werden zusätzlich in Abbildung \ref{fig:uml_parsing} im Anhang \ref{appendix_parsing_uml}. als UML-Diagramm illustriert.

\subsubsection{Repräsentation der strukturierten Kommentare}\label{chapter:structured_comments}
Neben der hierarchischen Repräsentation der einzelnen Komponenten müssen auch die strukturierten Kommentare (wie z.~B. Javadoc) geeignet in eine Datenstruktur umgewandelt werden. Wie in Kapitel \ref{chapter:javadoc}
 erläutert, besteht ein strukturierter Kommentar in vielen Fällen aus zwei Teilen. Der erste Teil ist eine allgemeine Beschreibung der Komponente. Im zweiten Teil werden bestimmte Strukturen genauer erläutert. So können einzelne Parameter erklärt werden oder der Rückgabewert beschrieben werden. 
 Dieser Aufbau findet sich auch in \textit{Doxygen} \cite{doxygen} oder \textit{Docstring} \cite{docstring}, sodass diese Struktur als Grundlage genommen wird. 
 
 Ein strukturierter Kommentar besteht also aus einer generellen Beschreibung, die auch weggelassen werden kann. Anschließend folgen null bis beliebig viele Tags. Jeder Tag besteht aus einem Typ (z.~B. \enquote{@param}, \enquote{@return} oder \enquote{@throws}), einem optionalen Parameter, welcher von einigen Tags benötigt wird und der Beschreibung des Tags. Die Namen der Tags sind generalisiert, dies bedeutet, dass unabhängig von der Programmiersprache der Tag zur Beschreibung eines Parameters immer \enquote{@param} heißen muss. Dies muss bei der Entwicklung eines Parsers beachtet werden. 
 
 Ein strukturierter Kommentar kann ebenfalls spezielle Elemente (wie z.~B. \ac{HTML}-Elemente oder Inline-Tags) enthalten. Hier werden diese Elemente unverarbeitet gelassen, also unverändert mit dem Rest des Kommentars als Zeichenkette gespeichert. Metriken, die diese Informationen benötigen, müssen diese Elemente also selbstständig extrahieren. Andere Metriken (wie z.~B. die Metriken in Kapitel \ref{chapter:metrics_semantic}) benötigen diese speziellen Elemente nicht, sodass eine einheitliche Schnittstelle, die sowohl die natürliche Sprache als auch die speziellen Elemente berücksichtigt und  zudem relativ programmiersprachenunabhängig sein müsste, viele Herausforderungen mit sich bringt.   
 
 Wird kein strukturierter Kommentar angegeben, so liefert der entsprechende Getter \textit{getComment} den Wert \enquote{null} zurück. 
\section{Konzeption der Metriken}\label{chapter:metric_conception}
Nachdem eine Datei in ihre einzelnen Komponenten zerlegt wurde, kann die Qualität der Softwaredokumentation überprüft werden. Jede gefundene Komponente besitzt einen Verweis auf die dazugehörige Dokumentation, die bei Nichtvorhandensein auch null sein kann. Anhand dieser Referenz kann geprüft werden, ob die Softwaredokumentation der Komponente ausreichend ist. Zur Bewertung der Dokumentation gibt es verschiedene  Möglichkeiten. Beispielsweise könnte überprüft werden, ob eine Komponente dokumentiert oder undokumentiert ist. Eine weitere Möglichkeit wäre es, die Verständlichkeit der Dokumentation zu prüfen. Alle diese Vorgehensweisen basieren auf Metriken, die auf wissenschaftliche Studien beruhen oder zumindest plausibel sind. In diesem Abschnitt wird ein Konzept erläutert, um eine Metrik zu implementieren. Anschließend wird beschrieben, wie die Ergebnisse jeder Metrik zusammengefasst werden, um ein Endresultat zu erhalten. 

\subsection{Implementation einer Metrik}\label{chapter:metric_impl}
Damit eine Metrik die Dokumentation bewerten kann, benötigt sie Zugriff auf die Komponente. Außerdem muss sie ihr Ergebnis irgendwie veröffentlichen bzw. zwischenspeichern, damit es später weiterverarbeitet werden kann. Des Weiteren ist nicht jede Metrik mit jeder Komponente kompatibel. Eine Metrik, die überprüft, ob jeder Methodenparameter dokumentiert ist, kann diese Aufgabe bei anderen Komponentenarten nicht erfüllen. Zudem sollte es die Möglichkeit geben, das Verhalten einer Metrik mittels Parameter anzupassen, damit die Metrik konfigurierbar bleibt. Außerdem sollte eine Metrik bei der Bewertung auch begründen, warum die Dokumentation einer Komponente nicht ausreichend ist. Zuletzt soll der Benutzer des Tools selbst auswählen können, welche Metriken angewendet werden sollen, da nicht jede Metrik immer sinnvoll ist. 

Eine Möglichkeit, diese Anforderung für eine Metrik umzusetzen, ist die Verwendung einer Methode pro Metrik, welche die Komponente und die Parameter als Eingabe erhält und daraus die Bewertung und eventuelle Begründung ermittelt und zurückgibt. Allerdings ist dieser Ansatz sehr prozedural, denn es gibt beispielsweise keine Kapselung zwischen den Metriken.  

Ein anderer Ansatz, der hier auch gewählt wird, ist es, jede gewünschte Metrik als Klasse zu implementieren. Jede implementierte Metrik kann somit die notwendigen Berechnungen abgekapselt von anderen Metriken erledigen, was die Wartbarkeit verbessert. Um trotzdem für eine einheitliche Schnittstelle zu sorgen, muss jede zu implementierende Metrik von einer abstrakten Basisklasse \textit{(DocumentationAnalysisMetric)} erben. Wenn eine neue Metrik implementiert werden soll, muss eine neue Klasse erstellt werden, die von dieser abstrakten Basisklasse erbt. Eine Instanz dieser Klasse wird im Folgenden \textbf{Metrikobjekt} genannt und repräsentiert eine konkrete Implementierung der Metrik, die bei der Bewertung der Dokumentation berücksichtigt werden kann. Um diese Bewertung durchzuführen, muss geprüft werden, ob eine Metrik mit einer Komponente kompatibel ist. Dies wird durch die  Methode \textit{shallConsider} erledigt,  welche dementsprechend einen Wahrheitswert zurückgibt. Die anschließende Analyse erfolgt durch die Methode \textit{analyze}. Diese führt den metrikspezifischen Algorithmus aus und speichert das Ergebnis wie in Kapitel \ref{chapter:store_metric} beschrieben, damit es zu einem Gesamtergebnis verarbeitet werden kann.

Da eine Metrik auch parametrisierbar sein soll, müssen bei der Instanziierung  eines Metrikobjektes Parameter übergeben werden, die von der Implementation der Metrik zur Modifikation des Verhaltens der Metrik verwendet werden können. Diese Parameter werden sehr abhängig von der Metrik sein, sodass eine einheitliche Schnittstelle nur schwer umsetzbar ist. Daher werden die Parameter als Datentyp \textit{any} übergeben, sodass es keine Typüberprüfung gibt. Alternativ wäre eine assoziative Liste möglich, bei dem ein Parametername als Zeichenkette ein Wert zugeordnet wird, aber auch hier könnte keine Überprüfung eines Datentypes vorgenommen werden. 

Eine  weitere Voraussetzung für ein Metrikobjekt ist ein eindeutiger Name. Dadurch kann die gleiche Metrik mit unterschiedlichen Parametern verwendet werden. Außerdem wird so eine Zuordnung von Gewichten vereinfacht. Standardmäßig besteht dieser eindeutige Name aus dem Namen der implementierten Metrik, gefolgt von einem Unterstrich und einer fortlaufenden Nummer. 

Ein UML-Diagramm der relevanten Klassen, die für die Berechnung der Dokumentationsqualität zuständig sind, befindet sich in Abbildung \ref{fig:uml_metrics} im Anhang \ref{appendix_metrics_uml}.


\subsubsection{Bewertung der Dokumentation}
Die Methode \textit{analyze} muss eine Bewertung darüber abgeben, ob die Qualität der Dokumentation ausreichend ist. Für die Repräsentation dieser Bewertung gibt es viele Möglichkeiten, allerdings ist eine numerische Bewertung mittels einer Intervallskala am sinnvollsten, da so der arithmetische Mittelwert, der Median etc. berechnet werden kann, was für die Bildung des Gesamtergebnisses wichtig ist.
Die numerische Bewertung soll eine Aussage über die Dokumentationsqualität liefern. Eine Bewertung von 0 steht für eine sehr schlechte bis nicht existente Dokumentation und die Bewertung 100 steht für eine exzellente Dokumentation, sodass die Bewertung sich als Prozent lesen lassen kann. Das Ergebnis einer implementierten Metrik sollte diesen Wertebereich nicht verlassen, da eine Fehlerbehandlung nicht implementiert ist. Bei Metriken, die per Design schon einen prozentualen Wert zurückgeben, wird diese Vorgabe stets eingehalten. Bei anderen Metriken (z.~B. die Flesch-Metrik in Kapitel \ref{chapter:metrics_semantic}) sollte eine mathematische Funktion gefunden werden, die das Ergebnis der Metrik auf den Wertebereich 0 bis 100 abbildet. Die genaue Umsetzung hängt von der Metrik ab. In jedem Falle sollte es für eine Metrik Ergebnisse geben, die auf eine gute bzw. schlechte Dokumentation hindeuten, damit diese auf 100 bzw. 0 abgebildet werden können. Nur durch diese Einschränkung auf einen fixen Wertebereich ist es möglich, den Mittelwert, Median etc. zu bilden und so eine Vergleichbarkeit zu ermöglichen. 

\subsubsection{Verwaltung der Metriken}

Für die spätere Ausführung der Metriken ist es sinnvoll, eine zentrale Stelle zu haben, welche die Verwaltung der Metriken übernimmt. Diese zentrale Stelle entkoppelt die Verwaltung der Metriken von dem restlichen Programmcode und sorgt so für eine bessere Struktur des Programms. Diese zentrale Komponente ist der \textbf{Metrikmanager}. Der Metrikmanager ist eine statische Klasse, die von allen Modulen des Tools benutzt werden kann, welche mit den Metriken arbeiten müssen.  

Eine wichtige Funktion des Metrikmanagers ist das Erzeugen neuer Metriken. Zwar wäre es möglich, die Metriken direkt zu instanziieren, wenn sie benötigt werden, allerdings entsteht dadurch eine direkte Abhängigkeit zwischen der Metrik und dem Modul, welches eine Metrik erzeugen möchte. Zur Vermeidung dieser direkten Abhängigkeit können Fabrikmethoden verwendet werden, bei dem ein Objekt nicht direkt erzeugt wird, sondern mittels einer Abfrage durch eine bestimmte Methode erzeugt wird und anschließend an das anfragende Objekt zurückgegeben wird \cite[S.~149--161]{gamma2015design}.

Basierend auf dieser Idee kann ein Modul, das ein konkretes Metrikobjekt benötigt, den Metrikmanager mit der Instanziierung beauftragen. Dabei benötigt der Metrikmanager eine Zeichenkette, um eine konkrete abgeleitete Klasse zu identifizieren, von der das neue Metrikobjekt instanziiert werden soll. Diese Zeichenkette wird eindeutig einer bestimmten abgeleiteten Klasse zugeordnet, sodass bei der Implementation einer neuen Metrik ein neuer Name spezifiziert werden muss. Alle Zeichenketten, die für eine bestimmte Metrik stehen, werden in einer Konstantensammlung  (\textit{Enum}) definiert, die ebenfalls bei der Definition einer neuen Metrik ergänzt werden muss.  Das neue Metrikobjekt benötigt, wie in Kapitel \ref{chapter:metric_impl} beschrieben, zudem einen eindeutigen Namen und Parameter, damit es eindeutig auffindbar ist und korrekt arbeiten kann. Dieses neue Metrikobjekt wird zudem vom Metrikmanager registriert und in einer Liste gespeichert, damit es später möglich sein wird, über alle erzeugten Metriken zu iterieren. 

Der Metrikmanager bietet zudem eine Methode an, mit denen der Standardwerte für die Parameter einer Metrik abgerufen werden können, sodass diese an einer Stelle verwaltet werden können. Außerdem kann der Metrikmanager auch einen \textit{MetricResultBuilder} erzeugen, um Teilergebnisse zu einem Gesamtresultat zu aggregieren. Dies geschieht ebenfalls über eine Fabrikmethode, sodass auch hier eine Entkopplung stattfindet. 

\subsubsection{Sprachspezifische Informationen für Metriken}\label{chapter:langSpec}
Da das Tool für möglichst viele objektorientierte Programmiersprachen konzipiert werden soll, muss eine Generalisierung erfolgen, da jede Sprache ihre Eigenheiten hat und möglicherweise besondere Funktionen anbietet, die nur schwer in einem abstrakten Format zu bringen sind.

Nichtsdestotrotz können solche sprachspezifischen Eigenheiten auch in der Dokumentation erwähnt werden. Daher ist es eine sinnvolle Idee, dass Metriken auch diese Besonderheiten benutzen, um ein genaueres Bild der Dokumentationsqualität zu erfahren, ohne jedoch zu wissen, welche Programmiersprache gerade analysiert wird. Beispielsweise können die Checked-Ausnahmen in Java mit den Informationen in der Javadoc verglichen werden. Auch können hierdurch überschriebene Methoden ignoriert werden, da diese oft nicht mehr dokumentiert werden müssen.

Um solche sprachspezifischen Analysen zu erlauben, besitzt jede Metrik Zugriff auf ein Objekt der Klasse \textit{LanguageSpecificHelper}. Wenn eine neue Programmiersprache hinzugefügt werden soll, kann von dieser geerbt werden. In der Klasse \textit{LanguageSpecificHelper} sind bereits einige Methoden definiert, die einigen Metriken bei der Bewertung helfen. So bewertet die Methode \textit{rateDocumentationCompatibility}, ob die Dokumentation alle sprachspezifischen Informationen erläutert (z.~B. \enquote{@throws}). Die Methode \textit{shallConsider} kann genutzt werden, um überschriebene Methoden zu ignorieren. 

Um eine eigene Methoden hinzuzufügen, muss diese in der Basisklasse definiert werden. Diese Methode sollte in der Basisklasse keine Aktionen durchführen, sondern entweder gar nichts tun oder Rückgabewerte haben, die keinen Einfluss auf Metriken haben. Anschließend kann diese Methode in einer sprachspezifischen abgeleiteten Klasse der Basisklasse korrekt implementiert werden. Die entsprechende Methode kann dann durch Änderung des Quellcodes der Metrik an den passenden Stellen von der Metrik verwendet werden. So kann beispielsweise das Resultat einer Metrik modifiziert werden oder weitere Ergebnisse mittels des \textit{MetricResultBuilders} angefügt werden.

\subsection{Einzelergebnisse verarbeiten}\label{chapter:store_metric}

Das berechnete Ergebnis einer Komponente muss nun gespeichert werden, damit es später ausgewertet werden kann. Dazu wird ein \textit{MetricResult}-Objekt erstellt, welches das im vorherigen Unterabschnitt berechnete Ergebnis enthält. Außerdem werden hier eventuelle Begründungen und Hinweise gespeichert, die den Anwender dabei unterstützen, die Qualität der Dokumentation zu verbessern. Jede Begründung enthält den Dateipfad der betroffenen Datei, den Namen der bemängelten Komponente und ein Zeilennummerintervall, sodass der Benutzer die problematische Stelle schnell finden kann. Zuletzt werden noch Informationen gespeichert, die beschreiben, in welchem Kontext das Ergebnis produziert wurde. Dies ist für die Gewichtung der Einzelergebnisse notwendig und wird in den nächsten Unterabschnitten noch genauer erläutert. 

Für die Speicherung des Objektes gibt es zwei Möglichkeiten. Die erste Möglichkeit wäre es, dass die \textit{analyze}-Methode das \textit{MetricResult}-Objekt zurückgibt, sodass der Aufrufer damit arbeiten kann. Bei der zweiten Möglichkeit wird das Ergebnis einem anderen Objekt übergeben, der dann die Weiterverarbeitung vornimmt. Dies hat den Vorteil, dass eine Metrik kein Ergebnis zurückliefern muss, wenn es kein sinnvolles Ergebnis berechnen kann. Bei einem Rückgabewert müsste ansonsten ein ungültiger Wert wie z.~B. \textit{null} vereinbart werden. Außerdem kann eine Metrik auch mehrere Resultate speichern, was bei komplexeren Komponenten in Betracht gezogen werden könnte. Dieses weitere Objekt ist ein  \textit{MetricResultBuilder}, der wie im nächsten Unterabschnitt beschrieben, die Softwaredokumentationsqualität jeder Komponente sammelt und daraus ein Gesamtergebnis berechnet.  

\subsubsection{Einzelergebnisse aggregieren}
Da ein Softwareprojekt aus Tausenden von Dateien bestehen kann, die wiederum aus verschiedenen Komponenten bestehen, müssen die Einzelergebnisse aggregiert werden, um so am Ende ein Gesamtergebnis zu erhalten, das zur Einschätzung der Qualität der Softwaredokumentation genutzt werden kann. Dazu wird dem \textit{MetricResultBuilder} jedes Ergebnis mittels der \textit{processResult}-Methode mitgeteilt, welches das Ergebnis in einer Liste speichert. Wenn alle Metriken verarbeitet sind, wird daraus ein Gesamtresultat gebildet. Dies geschieht durch die Methode \textit{getAggregratedResult}. Dabei wird standardmäßig ein arithmetischer Mittelwert gebildet.

Neue Algorithmen (wie z.~B. der Median oder der gewichtete Mittelwert) können implementiert werden, indem von dieser Klasse abgeleitet wird und die \textit{getAggregratedResult}-Methode überschrieben wird. 

Ein \textit{ResultBuilder} basiert auf dem Vorbild des Design-Patterns \enquote{Builder} aus \cite[S. 139--149]{gamma2015design}, da dieser aus einzelnen Metrikresultaten ein vollständiges Metrikergebnis baut.


 
\subsubsection{Zuordnung der Gewichte}\label{chapter_weights_assign}
Für einige Algorithmen muss eine Gewichtung vorgenommen werden, um bestimmte Ergebnisse besser oder schlechter zu bewerten. Dazu muss jedes Teilergebnis ein Gewicht zugeordnet werden. 

Insgesamt kann ein Teilergebnis anhand von drei Kategorien gewichtet werden. Durch die Gewichtung aufgrund der verwendeten Metrik können bestimmten Metriken einen größeren Einfluss auf das Gesamtergebnis haben, wenn diese als vertrauenswürdiger empfunden werden. Durch die Gewichtung von Dateien kann beispielsweise die öffentliche Schnittstelle einen größeren Einfluss auf die Bewertung nehmen, da diese Komponenten bzw. Dateien sehr kritisch sein können und daher gut verstanden werden müssen. Auch eine Gewichtung nach Komponenten kann in bestimmten Situationen sinnvoll sein. So kann beispielsweise argumentiert werden, dass Methoden, die durch ihre Parameter, Rückgabewerte und geworfenen Ausnahmen komplexer als Felder sind, höher gewichtet werden sollen.

Die Zuordnung der Gewichte erfolgt über einen \textit{WeightResolver}, welches eine Schnittstelle anbietet, um einen Bezeichner auf ein Gewicht abzubilden. Bei dem eindeutigen Namen eines Metrikobjektes kann hierfür eine assoziative Liste verwendet werden. Auch bei Komponenten, die durch ihren Klassennamen (wie z.~B. \textit{ClassComponent}) repräsentiert werden, ist  eine solche assoziative Liste sinnvoll, da nur eine begrenzte Anzahl an Metriken bzw. Komponenten existieren kann.
Für Dateipfade ist dies allerdings nicht praktikabel, da es eine Vielzahl an Dateien geben kann. Stattdessen können hier ähnlich wie bei der Filterung von Dateien in Kapitel \ref{chapter:traversing} Wildcard-Patterns verwendet werden. Eine assoziative Liste kann dazu jedes Wildcard-Patterns und das dazugehörige Gewicht speichern. Bei einer Abfrage kann das Gewicht des ersten Eintrages zurückgegeben werden, bei dem der Dateipfad mit dem Wildcard-Pattern kompatibel ist. Dies ermöglicht es, ganze Verzeichnisse oder Dateien mit bestimmten Namen stärker zu gewichten. 

Bei der Suche nach dem passenden Gewicht zu den Dateien und Komponenten kann es vorkommen, dass das passende Gewicht nicht gefunden wird. Um hierdurch entstehende Probleme zu vermeiden, wird ein Standardwert genommen (z.~B. 1). 

Jedes \textit{MetricResult}-Objekt enthält ein Tupel von drei Namen, welche die Quelle dieses Ergebnisses repräsentieren (Dateipfad, Komponententyp und eindeutiger Metrikname). Der Dateipfad beschreibt, in welcher Datei die Komponente liegt, die durch dieses Teilergebnis analysiert wurde. Der Komponententyp enthält den Klassennamen der Komponente und gibt somit Aufschluss darüber, ob diese Komponente eine Klasse, Methode etc. ist. Bei einer Klasse würde beispielsweise die Zeichenkette \enquote{ClassComponent} gespeichert werden. Durch den eindeutigen Metrikname (siehe Kapitel \ref{chapter:metric_impl}) wird  festgelegt, welche Metrik dieses Teilresultat produzierte. Alternativ könnte auch hier der Klassenname der Metrik verwendet werden. Allerdings kann eine Metrik durch Verwendung verschiedener Parameter auch mehrfach angewendet werden, sodass eine unterschiedliche Gewichtung möglich wäre. Daher ist der eindeutige Name hier sinnvoller.

\input{figures/chapter3/metrics_object_diagram}
Durch dieses Tupel kann für jedes Teilergebnis die Gewichtung der Metrik, der Komponente und des Dateipfades  abgerufen werden. Durch Multiplikation  aller Gewichtungen entsteht ein Gesamtgewicht.

Abbildung \ref{fig:metric_weighting} veranschaulicht anhand eines vereinfachten Objektdiagramms die Bildung eines Gesamtresultats mittels Gewichtung von verschiedenen Ergebnissen, indem der gewichtete Mittelwert aus Kapitel \ref{chapter:weighted_aggreg} verwendet wird. Hier wird ein hypothetisches Projekt mit zwei Dateien analysiert. Die erste Datei \enquote{Program.java} enthält nur eine öffentliche Methode \textit{method}, was in Java eigentlich nicht möglich ist, aber hier zur Vereinfachung zugelassen sein soll. Die zweite Datei \enquote{Main.java} enthält eine nichtöffentliche Klasse \textit{Main}. In diesem Beispiel werden zwei Metriken verwendet. Die erste Metrik prüft das Vorhandensein der Dokumentation bei allen Komponenten, während die zweite Metrik nur öffentliche Komponenten betrachtet (siehe Kapitel \ref{chapter:metrics_coverage}). In diesem Beispiel soll die Datei \enquote{Main.java} mit dem Faktor $3$ gewichtet werden. Methoden sollen mit dem Faktor $4$ gewichtet werden. Die zweite Metrik \textit{public\_members} wird mit dem Faktor $2$ gewichtet. Alle anderen Dateien, Komponenten und Metriken werden mit dem Standardfaktor $1$ gewichtet.

Das erste Teilergebnis \textit{r1} ist 100, da es die  Methode \textit{method} beschreibt, welche dokumentiert ist. Auch das dritte Teilergebnis \textit{r3} hat den Wert 100, da es die gleiche Komponente nur mit der zweiten Metrik beschreibt. Das zweite Teilergebnis \textit{r2} hat den Wert 0, da es die undokumentierte Klasse \textit{Main} beschreibt.  Es gibt kein weiteres Teilergebnis von \enquote{Main.java}, da die zweite Metrik jegliche nichtöffentliche Komponenten ignoriert. Die Teilergebnisse werden durch den \textit{WeightedMetricResultBuilder} intern gespeichert. 

Das Teilergebnis jeder gefundenen Komponente enthält gemäß dem obigen Vorgehen ein Tupel aus Dateiname, Komponentenname und Metrikname. Beispielsweise enthält das erste  Teilergebnis \textit{r1} das Tupel \enquote{(Program.java, MethodComponent, simple\_comment)}, wobei die einzelnen Bestandteile des Tupels Zeichenketten sind. Basierend auf diesen Informationen kann der \textit{WeightedResultBuilder} eine Gewichtung vornehmen. Das erste Teilergebnis würde mit dem Faktor $1*4*1=4$ gewichtet werden, da es von einer Methode stammt  und ansonsten nicht besonders gewichtet wird. Das zweite Teilergebnis erhält das Gewicht $3*1*1=3$, da es von der Datei \enquote{Main.java} stammt. Zuletzt besitzt das dritte Teilresultat die Gewichtung $1*4*2=2$, da es von der zweiten Metrik berechnet wurde und auch von einer Methode stammt. 

Basierend auf diesen Teilresultaten kann ein Gesamtergebnis berechnet werden. Hierzu wird jedes Ergebnis mit dessen Gewicht multipliziert und anschließend wird durch die Summe der Gewichte geteilt. Bezogen auf das Beispiel würde die Rechnung folgendermaßen aussehen:
\begin{equation}
    \frac{4*100 + 3*0 + 8*100}{4+3+8}=80
\end{equation}
Dieses Ergebnis ist dann Maß für die Bewertung der Dokumentationsqualität dieses hypothetischen Projektes. 








