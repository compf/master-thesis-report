
> masterthesis@1.0.0 split_files
> node split_files.js

Start
[0,0,0,0]
CURR LINE \documentclass[a4paper,12pt,oneside, bibliography=totoc]
CURR LINE {scrbook}
CURR LINE \usepackage[utf8]{inputenc}
CURR LINE \usepackage{pgf-umlcd}
CURR LINE % Schrift und -kodierung
CURR LINE \usepackage[T1]{fontenc}
CURR LINE \usepackage{lmodern}
CURR LINE \usepackage{tcolorbox}
CURR LINE % Sprache/Silbentrennung
CURR LINE \usepackage[english]{babel} %TODO change to german if desired
CURR LINE \usepackage{booktabs}
CURR LINE \usepackage{amsmath}
CURR LINE \usepackage{floatflt}
CURR LINE \usepackage{float}
CURR LINE \usepackage{verbatim}
CURR LINE \usepackage{hyperref}
CURR LINE \usepackage{graphicx}
CURR LINE \usepackage{pbox}
CURR LINE \usepackage{algorithmic}
CURR LINE \usepackage{algorithm}
CURR LINE \usepackage{subcaption}
CURR LINE \usepackage{siunitx}
CURR LINE \usepackage[autostyle]{csquotes}
CURR LINE \usepackage{todonotes}
CURR LINE \usepackage{svg}
CURR LINE \usepackage[page, title, titletoc, header]{appendix} %prettier appendix
CURR LINE 
CURR LINE \svgpath{{../figures/}}
CURR LINE 
CURR LINE \usepackage[printonlyused]{acronym}
CURR LINE \usepackage{listings}
CURR LINE %\usepackage{subfig}
CURR LINE \lstset{xleftmargin=2em} %Proper indention of listings
CURR LINE 
CURR LINE \widowpenalty10000
CURR LINE \clubpenalty10000
CURR LINE \usepackage{tabularx} %For tables
CURR LINE \usepackage{csquotes} %For Quotes
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE %Footnote Numbering not reset in new chapters
CURR LINE \usepackage{chngcntr}
CURR LINE \counterwithout{footnote}{chapter}
CURR LINE 
CURR LINE 
CURR LINE %Remove last point after section/subsections
CURR LINE \renewcommand{\autodot}{}
CURR LINE 
CURR LINE \usepackage[htt]{hyphenat} %damit texttt noch Linebreaks mit Silbentrennung erzeugt
CURR LINE \newcommand{\code}[1]{\texttt{#1}} %Programmcode im Textfluss in passendem Font ausgeben
CURR LINE 
CURR LINE %Literatur
CURR LINE %Ordering in references checken, vermutlich was mit style=numeric zu tun
CURR LINE \usepackage[
CURR LINE backend=biber,
CURR LINE sorting= none,
CURR LINE giveninits=true,
CURR LINE date=long,
CURR LINE urldate=long,
CURR LINE url=false
CURR LINE ]{biblatex}
CURR LINE \addbibresource{database.bib}
CURR LINE %\addbibresource{literatur2.bib}
CURR LINE 
CURR LINE 
CURR LINE \usepackage[]{hyperref}
CURR LINE 
CURR LINE \begin{document}
found begin
CURR LINE \frontmatter %roman page numbers
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \titlehead{
found end
CURR LINE \begin{center}
found end
CURR LINE \includegraphics[width=10cm]{figures/unilogo.pdf}\\
found end
CURR LINE Institute of Computer Science, Software Engineering Group
found end
CURR LINE \end{center}
found end
CURR LINE }
CURR LINE \subject{Master Thesis: }
CURR LINE \title{ Refactoring data clumps with the help of ChatGPT
CURR LINE }
CURR LINE \author{Timo Schoemaker\\ Immatriculation number: 978621} %engl. Matriculation Number
CURR LINE 
CURR LINE \date{\today\\
CURR LINE Advisor: Prof. Dr.-Ing. Elke Pulvermüller \\ %Deutsch: Erstebetreuer
CURR LINE Co-Advisor: Nils Baumgartner, M. Sc.} %Deutsch: Zweitbetreuer
CURR LINE 
CURR LINE \maketitle
CURR LINE 
CURR LINE \clearpage
CURR LINE 
CURR LINE \input{Misc/Abstract}
input
masterthesis/Misc/Abstract
CURR LINE \addchap*{Abstract}
CURR LINE \textbf{Deutsch}
CURR LINE Die Softwaredokumentation ist ein essenzieller Bestandteil der heutigen Softwareentwicklung geworden. Nichtsdestotrotz leidet die Qualität der Dokumentation häufig und viele Entwickler sind nicht motiviert genug, um eine gute Dokumentation zu schreiben. Das Ziel dieser Arbeit ist es, ein Tool zu entwickeln, dass exemplarisch die Dokumentationsqualität in Java-Programmen analysiert und mittels verschiedener Metriken (Anteil dokumentierter Komponenten an allen Komponenten, Flesch-Score, Kohärenz und Nichterwähnung von Randfällen) bewertet. Dieses Tool ist in GitHub Actions eingebunden, um den Entwickler bei einer sehr schlechten Dokumentationsqualität zu warnen und gegebenenfalls Mergevorgänge zu verhindern.
CURR LINE 
CURR LINE %\linebreak
CURR LINE \bigskip
CURR LINE 
CURR LINE \noindent
CURR LINE %\bigskip
CURR LINE \textbf{English}
CURR LINE The software documentation has become an integral part of software development. Nevertheless, the quality of the documentation is often poor and developers are often not motivated to write good documentation. The goal of this thesis is to develop a tool that can analyze the documentation quality of Java applications by applying different metrics (percentage of documented components in all components, Flesch score, coherence, not mentioning the handling of edge cases). This tool will be integrated in GitHub Actions to warn the developer about poor software documentation quality and to prevent a merge if the quality becomes too poor.
CURR LINE 
CURR LINE %TODO Bis jetzt nur Osi Abstract, evtl. etwas ausführlicher für Masterarbeit
CURR LINE 
CURR LINE 
CURR LINE \clearpage
CURR LINE 
CURR LINE \tableofcontents
CURR LINE \clearpage
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \input{Misc/langDef}
input
masterthesis/Misc/langDef
CURR LINE \lstdefinelanguage{YAML}{
CURR LINE morekeywords=
CURR LINE {
CURR LINE name:,on:,jobs:,steps:,uses:,run:,echo,workflow_dispatch:,description:,inputs:,required:,default:,runs-on:,using:,main:,with:,author:, absolute_threshold:
CURR LINE },
CURR LINE keywordstyle=\color{black}\bfseries,
CURR LINE ndkeywords={false,compf/JavaDocEvaluator@action},
CURR LINE ndkeywordstyle=\color{black}\bfseries,
CURR LINE identifierstyle=\color{black},
CURR LINE sensitive=false,
CURR LINE comment=[l]{//},
CURR LINE morecomment=[s]{/*}{*/},
CURR LINE commentstyle=\color{purple}\ttfamily,
CURR LINE %stringstyle=\color{red}\ttfamily,
CURR LINE morestring=[b]',
CURR LINE morestring=[b]",
CURR LINE alsodigit={:},
CURR LINE alsoletter={/,@,-}
CURR LINE }
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \lstdefinelanguage{ANTLR}{
CURR LINE keywords=
CURR LINE {
CURR LINE formalParameter:,variableModifier:,typeType:,variableDeclaratorId:,JCOMMENT:
CURR LINE },
CURR LINE keywordstyle=\color{black}\itshape,
CURR LINE identifierstyle=\color{black},
CURR LINE sensitive=false,
CURR LINE comment=[l]{//},
CURR LINE morecomment=[s]{/*}{*/},
CURR LINE commentstyle=\color{purple}\ttfamily,
CURR LINE morestring=[b]',
CURR LINE morestring=[b]",
CURR LINE alsodigit={:},
CURR LINE }
CURR LINE 
CURR LINE \lstdefinelanguage{JSON}{
CURR LINE tabsize             = 4,
CURR LINE showstringspaces    = false,
CURR LINE keywords            = {false,true,include,exclude,metrics,metric_name,weight,unique_name,params,absolute_threshold,builder,relative_threshold},
CURR LINE alsoletter          = 0123456789.*,
CURR LINE ndkeywordstyle         = \color{red},
CURR LINE keywordstyle=\color{black}\bfseries,
CURR LINE }
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \lstdefinelanguage{javascript}{
CURR LINE keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break,let,this,private},
CURR LINE keywordstyle=\color{black}\bfseries,
CURR LINE identifierstyle=\color{black},
CURR LINE sensitive=true,
CURR LINE comment=[l]{//},
CURR LINE morecomment=[s]{/*}{*/},
CURR LINE commentstyle=\color{purple}\ttfamily,
CURR LINE stringstyle=\color{red}\ttfamily,
CURR LINE morestring=[b]',
CURR LINE morestring=[b]",
CURR LINE }
CURR LINE \input{Misc/Acronyms} %See inside for usage of acronmys
input
masterthesis/Misc/Acronyms
CURR LINE %\ac{Abk.}         % fügt die Abkürzung ein, außer beim ersten Aufruf, hier wird die Erklärung mit angefügt
CURR LINE %\acs{Abk.}        % fügt die Abkürzung ein
CURR LINE %\acf{Abk.}        % fügt die Abkürzung UND die Erklärung ein
CURR LINE %\acl{Abk.}        % fügt nur die Erklärung ein
CURR LINE 
CURR LINE %\chapter*{Acronyms}
CURR LINE \addchap{Acronyms}
CURR LINE 
CURR LINE %%%%%%%%%%%%%%%%%%%%%%%
CURR LINE \begin{acronym}[E/E/PE] %sorgt fuer proper indention
found begin
CURR LINE \acro{API}{\emph{Application Programming Interface}}
found end
CURR LINE \acro{AST}{\emph{Abstract Syntax Tree}}
found end
CURR LINE \acro{ATL}{\emph{Atlas Transformation Language}}
found end
CURR LINE \acro{BMWi}{\emph{Bundesministerium für Wirtschaft und Energie}}
found end
CURR LINE \acro{CIM}{\emph{Computation-Independent Model}}
found end
CURR LINE \acro{CDC}{\emph{Code-level design choice}}
found end
CURR LINE \acro{CR}{\emph{Code-level requirement}}
found end
CURR LINE \acro{CI/CD}{\emph{Continuous Integration/Continuous Delivery}}
found end
CURR LINE \acro{CRC}{\emph{Cycling Redundancy Checks}}
found end
CURR LINE \acro{E/E/PE}{\emph{Electrical/Electronic/Programmable Electronic}}
found end
CURR LINE \acro{ECC}{\emph{Error Detecting and Correcting Codes}}
found end
CURR LINE \acro{EMF}{\emph{Eclipse Modeling Framework}}
found end
CURR LINE \acro{EGL}{\emph{Epsilon Generation Language}}
found end
CURR LINE \acro{EOL}{\emph{Epsilon Object Language}}
found end
CURR LINE \acro{HTML}{\emph{Hyper Text Markup Language}}
found end
CURR LINE \acro{Epsilon}{\emph{Extensible Platform of Integrated Languages for mOdel maNagement}}
found end
CURR LINE \acro{FS}{\emph{Functional Safety}}
found end
CURR LINE \acro{HAL}{\emph{Hardware Abstraction Layer}}
found end
CURR LINE \acro{HolMES}{\emph{Holistische Modell-getriebene Entwicklung für Eingebettete Systeme unter Berücksichtigung unterschiedlicher Hardware-Architekturen}}
found end
CURR LINE \acro{IDE}{\emph{Integrated Development Environment}}
found end
CURR LINE \acro{JSON}{\emph{JavaScript Object Notation}}
found end
CURR LINE \acro{JDT}{\emph{Java Development Tools}}
found end
CURR LINE 
found end
CURR LINE \acro{LOC}{\emph{Lines of Code}}
found end
CURR LINE \acro{LLM}{\emph{Large Language Model}}
found end
CURR LINE \acro{LSP}{\emph{Language Server Protocol}}
found end
CURR LINE \acro{MISRA}{\emph{Motor Industry Software Reliability Association}}
found end
CURR LINE \acro{MBU}{\emph{Multi Bit Upset}}
found end
CURR LINE \acro{MDA}{\emph{Model Driven Architecture}}
found end
CURR LINE \acro{MDC}{\emph{Model-level design choice}}
found end
CURR LINE \acro{MDD}{\emph{Model Driven Development}}
found end
CURR LINE \acro{MDE}{\emph{Model Driven Engineering}}
found end
CURR LINE \acro{MOF}{\emph{Meta Object Facility}}
found end
CURR LINE \acro{MR}{\emph{Model-level requirement}}
found end
CURR LINE \acro{NLP}{\emph{Natural Language Processing}}
found end
CURR LINE \acro{OCL}{\emph{Object Constraint Language}}
found end
CURR LINE \acro{OMG}{\emph{Object Management Group}}
found end
CURR LINE \acro{PIM}{\emph{Platform-Independent Model}}
found end
CURR LINE \acro{PSM}{\emph{Platform-Specific Model}}
found end
CURR LINE \acro{SER}{\emph{Soft Error Rate}}
found end
CURR LINE \acro{SEU}{\emph{Single Event Upset}}
found end
CURR LINE \acro{TMR}{\emph{Triple Modular Redundancy}}
found end
CURR LINE \acro{UML}{\emph{Unified Modeling Language}}
found end
CURR LINE \acro{VCS}{\emph{Version Control System}}
found end
CURR LINE 
found end
CURR LINE %\acro{cMOF}{\emph{complete MOF}}
found end
CURR LINE %\acro{eMOF}{\emph{essential MOF}}
found end
CURR LINE 
found end
CURR LINE %	\acro{ETL}{\emph{Epsilon Transformation Language}}
found end
CURR LINE %	\acro{EWL}{\emph{Epsilon Wizard Language}}
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \end{acronym}
found end
CURR LINE \mainmatter %switch roman auf arabic page numbers
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \chapter{Introduction}
[
  'head',
  'scrbook',
  '% Schrift und -kodierung',
  '% Sprache/Silbentrennung',
  '',
  '',
  '%',
  '',
  '',
  '',
  '',
  '',
  '',
  '%Footnote Numbering not reset in new chapters',
  '',
  '',
  '%Remove last point after section/subsections',
  '',
  '',
  '%Literatur',
  '%Ordering in references checken, vermutlich was mit style=numeric zu tun',
  'backend=biber,',
  'sorting= none,',
  'giveninits=true,',
  'date=long,',
  'urldate=long,',
  'url=false',
  ']biblatex',
  '%',
  '',
  '',
  '',
  '',
  '',
  '',
  'Advisor: Prof. Dr.-Ing. Elke Pulvermüller ',
  'Co-Advisor: Nils Baumgartner, M. Sc. %Deutsch: Zweitbetreuer',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  ''
] [ 1, 0, 0, 0 ]
CURR LINE \setcounter{page}{1} %Seitenzahlen hier mit 1 anfangen
CURR LINE 
CURR LINE %Include text from other files into the document --> great for structuring
CURR LINE \input{Introduction/introduction}
input
masterthesis/Introduction/introduction
CURR LINE \label{sec:introduction}
CURR LINE Code smells are a major issue  in modern software development as they tend to induce bugs and increase costs in the future \cite{mealyEvaluatingSoftwareRefactoring2006}.
----
Code smells are a major issue  in modern software development as they tend to induce bugs and increase costs in the future \citemealyEvaluatingSoftwareRefactoring2006.
Code smells are a major issue  in modern software development as they tend to induce bugs and increase costs in the future 
----
CURR LINE 
CURR LINE One example of code smells is data clumps \cite{BaumgartnerAP23}. These can be defined as a group of variables that are used in multiple parts of a software project \cite{fowler2019refactoring}. For instance, if the variables \textit{x}, \textit{y}, and \textit{z} are used multiple times in the source code, they could be interpreted as an Euclidean point or vector. Therefore, one could extract  a class \textit{Vector3d} containing the fields x, y, and z.
----
One example of code smells is data clumps \citeBaumgartnerAP23. These can be defined as a group of variables that are used in multiple parts of a software project \cite{fowler2019refactoring}. For instance, if the variables  {x}, \textit{y}, and \textit{z} are used multiple times in the source code, they could be interpreted as an Euclidean point or vector. Therefore, one could extract  a class \textit{Vector3d} containing the fields x, y, and z.
One example of code smells is data clumps  For instance, if the variables  {x},  and are used multiple times in the source code, they could be interpreted as an Euclidean point or vector. Therefore, one could extract  a class containing the fields x, y, and z.
----
CURR LINE 
CURR LINE This refactoring approach reduces the code side, makes the code more readable, and simplifies further changes to the source code \cite{data_clumps_refactoring_guru} \cite{join_data_items}.
----
This refactoring approach reduces the code side, makes the code more readable, and simplifies further changes to the source code \citedata_clumps_refactoring_guru \cite{join_data_items}.
This refactoring approach reduces the code side, makes the code more readable, and simplifies further changes to the source code 
----
CURR LINE 
CURR LINE There are many approaches to detecting code smells (e.g. SonarCloud,  PMD, Checkstyle) that can be integrated into the development process but these do not automatically fix them  \cite{vidalApproachPrioritizeCode2016}.
----
There are many approaches to detecting code smells (e.g. SonarCloud,  PMD, Checkstyle) that can be integrated into the development process but these do not automatically fix them  \citevidalApproachPrioritizeCode2016.
There are many approaches to detecting code smells (e.g. SonarCloud,  PMD, Checkstyle) that can be integrated into the development process but these do not automatically fix them  
----
CURR LINE Since developers are often distracted with implementing new features, fixing bugs, or doing similar tasks, the refactoring of code smells gets pushed back so that many code smells (even if detected) remain unfixed.  \cite{10.1145/2393596.2393655}.
----
Since developers are often distracted with implementing new features, fixing bugs, or doing similar tasks, the refactoring of code smells gets pushed back so that many code smells (even if detected) remain unfixed.  \cite10.1145/2393596.2393655.
Since developers are often distracted with implementing new features, fixing bugs, or doing similar tasks, the refactoring of code smells gets pushed back so that many code smells (even if detected) remain unfixed.  
----
CURR LINE 
CURR LINE 
CURR LINE One approach to solve this issue is to automatically fix certain code smells that are easy to define so that human intervention is minimized. This automation can be regularly applied, allowing code smells to be gradually addressed without distracting developers from their primary tasks but profiting from cleaner code.
CURR LINE However,  automatic refactoring  is more difficult than a simple detection because it requires changes to the source code that have the potential to induce bugs or even make the software project unable to build \cite{9796303}. Hence, the tools used must be carefully assessed.
----
However,  automatic refactoring  is more difficult than a simple detection because it requires changes to the source code that have the potential to induce bugs or even make the software project unable to build \cite9796303. Hence, the tools used must be carefully assessed.
However,  automatic refactoring  is more difficult than a simple detection because it requires changes to the source code that have the potential to induce bugs or even make the software project unable to build 
----
CURR LINE 
CURR LINE The data clump example mentioned above is one of those code smells that can be clearly defined. For instance, one can propose that a group of more than three variables that also appear in more than three different parts of the source code constitutes a data clump \cite{zhangImprovingPrecisionFowler2008}.
----
The data clump example mentioned above is one of those code smells that can be clearly defined. For instance, one can propose that a group of more than three variables that also appear in more than three different parts of the source code constitutes a data clump \citezhangImprovingPrecisionFowler2008.
The data clump example mentioned above is one of those code smells that can be clearly defined. For instance, one can propose that a group of more than three variables that also appear in more than three different parts of the source code constitutes a data clump 
----
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE However, a successful re-factorization of data clumps requires several steps. This includes renaming of identifiers, removing symbols, and extracting a class. Also a class name for the extracted class (e.g. \textit{Vector3}) must be determined. To minimize human intervention, a suitable identifier of the class must be found that accurately describes the purpose of these variables and their connection. Hence, domain knowledge and some creativity are necessary to fully perform the refactoring process.
CURR LINE 
CURR LINE As a result, additional tools (such as ChatGPT \cite{ChatGPT_url}) are needed to fully automate the refactoring pipeline while minimizing the need for manual changes.
----
As a result, additional tools (such as ChatGPT \citeChatGPT_url) are needed to fully automate the refactoring pipeline while minimizing the need for manual changes.
As a result, additional tools (such as ChatGPT 
----
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \section{Objective}
[
  '',
  'Code smells are a major issue  in modern software development as they tend to induce bugs and increase costs in the future ',
  '',
  'One example of code smells is data clumps  For instance, if the variables  {x},  and are used multiple times in the source code, they could be interpreted as an Euclidean point or vector. Therefore, one could extract  a class containing the fields x, y, and z.',
  '',
  'This refactoring approach reduces the code side, makes the code more readable, and simplifies further changes to the source code ',
  '',
  'There are many approaches to detecting code smells (e.g. SonarCloud,  PMD, Checkstyle) that can be integrated into the development process but these do not automatically fix them  ',
  'Since developers are often distracted with implementing new features, fixing bugs, or doing similar tasks, the refactoring of code smells gets pushed back so that many code smells (even if detected) remain unfixed.  ',
  '',
  '',
  'One approach to solve this issue is to automatically fix certain code smells that are easy to define so that human intervention is minimized. This automation can be regularly applied, allowing code smells to be gradually addressed without distracting developers from their primary tasks but profiting from cleaner code.',
  'However,  automatic refactoring  is more difficult than a simple detection because it requires changes to the source code that have the potential to induce bugs or even make the software project unable to build ',
  '',
  'The data clump example mentioned above is one of those code smells that can be clearly defined. For instance, one can propose that a group of more than three variables that also appear in more than three different parts of the source code constitutes a data clump ',
  '',
  '',
  '',
  '',
  'However, a successful re-factorization of data clumps requires several steps. This includes renaming of identifiers, removing symbols, and extracting a class. Also a class name for the extracted class (e.g.  Vector3) must be determined. To minimize human intervention, a suitable identifier of the class must be found that accurately describes the purpose of these variables and their connection. Hence, domain knowledge and some creativity are necessary to fully perform the refactoring process.',
  '',
  'As a result, additional tools (such as ChatGPT ',
  '',
  '',
  '',
  ''
] [ 1, 1, 0, 0 ]
CURR LINE 
CURR LINE Therefore, multiple programs or tools need to be combined to refactor data clumps. Each can tool can be regarded as  a service that provides a certain functionality and is encapsulated from other tools so that replacing a program by another can always be done in an efficient manner.
CURR LINE 
CURR LINE 
CURR LINE The goal of this master thesis is to develop a tool that  combines ChatGPT and other refactoring tools  to automatically detect and refactor data clumps in software projects. The program  shall at least support the Java programming language but shall be extendable to  other programming languages. The tool shall also  be able to filter out some files and data clumps by several criteria to reduce resources and costs.
CURR LINE 
CURR LINE The methodology of this master thesis will be evaluated by sending  pull requests about discovered data clumps and a refactoring proposal to several public GitHub repositories and analyzing whether the pull request is accepted, rejected, or amended.
CURR LINE 
CURR LINE \section{Approach}
[
  'Objective',
  '',
  'Therefore, multiple programs or tools need to be combined to refactor data clumps. Each can tool can be regarded as  a service that provides a certain functionality and is encapsulated from other tools so that replacing a program by another can always be done in an efficient manner.',
  '',
  '',
  'The goal of this master thesis is to develop a tool that  combines ChatGPT and other refactoring tools  to automatically detect and refactor data clumps in software projects. The program  shall at least support the Java programming language but shall be extendable to  other programming languages. The tool shall also  be able to filter out some files and data clumps by several criteria to reduce resources and costs.',
  '',
  'The methodology of this master thesis will be evaluated by sending  pull requests about discovered data clumps and a refactoring proposal to several public GitHub repositories and analyzing whether the pull request is accepted, rejected, or amended.',
  ''
] [ 1, 2, 0, 0 ]
CURR LINE 
CURR LINE A major part of the master thesis will deal with how to use and integrate ChatGPT into the refactoring pipeline.
CURR LINE ChatGPT is an AI language model developed by OpenAI that uses a Generative Pre-trained Transformer (GPT) model to process textual input data (i.e. natural language or source code). Users can provide queries, questions, or other textual material to ChatGPT and the model responds with a textual reply attempting to satisfy the user's request \cite{yetistirenEvaluatingCodeQuality2023}. It also employs a conversation feature so that previous requests and replies can be referred to by future requests and responses \cite{sobania2023analysis}.
----
ChatGPT is an AI language model developed by OpenAI that uses a Generative Pre-trained Transformer (GPT) model to process textual input data (i.e. natural language or source code). Users can provide queries, questions, or other textual material to ChatGPT and the model responds with a textual reply attempting to satisfy the user's request \citeyetistirenEvaluatingCodeQuality2023. It also employs a conversation feature so that previous requests and replies can be referred to by future requests and responses \cite{sobania2023analysis}.
ChatGPT is an AI language model developed by OpenAI that uses a Generative Pre-trained Transformer (GPT) model to process textual input data (i.e. natural language or source code). Users can provide queries, questions, or other textual material to ChatGPT and the model responds with a textual reply attempting to satisfy the user's request 
----
CURR LINE 
CURR LINE Since ChatGPT can also process source code \cite{sadik2023analysis}\cite{guo2023exploring}, one goal of this master thesis is to test to what extent it can help developers find data clumps and refactor them. Different extents of ChatGPT inclusion will be tested
----
Since ChatGPT can also process source code \citesadik2023analysis\cite{guo2023exploring}, one goal of this master thesis is to test to what extent it can help developers find data clumps and refactor them. Different extents of ChatGPT inclusion will be tested
Since ChatGPT can also process source code  one goal of this master thesis is to test to what extent it can help developers find data clumps and refactor them. Different extents of ChatGPT inclusion will be tested
----
CURR LINE 
CURR LINE With a minimal ChatGPT inclusion approach,  data clumps will be found using traditional approaches and ChatGPT will be provided with a list of data clump variables and asked to suggest a suitable name for the extracted class, while the refactoring process will be executed using other refactoring tools.
CURR LINE 
CURR LINE 
CURR LINE Conversely, it will be tested whether ChatGPT can execute the refactoring itself by providing the whole source code of a software project and providing specific queries to find the data clumps, refactor them, and output the refactored source code. \cite{White2023ChatGPTPP}.
----
Conversely, it will be tested whether ChatGPT can execute the refactoring itself by providing the whole source code of a software project and providing specific queries to find the data clumps, refactor them, and output the refactored source code. \citeWhite2023ChatGPTPP.
Conversely, it will be tested whether ChatGPT can execute the refactoring itself by providing the whole source code of a software project and providing specific queries to find the data clumps, refactor them, and output the refactored source code. 
----
CURR LINE 
CURR LINE Each task of the data clump detection and refactoring pipeline will be assigned to one tool, and the intermediate results of one tool will be used later for the succeeding tasks in the pipeline.
CURR LINE 
CURR LINE The goal is to find a ChatGPT usage that finds most data clumps, while ensuring that they are refactored correctly. Also possible costs for the usage of ChatGPT and other resources will be considered \cite{xia2023conversation}. \cite{4ef0b456377aafb68884e643779dffb36b8e7cc1}.
----
The goal is to find a ChatGPT usage that finds most data clumps, while ensuring that they are refactored correctly. Also possible costs for the usage of ChatGPT and other resources will be considered \citexia2023conversation. \cite{4ef0b456377aafb68884e643779dffb36b8e7cc1}.
The goal is to find a ChatGPT usage that finds most data clumps, while ensuring that they are refactored correctly. Also possible costs for the usage of ChatGPT and other resources will be considered 
----
CURR LINE 
CURR LINE \begin{comment}
found begin
CURR LINE Ein wichtiger Bestandteil der Softwareentwicklung von heute ist die Softwaredokumentation. Dies liegt unter anderem daran, dass die Größe von Softwareprojekten steigt, sodass die Entwickler schnell den Überblick über das Projekt verlieren können und daher zusätzliche Informationen neben dem Code benötigen \cite[S.~1]{StaticAnalysis:AnIntroduction:TheFundamentalChallengeofSoftwareEngineeringisOneofComplexity.}. Nichtsdestotrotz wird die Softwaredokumentation von Entwicklern oft vernachlässigt \cite[S.~83]{Qualityanalysisofsourcecodecomments}.  Die Gründe für schlechte Dokumentation sind vielfältig. Das Schreiben der Dokumentation wird oft als mühevoll empfunden und erfordert Fähigkeiten, die ein Programmierer nicht zwangsläufig besitzt \cite[S.~70]{AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner} \cite[S.~593]{Softwareengineeringandsoftwaredocumentation:aunifiedlongcourse}.
found end
CURR LINE 
found end
CURR LINE Weitere Studien verdeutlichen die Problematik der mangelhaften Softwaredokumentation. So belegt eine Umfrage aus dem Jahr 2002 mit 48 Teilnehmern  beispielsweise, dass die Dokumentation  bei Änderungen am System  nur mit Verzögerung angepasst wird. Knapp 70~\% der Teilnehmer stimmen der Aussage zu, dass die Dokumentation immer veraltet ist.   \cite[S.~28--29]{TheRelevanceofSoftwareDocumentationToolsandTechnologies:ASurvey}
found end
CURR LINE 
found end
CURR LINE Eine weitere Studie  \cite[S.~1199--1208]{SoftwareDocumentationIssuesUnveiled} aus dem Jahr 2019 verdeutlicht viele Aspekte aus der vorgenannten Umfrage. Es wurden dabei Daten aus Stack Overflow, GitHub-Issues, Pull-Requests und Mailing-Listen automatisiert heruntergeladen und dann von den Autoren analysiert, ob und inwieweit diese durch mangelhafte Softwaredokumentation verursacht wurden.  Die Studie belegt, dass von 824 Problemen, die etwas mit dem Thema \enquote{Softwaredokumentation} zu tun haben, 485 sich auf den Inhalt der Dokumentation beziehen (wie z.~B. unvollständige, veraltete oder sogar inkorrekte Dokumentation). Bei 255 Einträgen gab es Probleme mit der Struktur der Dokumentation, sodass beispielsweise Informationen schlecht auffindbar sind oder nicht gut verständlich sind.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE Eine andere Umfrage aus dem Jahr 2014 mit 88 Teilnehmern zeigt, dass eine automatisierte Überprüfung der Dokumentationsqualität von knapp der Hälfte der befragten Entwickler gewünscht wird. Die Autoren der Studie sehen dies als Zeichen dafür, dass ein grundsätzliches Bedürfnis zur automatisierten Bewertung von Dokumentationen besteht und daher weitere Studien notwendig sind. \cite[S.~340]{TheValueofSoftwareDocumentationQuality}
found end
CURR LINE 
found end
CURR LINE Die mangelhafte Dokumentation führt dazu, dass nicht nur nachfolgende Entwickler Probleme mit dem Codeverständnis haben, sondern auch Entwickler eines Moduls nach einer längeren Pause Zeit aufbringen müssen, um den Code wieder zu verstehen \cite[S.~511]{vestdam}.  Auch für Kunden/Auftraggeber ist eine gute Dokumentation wichtig, da gut dokumentierte Software tendenziell besser wartbar ist und somit mehr Nutzen bringt \cite[S.~83]{Qualityanalysisofsourcecodecomments} \cite[S.~1]{SoftwareDocumentationManagementIssuesandPractices:ASurvey}.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \section{Zielsetzung}
found end
CURR LINE Aufgrund der Relevanz von gut dokumentierter Software ist eine regelmäßige Rückmeldung über die Dokumentation von hoher Bedeutung. Spezielle Metriken, die eine numerische Auskunft über die Qualität der Softwaredokumentation liefern, sind eine Möglichkeit, diese Rückmeldung zu geben. Diese Metriken verschaffen dem Programmierer eine Einschätzung darüber, ob die Softwaredokumentation ausreichend ist oder eine Verbesserung sinnvoll wäre. Die Qualität der Softwaredokumentation kann auf unterschiedliche Art und Weise bewertet werden. So kann beispielsweise die bloße Existenz einer Dokumentation geprüft werden oder aber auch die Verständlichkeit der Dokumentation bewertet werden, daher kann es sinnvoll sein, mehrere Metriken zu verwenden \cite[S.~29]{pfleeger1992using}. Damit ein Entwickler einen Gesamtüberblick über die Dokumentationsqualität erhält, können diese Metriken kombiniert werden, um eine einzelne numerische Bewertung der Qualität der Dokumentation zu erhalten.
found end
CURR LINE Dabei ist es auch ratsam, die Metriken zu gewichten oder eine andere Methode zur Kombination der Metrikergebnisse zu benutzen, weil nicht jede Metrik die gleiche Zuverlässigkeit und Relevanz besitzt \cite[S.~1117ff.]{Softwarequalitymetricsaggregationinindustry}.
found end
CURR LINE 
found end
CURR LINE Damit das Feedback über die Softwaredokumentation auch wahrgenommen wird, sollte die Qualität regelmäßig  überprüft werden. Dies kann automatisiert im \ac{CI/CD}-Prozess erfolgen, bei dem Software kontinuierlich getestet und für den Release (z.~dt. Veröffentlichung) vorbereitet werden kann. Durch CI/CD können Unternehmen effizienter und besser Software entwickeln. So konnte das Unternehmen \textit{ING NL} die gelieferten Function-Points vervierfachen und die Kosten für einen Function-Point auf einen Drittel reduzieren \cite[S.~520]{Vassallo2016}.
found end
CURR LINE 
found end
CURR LINE \hfill
found end
CURR LINE 
found end
CURR LINE Basierend auf diesen Überlegungen soll ein Tool (z.~dt. Werkzeug) entwickelt werden. Dieses Tool (im Folgenden auch \textit{DocEvaluator} soll ein gegebenes Software-Projekt analysieren und eine numerische Bewertung abgeben, die eine heuristische Aussage über die Qualität der Softwaredokumentation trifft.  Dabei soll das Tool primär für Javadoc und Java bis Version 8 konzipiert werden, allerdings soll während der Entwicklung auch darauf geachtet werden, dass eine Portierung auf eine andere Programmiersprache ermöglicht wird und die Bewertung der Dokumentation unabhängig von der Programmiersprache funktioniert. Außerdem wird zur Vereinfachung nur englischsprachige Dokumentationen betrachtet. Komplexe \ac{NLP}-Metriken sollen dabei außer Acht gelassen werden. Auch Verfahren, die  den  Quellcode mit der Dokumentation vergleichen, wie z.~B. \textit{iComment} in \cite[S.~145ff.]{icomment}, sollen unberücksichtigt bleiben, da sie im Rahmen dieser Bachelorarbeit zu komplex sind.
found end
CURR LINE 
found end
CURR LINE Dabei sollte es nicht unbedingt das Ziel sein, dass jede Komponente dokumentiert ist, sondern dass die wichtigen Komponenten eine gute Dokumentationsqualität haben und somit die Wartung vereinfacht wird. Als Komponente im Sinne dieser Bachelorarbeit werden dabei Klassen, Schnittstellen, Methoden und Felder verstanden.
found end
CURR LINE 
found end
CURR LINE Dieses Tool soll anschließend in den \ac{CI/CD}-Prozess eingebunden werden, sodass die Dokumentationsqualität kontinuierlich geprüft werden kann. Als \ac{CI/CD}-Plattform soll dabei \textit{GitHub Actions} \cite{GithubActions} verwendet werden, da GitHub von der Mehrzahl der Entwickler und großen Unternehmen verwendet wird \cite{github_popular}. Mittels GitHub Actions soll das Tool bei einer sehr schlechten Dokumentationsqualität den Entwickler auf diesen Umstand hinweisen, indem beispielsweise ein Merge (z.~dt. Verschmelzung) in GitHub verhindert wird. Auch bei einer deutlichen inkrementellen Verschlechterung der Qualität soll der Entwickler informiert werden, um so eine ausreichende Qualität der Dokumentation sicherzustellen.
found end
CURR LINE 
found end
CURR LINE Ein Forschungsziel dieser Bachelorarbeit ist es zu prüfen, wie das Programm konzipiert werden muss, um mehrere Programmiersprachen zu unterstützen. Ein weiteres Ziel der Arbeit beschäftigt sich mit der Frage, wie die Ergebnisse der Metriken kombiniert werden können, um eine präzise Aussage über die Gesamtqualität der Dokumentation eines Softwareprojektes zu erhalten. Die Konzeption einer Architektur, mit der weitere Metriken hinzugefügt werden können und der Nutzer des Tools auswählen kann, welche Metriken bei der Bewertung der Dokumentationsqualität berücksichtigt werden sollen, soll ebenfalls als Forschungsziel untersucht werden. Zuletzt soll als Forschungsfrage diskutiert werden, welche Metriken eine heuristische Aussage über die Qualität der Dokumentation treffen können.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \section{Gliederung}
found end
CURR LINE In Kapitel \ref{sec:background} werden die wichtigen Grundlagen über die Themen dieser Bachelorarbeit erläutert. Dazu  wird zunächst der Begriff Softwaredokumentation definiert und ein Bezug zu Code-Smells hergestellt. Mittels Javadoc wird dann erläutert, wie Software dokumentiert werden kann. Anschließend wird eine Einführung in GitHub Actions gegeben.  Zudem wird eine Einführung in ANTLR4 gegeben, das für das Parsing der Quellcodedateien in Java verwendet wird. Zuletzt werden einige wissenschaftliche Arbeiten mit vergleichbaren Zielsetzungen präsentiert und Tools vorgestellt, die ebenfalls die Qualität der Softwaredokumentation bewerten können.
found end
CURR LINE 
found end
CURR LINE In Kapitel \ref{chapter_conception} werden die Fragestellungen besprochen, die sich beim Design des Tools ergeben haben. Dazu gehören die notwendigen Objekte und ihre Interaktion untereinander und wie von einer losen Ansammlung von Dateien zu einer Bewertung der Softwaredokumentation gelangt werden kann.
found end
CURR LINE 
found end
CURR LINE In Kapitel \ref{chapter:program} wird anschließend erläutert, wie aus dieser Konzeption ein vollständiges Programm entwickelt wird. Dazu wird erläutert, wie das Programm in GitHub Action eingebunden werden kann. Im Anschluss daran wird ein Überblick über die implementierten Metriken mit ihren Vor- und Nachteilen gegeben. Außerdem werden die Algorithmen bzw. Verfahren erläutert, um die Ergebnisse der einzelnen Metriken zu einem Gesamtergebnis zu aggregieren.
found end
CURR LINE 
found end
CURR LINE In Kapitel \ref{sec:evaluation} wird das Programm dann mit ähnlichen Tools verglichen, indem beispielhafte Java-Projekte aus GitHub mit allen Programmen analysiert werden und die Geschwindigkeit und die Qualität jedes Programmes ermittelt wird.
found end
CURR LINE 
found end
CURR LINE Im abschließenden Kapitel wird der Inhalt der Arbeit zusammengefasst und ein Fazit gezogen. Es werden offengebliebene Fragen beleuchtet und ein Ausblick gegeben, welche Möglichkeiten zur Verbesserung des Tools sinnvoll wären.
found end
CURR LINE \end{comment}
found end
CURR LINE \begingroup
CURR LINE \renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
CURR LINE \renewcommand{\clearpage}{}
CURR LINE \chapter{Background}\label{chapter:background}
[
  'Introduction',
  '',
  '%Include text from other files into the document --> great for structuring'
] [ 2, 0, 0, 0 ]
CURR LINE \endgroup
CURR LINE 
CURR LINE %Multiple input files for larger chapters are also possible
CURR LINE \input{Background/grundlagen}
input
masterthesis/Background/grundlagen
CURR LINE In this chapter, the background of data clumps will be discussed. A formal definition of data clumps will be presented (\ref{sec:data_clump_def}). ChatGPT will be discussed in section \ref{sec:chatgpt}. Also, the data clump type context format will be discussed.
CURR LINE \section{Code smells}
[
  '',
  'In this chapter, the background of data clumps will be discussed. A formal definition of data clumps will be presented ( Also, the data clump type context format will be discussed.'
] [ 2, 1, 0, 0 ]
CURR LINE 
CURR LINE The term \enquote{Code smell} is a term suggested by Kent Beck in \cite{fowler2019refactoring} for source code that may need refactoring. If the refactoring is not performed timely, the costs of maintenance of the source code and the software project can be higher and the efficiency of implementing changes is reduced. Some examples of code smells include unclear variables names, large classes, large method, missing documentation or code duplicates. Another possible code smell are data clumps.
----
The term  Code smell is a term suggested by Kent Beck in \cite{fowler2019refactoring} for source code that may need refactoring. If the refactoring is not performed timely, the costs of maintenance of the source code and the software project can be higher and the efficiency of implementing changes is reduced. Some examples of code smells include unclear variables names, large classes, large method, missing documentation or code duplicates. Another possible code smell are data clumps.
The term  Code smell is a term suggested by Kent Beck in for source code that may need refactoring. If the refactoring is not performed timely, the costs of maintenance of the source code and the software project can be higher and the efficiency of implementing changes is reduced. Some examples of code smells include unclear variables names, large classes, large method, missing documentation or code duplicates. Another possible code smell are data clumps.
----
CURR LINE 
CURR LINE \section{Data clumps}\label{sec:data_clump_def}
[
  'Code smells',
  '',
  'The term  Code smell is a term suggested by Kent Beck in for source code that may need refactoring. If the refactoring is not performed timely, the costs of maintenance of the source code and the software project can be higher and the efficiency of implementing changes is reduced. Some examples of code smells include unclear variables names, large classes, large method, missing documentation or code duplicates. Another possible code smell are data clumps.',
  ''
] [ 2, 2, 0, 0 ]
CURR LINE The term \enquote{Data Clump} was coined by Martin Fowler as one possible code smell that can occur in source code. He describes data clumps as follows:
CURR LINE 
CURR LINE \begin{displayquote}
found begin
CURR LINE Data items tend to be like children: They enjoy hanging around together around in groups. Often you will see
found end
CURR LINE the same three or four data items together in lots of
found end
CURR LINE places: fields in a couple of classes, parameters in many
found end
CURR LINE method signatures. \cite{fowler2019refactoring}
found end
CURR LINE \end{displayquote}
found end
CURR LINE 
CURR LINE This definition is somewhat imprecise. It is not specified whether three or four data items are necessary. Also, \enquote{a couple of classes} and \enquote{in many method signatures} do not define concrete numbers. The author suggests checking whether the removal of one data clump item would have a significant effect on the coherence of the code.
CURR LINE 
CURR LINE A more precise and algorithmic definition of \enquote{data clumps} is provided by \cite{zhangImprovingPrecisionFowler2008}. They say a data clump  can be defined on the field or method-parameter levels.
----
A more precise and algorithmic definition of  data clumps is provided by \cite{zhangImprovingPrecisionFowler2008}. They say a data clump  can be defined on the field or method-parameter levels.
A more precise and algorithmic definition of  data clumps is provided by  They say a data clump  can be defined on the field or method-parameter levels.
----
CURR LINE To be a method parameter data clump, a group of at least three variables must appear in multiple methods. Those variables must be duplicated, meaning they share the same name and data type. However, the inner order of the group does not need to be the same.
CURR LINE 
CURR LINE These conditions often need to be more relaxed. For instance, methods can be inherited and overridden so that a group of parameters may appear in each derived class, thereby fulfilling the definition of a method parameter data clump. Since (except for the identifiers of the parameters) an overriding method must be the same as the overridden method, they are not considered data clumps.
CURR LINE 
CURR LINE For field data clumps, similar conditions apply. There must be at least three fields that appear in more than one class, and the names and data types of the variables must be the same, while the inner order may be different. Since in most programming languages, a field can have an additional access modifier (e.g., \textit{private}, \textit{static} etc. ), the access modifier should also be included to determine whether two groups of variables are identical and hence a data clump.
CURR LINE 
CURR LINE The definition might also need to be more relaxed for both method and field data clumps. Two variables that have the same name but a compatible type in at least one direction  (e.g., \textit{int} and  \textit{double}), would be disregarded as a data clump according to the formalized definition. However, some would regard them as a data clump.
CURR LINE 
CURR LINE Also, modification of a variable's identifier might not change its meaning. For instance, typos can happen, or synonyms can be used so that an automatic algorithm might not discover the connection between two variables but requires knowledge of the semantics of the source code. \cite{zhangImprovingPrecisionFowler2008}
----
Also, modification of a variable's identifier might not change its meaning. For instance, typos can happen, or synonyms can be used so that an automatic algorithm might not discover the connection between two variables but requires knowledge of the semantics of the source code. \citezhangImprovingPrecisionFowler2008
Also, modification of a variable's identifier might not change its meaning. For instance, typos can happen, or synonyms can be used so that an automatic algorithm might not discover the connection between two variables but requires knowledge of the semantics of the source code. 
----
CURR LINE 
CURR LINE 
CURR LINE To conclude, the core definition of a data clump is clear. However, this definition still leaves out some edge cases that require a semantic understanding of the source code.
CURR LINE 
CURR LINE An example of a data clump is shown in listing \ref{lst:math_stuff_java}
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{Some operations on vectors},' ]
0
CURR LINE label={lst:math_stuff_java},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/dataClump/MathStuff.java}
CURR LINE \end{figure}
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE Listing \ref{lst:math_stuff_java} contains three methods that execute some vector operations (calculation of length, sum of coordinates, and the maximum coordinate).
CURR LINE 
CURR LINE 
CURR LINE It can be seen that  the snippet contains a method parameter data clump since the variables \textit{x}, \textit{y}, and  \textit{z} occur thrice.  These variables might be called \textbf{data clump items}
CURR LINE 
CURR LINE \subsection{Refactoring data clumps}
[
  'Data clumps',
  'The term  Data Clump was coined by Martin Fowler as one possible code smell that can occur in source code. He describes data clumps as follows:',
  '',
  '',
  'This definition is somewhat imprecise. It is not specified whether three or four data items are necessary. Also,  a couple of classes and do not define concrete numbers. The author suggests checking whether the removal of one data clump item would have a significant effect on the coherence of the code.',
  '',
  'A more precise and algorithmic definition of  data clumps is provided by  They say a data clump  can be defined on the field or method-parameter levels.',
  'To be a method parameter data clump, a group of at least three variables must appear in multiple methods. Those variables must be duplicated, meaning they share the same name and data type. However, the inner order of the group does not need to be the same.',
  '',
  'These conditions often need to be more relaxed. For instance, methods can be inherited and overridden so that a group of parameters may appear in each derived class, thereby fulfilling the definition of a method parameter data clump. Since (except for the identifiers of the parameters) an overriding method must be the same as the overridden method, they are not considered data clumps.',
  '',
  'For field data clumps, similar conditions apply. There must be at least three fields that appear in more than one class, and the names and data types of the variables must be the same, while the inner order may be different. Since in most programming languages, a field can have an additional access modifier (e.g.,  private, etc. ), the access modifier should also be included to determine whether two groups of variables are identical and hence a data clump.',
  '',
  'The definition might also need to be more relaxed for both method and field data clumps. Two variables that have the same name but a compatible type in at least one direction  (e.g.,  int and  , would be disregarded as a data clump according to the formalized definition. However, some would regard them as a data clump.',
  '',
  "Also, modification of a variable's identifier might not change its meaning. For instance, typos can happen, or synonyms can be used so that an automatic algorithm might not discover the connection between two variables but requires knowledge of the semantics of the source code. ",
  '',
  '',
  'To conclude, the core definition of a data clump is clear. However, this definition still leaves out some edge cases that require a semantic understanding of the source code.',
  '',
  'An example of a data clump is shown in listing ',
  'undefinedSome operations on vectorsundefined,',
  'label=lst:math_stuff_java,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/dataClump/MathStuff.java',
  '',
  '',
  '',
  'Listing ',
  '',
  '',
  'It can be seen that  the snippet contains a method parameter data clump since the variables  x,  and  occur thrice.  These variables might be called  {data clump items}',
  ''
] [ 2, 2, 1, 0 ]
CURR LINE Fowler suggests two  steps to refactor a data clump:
CURR LINE 
CURR LINE In the  \textbf{Extract-Class}-step, a class with fields for each data clump item is extracted. A class for this purpose might already exist so that it can be re-used.
CURR LINE 
CURR LINE In the second step, \textbf{Preserve Whole Object} or \textbf{Introduce Parameter Object} might be applied. This means that the signature of the method is changed so that the extracted class replaces the data clump items, and all references to the method are changed accordingly.
CURR LINE 
CURR LINE 
CURR LINE To illustrate the suggested data clump refactoring process, listing \ref{lst:math_user_java} shows how the methods in \ref{lst:math_stuff_java} can be used.
CURR LINE 
CURR LINE 
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{Some operations on vectors},' ]
0
CURR LINE label={lst:math_user_java},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/dataClump/MathUser.java}
CURR LINE \end{figure}
CURR LINE 
CURR LINE In the first step, a new class can be extracted, which contains all data clump items as fields. It should also contain setters and getters to access the fields in an encapsulated manner. The class should be named so that its purpose is clear. Listing \ref{lst:coordinate_java} shows how such a class may look like.
CURR LINE 
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{ Resulting Coordinate class},' ]
0
CURR LINE label={lst:coordinate_java},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/dataClump/Coordinate.java}
CURR LINE \end{figure}
CURR LINE 
CURR LINE In the second step, a parameter object is introduced that replaces the three previous data clump items in each of the three methods of the \textit{MathStuff} class so that the signature only contains one parameter of type \textit{Coordinate}. Also, the calls in \textit{MathUsage} are refactored to reflect that change ( listing \ref{lst:math_stuff_refactored_java} and \ref{lst:math_user_refactored_java}).
CURR LINE 
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{ Resulting Coordinate class},' ]
0
CURR LINE label={lst:math_stuff_refactored_java},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/dataClump/MathStuffRefactored.java}
CURR LINE \end{figure}
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{ Resulting Coordinate class},' ]
0
CURR LINE label={lst:math_user_refactored_java},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/dataClump/MathStuffRefactored.java}
CURR LINE \end{figure}
CURR LINE 
CURR LINE \subsection{ Data clumps Type Context }\label{sec:data_clump_format}
[
  'Refactoring data clumps',
  'Fowler suggests two  steps to refactor a data clump:',
  '',
  'In the   Extract-Class-step, a class with fields for each data clump item is extracted. A class for this purpose might already exist so that it can be re-used.',
  '',
  'In the second step,  Preserve Whole Object or might be applied. This means that the signature of the method is changed so that the extracted class replaces the data clump items, and all references to the method are changed accordingly.',
  '',
  '',
  'To illustrate the suggested data clump refactoring process, listing can be used.',
  '',
  '',
  'undefinedSome operations on vectorsundefined,',
  'label=lst:math_user_java,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/dataClump/MathUser.java',
  '',
  'In the first step, a new class can be extracted, which contains all data clump items as fields. It should also contain setters and getters to access the fields in an encapsulated manner. The class should be named so that its purpose is clear. Listing ',
  '',
  'undefined Resulting Coordinate classundefined,',
  'label=lst:coordinate_java,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/dataClump/Coordinate.java',
  '',
  'In the second step, a parameter object is introduced that replaces the three previous data clump items in each of the three methods of the  MathStuff class so that the signature only contains one parameter of type  Also, the calls in are refactored to reflect that change ( listing and .',
  '',
  'undefined Resulting Coordinate classundefined,',
  'label=lst:math_stuff_refactored_java,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/dataClump/MathStuffRefactored.java',
  'undefined Resulting Coordinate classundefined,',
  'label=lst:math_user_refactored_java,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/dataClump/MathStuffRefactored.java',
  ''
] [ 2, 2, 2, 0 ]
CURR LINE 
CURR LINE The \textbf{Data clumps Type Context} \cite{dataclump_type_context} is developed by Baumgartner et al. to establish a standard for reporting data clumps.
----
The  Data clumps Type Context \cite{dataclump_type_context} is developed by Baumgartner et al. to establish a standard for reporting data clumps.
The  Data clumps Type Context is developed by Baumgartner et al. to establish a standard for reporting data clumps.
----
CURR LINE 
CURR LINE An example of the Data clump type context can be seen in listing \ref{lst:data_clump_type_context_example}. Only a subset of the formate will be discussed here because of space reasons.
CURR LINE 
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{ Resulting Coordinate class},' ]
0
CURR LINE label={lst:data_clump_type_context_example},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/data_clump_type_context_example.json}
CURR LINE \end{figure}
CURR LINE 
CURR LINE The format consists of three layers. In the outer layer which is left out here, general project information is defined: For instance, the programming language or the location of the project. Also the number of methods, classes, and the number of detected data clumps can be obtained in this general part.
CURR LINE 
CURR LINE In the next layer, each detected data clump is mapped with a unique key (l. 4).
CURR LINE 
CURR LINE The detected data clumps are described as a link between two files. These files might be identical if the data clumps are located in the same file. Here one has to differ between the \enquote{from-part} and the \enquote{to-part} which represent the two nodes in the data clump graph. For instance \enquote{from\_file\_path} indicates the location of the one part of the data clump, while \enquote{to\_file\_path} provides the path to the opposite end.
CURR LINE 
CURR LINE A similar principle is apllied to the classes in which a data clump is located. Because a class name might not be unique, not only the names of the two classes but also unique ientifier of those classes are provided (l. 9-10 and 14-15).
CURR LINE 
CURR LINE The information about the methods of the data clump )l. 11, 12, 16, 17) is optional. if one part of the data clump is a field-to-field data clump, no method is involved so the respective part would be \textit{null}.
CURR LINE 
CURR LINE The field \enquote{probability} can be used by probabilistic data clump detection tools to indicate the probability that the detected data clump is indeed a data clump. For purposes of this master thesis, it will be ignored.
CURR LINE 
CURR LINE In lines 19-42, each variable that is part of the data clump is  described. Here only one variable is listed although for a data clump there must be at least three variables. As for the data clump itself, the individual variables are separated into  a \enquote{from-part} and a \enquote{to-part}. The former is implicitly defined (l.~20-25, 38-42), while the latter has a specifically named sub object \enquote{to\_variable} (l. 27-37). For each variable, the name (l. 22 and l. 28)) and the data type (l.~23 and l. 29) is provided. As for the general data clump, a probability is given whether the variable is indeed part of a data clump (l. 24). As mentioned above, this information will be ignored.  Additionally modifiers like \enquote{private}, \enquote{final} etc. are also stored.
CURR LINE 
CURR LINE In order to find the variable in the source code, detailed location information is needed. In source code, the line number (l.~32 and l.~39) and the column number (l.~33 and l.~40) is required . To avoid any ambiguity, the end position of the variable is saved too. Both numbers are one-index-based meaning that the first line is 1 and the first column is also 1.
CURR LINE 
CURR LINE 
CURR LINE \section{ChatGPT}\label{sec:chatgpt}
[
  ' Data clumps Type Context ',
  '',
  'The  Data clumps Type Context is developed by Baumgartner et al. to establish a standard for reporting data clumps.',
  '',
  'An example of the Data clump type context can be seen in listing ',
  '',
  'undefined Resulting Coordinate classundefined,',
  'label=lst:data_clump_type_context_example,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/data_clump_type_context_example.json',
  '',
  'The format consists of three layers. In the outer layer which is left out here, general project information is defined: For instance, the programming language or the location of the project. Also the number of methods, classes, and the number of detected data clumps can be obtained in this general part.',
  '',
  'In the next layer, each detected data clump is mapped with a unique key (l. 4).',
  '',
  'The detected data clumps are described as a link between two files. These files might be identical if the data clumps are located in the same file. Here one has to differ between the  from-part and the which represent the two nodes in the data clump graph. For instance indicates the location of the one part of the data clump, while provides the path to the opposite end.',
  '',
  'A similar principle is apllied to the classes in which a data clump is located. Because a class name might not be unique, not only the names of the two classes but also unique ientifier of those classes are provided (l. 9-10 and 14-15).',
  '',
  'The information about the methods of the data clump )l. 11, 12, 16, 17) is optional. if one part of the data clump is a field-to-field data clump, no method is involved so the respective part would be  null.',
  '',
  'The field  probability can be used by probabilistic data clump detection tools to indicate the probability that the detected data clump is indeed a data clump. For purposes of this master thesis, it will be ignored.',
  '',
  'In lines 19-42, each variable that is part of the data clump is  described. Here only one variable is listed although for a data clump there must be at least three variables. As for the data clump itself, the individual variables are separated into  a  from-part and a  The former is implicitly defined (l.~20-25, 38-42), while the latter has a specifically named sub object (l. 27-37). For each variable, the name (l. 22 and l. 28)) and the data type (l.~23 and l. 29) is provided. As for the general data clump, a probability is given whether the variable is indeed part of a data clump (l. 24). As mentioned above, this information will be ignored.  Additionally modifiers like  etc. are also stored.',
  '',
  'In order to find the variable in the source code, detailed location information is needed. In source code, the line number (l.~32 and l.~39) and the column number (l.~33 and l.~40) is required . To avoid any ambiguity, the end position of the variable is saved too. Both numbers are one-index-based meaning that the first line is 1 and the first column is also 1.',
  '',
  ''
] [ 2, 3, 0, 0 ]
CURR LINE 
CURR LINE ChatGPT \cite{ChatGPT_url} is a \ac{LLM} developed by OpenAI and released in November 2022. As a \ac{LLM}, ChatGPT can interpret user queries and return an appropriate response.
----
ChatGPT \citeChatGPT_url is a \ac{LLM} developed by OpenAI and released in November 2022. As a \ac{LLM}, ChatGPT can interpret user queries and return an appropriate response.
ChatGPT developed by OpenAI and released in November 2022. As a  ChatGPT can interpret user queries and return an appropriate response.
----
CURR LINE 
CURR LINE A query can be a question or a prompt directing ChatGPT to answer a question or provide some output. The range of topics ChatGPT can help with is basically unlimited. For instance, ChatGPT can help with math, history, politics, or coding topics. ChatGPT can also understand programming language and therefore, help developers to code. Since September 2023, ChatGPT can also process images \cite{ChatGPT_image}. However, it must be noted that ChatGPT may not always provide accurate responses since it is, in the end, just a language model without knowledge about the inherent meanings of its responses.
----
A query can be a question or a prompt directing ChatGPT to answer a question or provide some output. The range of topics ChatGPT can help with is basically unlimited. For instance, ChatGPT can help with math, history, politics, or coding topics. ChatGPT can also understand programming language and therefore, help developers to code. Since September 2023, ChatGPT can also process images \citeChatGPT_image. However, it must be noted that ChatGPT may not always provide accurate responses since it is, in the end, just a language model without knowledge about the inherent meanings of its responses.
A query can be a question or a prompt directing ChatGPT to answer a question or provide some output. The range of topics ChatGPT can help with is basically unlimited. For instance, ChatGPT can help with math, history, politics, or coding topics. ChatGPT can also understand programming language and therefore, help developers to code. Since September 2023, ChatGPT can also process images 
----
CURR LINE 
CURR LINE The usage of ChatGPT is nevertheless somewhat restricted. For instance, content regarded as hate speech or used for illegal purposes will be suppressed.
CURR LINE 
CURR LINE Another essential feature of ChatGPT is the ability to store conversations. A conversation is a collection of queries and linked responses sent to ChatGPT. Using conversations, a user can refer to a previous query or response in a later query. For instance, if ChatGPT makes a mistake or misinterprets a query, a user can send another request connected to the previous request and point out the mistake or give more context, helping ChatGPT auto-correct itself.
CURR LINE 
CURR LINE ChatGPT can be used via a browser or via an API. Using ChatGPT via the browser is free although restricted. A paid version is available that is faster and uses an improved model that supports the beforementioned image generation and other features not relevant for this master thesis.  Using the API requires a payment based on used token. A token is approximately a word but a detailed definition of a token is difficult to specify.
CURR LINE 
CURR LINE Figure \ref{fig:chatgpt_browser} illustrate how ChatGPT can be used in the browser.
CURR LINE \begin{figure}
[ '    \\caption{ChatGPT in the browser}' ]
0
CURR LINE \label{fig:chatgpt_browser}
CURR LINE \end{figure}
CURR LINE In the main panel that encompasses the most area of the figure, a chat is visualized. This is the the chat with the ChatGPT model. The queries are headlined with \textit{You} and the responses with \textit{ChatGPT}. In this example, ChatGPT is asked to create a hello world program with gradle. As a response, the model returned code blocks which the user can simply copy and use. Also descriptions are provided to explain the context and usage of the code.
CURR LINE 
CURR LINE On the right side, all conversation with ChatGPT are listed. For instance, there is a conversation about \enquote{IntelliJ PSI Modification Fix}. A user can have multiple independent conversations where each has it own context.
CURR LINE 
CURR LINE \subsection{ChatGPT API}
[
  'ChatGPT',
  '',
  'ChatGPT developed by OpenAI and released in November 2022. As a  ChatGPT can interpret user queries and return an appropriate response.',
  '',
  'A query can be a question or a prompt directing ChatGPT to answer a question or provide some output. The range of topics ChatGPT can help with is basically unlimited. For instance, ChatGPT can help with math, history, politics, or coding topics. ChatGPT can also understand programming language and therefore, help developers to code. Since September 2023, ChatGPT can also process images ',
  '',
  'The usage of ChatGPT is nevertheless somewhat restricted. For instance, content regarded as hate speech or used for illegal purposes will be suppressed.',
  '',
  'Another essential feature of ChatGPT is the ability to store conversations. A conversation is a collection of queries and linked responses sent to ChatGPT. Using conversations, a user can refer to a previous query or response in a later query. For instance, if ChatGPT makes a mistake or misinterprets a query, a user can send another request connected to the previous request and point out the mistake or give more context, helping ChatGPT auto-correct itself.',
  '',
  'ChatGPT can be used via a browser or via an API. Using ChatGPT via the browser is free although restricted. A paid version is available that is faster and uses an improved model that supports the beforementioned image generation and other features not relevant for this master thesis.  Using the API requires a payment based on used token. A token is approximately a word but a detailed definition of a token is difficult to specify.',
  '',
  'Figure ',
  'In the main panel that encompasses the most area of the figure, a chat is visualized. This is the the chat with the ChatGPT model. The queries are headlined with  You and the responses with  In this example, ChatGPT is asked to create a hello world program with gradle. As a response, the model returned code blocks which the user can simply copy and use. Also descriptions are provided to explain the context and usage of the code.',
  '',
  'On the right side, all conversation with ChatGPT are listed. For instance, there is a conversation about  IntelliJ PSI Modification Fix. A user can have multiple independent conversations where each has it own context.',
  ''
] [ 2, 3, 1, 0 ]
CURR LINE In order to use ChatGPT \ac{API}, a user has to create an OpenAI account and deposit financial information so that the user can be charged. Each request to the API consumes a certain amount of tokens dependent on the query.
CURR LINE 
CURR LINE A token is the smallest unit used by ChatGPT to process a query. For instance, a token could be one english word, o syntactical part of a programming language, a number or a similar independent part of a query. According to OpenAI, a token is 3/4 of a word so that 100 tokens are about 75 words. However, this is just an estimate for natural language texts in the English language.
CURR LINE 
CURR LINE The \ac{API} itself used JSON combined with HTTPS. For the purpose of this master thesis, only a subset of the available means to use the \ac{API} will be explained because the whole \ac{API} would be too complex and irrelevant.
CURR LINE 
CURR LINE \subsubsection{Query}
[
  'ChatGPT API',
  'In order to use ChatGPT ',
  '',
  'A token is the smallest unit used by ChatGPT to process a query. For instance, a token could be one english word, o syntactical part of a programming language, a number or a similar independent part of a query. According to OpenAI, a token is 3/4 of a word so that 100 tokens are about 75 words. However, this is just an estimate for natural language texts in the English language.',
  '',
  'The will be explained because the whole would be too complex and irrelevant.',
  ''
] [ 2, 3, 1, 1 ]
CURR LINE Listing \ref{lst:chatgpt_api} shows how an \ac{API} query may look like if it used via \textit{curl} which is a tool perform HTTP requests:
CURR LINE \begin{figure} [htbp!]
[
  '\t\t\t[caption',
  '{ Example query to ChatGPT  \\cite{ChatGPT_url}},'
]
0
CURR LINE label={lst:chatgpt_api},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/chatgpt_api.json}
CURR LINE \end{figure}
CURR LINE 
CURR LINE 
CURR LINE Since ChatGPT uses \ac{JSON} for communication, the content header of HTTP  must be set to \enquote{application/json} (l. 2). Afterwards, a token must be provided to ChatGPT (l. 23). This  token must be generated on the OpenAI website and will be used to connect the query to an OpenAI account so that the user can be charged properly- The token should remain secret and may not be disclosed (e.~g. via \ac{VCS})
CURR LINE 
CURR LINE Then, the actual query is defined (l. 4-24.)Firstly, the model is defined (l. 5). OpenAI provides multiple models that have different advantages and disadvantages. For instance, a newer model like \textit{gpt-4} has more capabilities but is more expensive.
CURR LINE 
CURR LINE Afterwards, the actual messages are provided (l. 6-24). Each message is a tuple of a \textbf{role} and a \textbf{content}. The content of a message is the input or output provided to or by ChatGPT.
CURR LINE 
CURR LINE The role of a message indicates the source of a message. If the content of the messages comes from a user, the role should be \enquote{user} (l. 12). A reply for a query is defined as the role \enquote{assistant} (l. 16). The system role is a more specific role and can be used to include further context for ChatGPT without eliciting a response (l. 8)
CURR LINE 
CURR LINE ChatGPT is stateless. This means that the token id explained above is solely used for accounting purposes and not to store the queries reliably. As a result, if one wants to hold a conversations similar to the browser version,  not only all previous messages from the user must be sent to ChatGPT, but also all replies and the messages of the system role. This should be considered while using the OpenAI \ac{API} because the number of required tokens can strongly increase for longer conversations.
CURR LINE 
CURR LINE \begin{figure}
[
  '    \\caption{Visualization of statelessness of ChatGPT. Rectangles are queries by the user. Ellipses are replies by ChatGPT}'
]
0
CURR LINE \label{fig:chatgpt_stateless}
CURR LINE \end{figure}
CURR LINE 
CURR LINE 
CURR LINE Figure \ref{fig:chatgpt_stateless} visualize this issue. Here, the user ask the ChatGPT API to find all data clump in a java file \textit{(request 1)}. ChatGPT respond with some data clumps. These might not be all data clumps so that a follow-up request might improve the results \cite{10062688}. Therefore, the user sends another request \textit{(request 2)} to instruct the model to find more data clumps. However ChatGPT responds with a general stating that it does not know the content of the file \enquote{MathStuff.java}. This is the result of the statelessness which requires that the user always send the whole context with each query. In \textit{request 3}, the context is provided so that ChatGPT acurately responds by finding another data clump.
----
Figure \reffig:chatgpt_stateless visualize this issue. Here, the user ask the ChatGPT API to find all data clump in a java file  {(request 1)}. ChatGPT respond with some data clumps. These might not be all data clumps so that a follow-up request might improve the results \cite{10062688}. Therefore, the user sends another request \textit{(request 2)} to instruct the model to find more data clumps. However ChatGPT responds with a general stating that it does not know the content of the file  {MathStuff.java}. This is the result of the statelessness which requires that the user always send the whole context with each query. In \textit{request 3}, the context is provided so that ChatGPT acurately responds by finding another data clump.
Figure  ChatGPT respond with some data clumps. These might not be all data clumps so that a follow-up request might improve the results  Therefore, the user sends another request to instruct the model to find more data clumps. However ChatGPT responds with a general stating that it does not know the content of the file  {MathStuff.java}. This is the result of the statelessness which requires that the user always send the whole context with each query. In  the context is provided so that ChatGPT acurately responds by finding another data clump.
----
CURR LINE 
CURR LINE Since follow-up request make only sense if ChatGPT has already responded, the whole original request, the reply by ChatGPT, and any previous conversation must be provided which can have a large impact on costs and rate limits.
CURR LINE 
CURR LINE \subsubsection{Response}
[
  'Query',
  'Listing query may look like if it used via  {curl} which is a tool perform HTTP requests:',
  'undefined Example query to ChatGPT  \\cite{ChatGPT_urlundefined},',
  'label=lst:chatgpt_api,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/chatgpt_api.json',
  '',
  '',
  'Since ChatGPT uses (l. 2). Afterwards, a token must be provided to ChatGPT (l. 23). This  token must be generated on the OpenAI website and will be used to connect the query to an OpenAI account so that the user can be charged properly- The token should remain secret and may not be disclosed (e.~g. via ',
  '',
  'Then, the actual query is defined (l. 4-24.)Firstly, the model is defined (l. 5). OpenAI provides multiple models that have different advantages and disadvantages. For instance, a newer model like  gpt-4 has more capabilities but is more expensive.',
  '',
  'Afterwards, the actual messages are provided (l. 6-24). Each message is a tuple of a  role and a  The content of a message is the input or output provided to or by ChatGPT.',
  '',
  'The role of a message indicates the source of a message. If the content of the messages comes from a user, the role should be  user (l. 12). A reply for a query is defined as the role (l. 16). The system role is a more specific role and can be used to include further context for ChatGPT without eliciting a response (l. 8)',
  '',
  'ChatGPT is stateless. This means that the token id explained above is solely used for accounting purposes and not to store the queries reliably. As a result, if one wants to hold a conversations similar to the browser version,  not only all previous messages from the user must be sent to ChatGPT, but also all replies and the messages of the system role. This should be considered while using the OpenAI ',
  '',
  '',
  '',
  'Figure  ChatGPT respond with some data clumps. These might not be all data clumps so that a follow-up request might improve the results  Therefore, the user sends another request to instruct the model to find more data clumps. However ChatGPT responds with a general stating that it does not know the content of the file  {MathStuff.java}. This is the result of the statelessness which requires that the user always send the whole context with each query. In  the context is provided so that ChatGPT acurately responds by finding another data clump.',
  '',
  'Since follow-up request make only sense if ChatGPT has already responded, the whole original request, the reply by ChatGPT, and any previous conversation must be provided which can have a large impact on costs and rate limits.',
  ''
] [ 2, 3, 1, 2 ]
CURR LINE 
CURR LINE The response of the ChatGPT \ac{API} based on listing \ref{lst:chatgpt_api} is displayed in listing \ref{lst:chatgpt_api_response}.
CURR LINE \begin{figure} [htbp!]
[
  '\t\t\t[caption',
  '{ Example query to ChatGPT  \\cite{ChatGPT_url}},'
]
0
CURR LINE label={lst:chatgpt_api_response},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter2/chatgpt_api_response.json}
CURR LINE \end{figure}
CURR LINE 
CURR LINE In the lower part of the listing (l. 13-19), meta information is provided. This includes the response generation time (l. 12), the used model for the response (l. 14), an id for the response (l. 13).
CURR LINE 
CURR LINE In order for clients to calculate the costs of using ChatGPT, each response also includes how many tokens have been used by the prompt (l. 18), by the response (l. 17), and the total number of tokens (l. 19).
CURR LINE 
CURR LINE In the upper part of the response (l. 1-11), the actual response is provided. The response is an array of so-called choices (l. 2~-~11). Each choice consists of the actual message (l.~ 6-8), which itself consists of the message content (l. 7) and the role of  the content (in most cases this would be \enquote{assistant}).
CURR LINE 
CURR LINE The \enquote{finish\_reason} indicates how the \ac{LLM} has finished on the prompt. If the value is \enquote{stop}, the prompt was executed without faults so that the response is valid. If the value is \enquote{length}, the output would esceed the maximum token limit, so that the output will be incomplete. The value \enquote{content\_filter} indicated the OpenAI censured the requests because it violates the terms of use of OpenAI. \cite{ChatGPT_url}
----
The  finish\_reason indicates how the \ac{LLM} has finished on the prompt. If the value is \enquote{stop}, the prompt was executed without faults so that the response is valid. If the value is \enquote{length}, the output would esceed the maximum token limit, so that the output will be incomplete. The value \enquote{content\_filter} indicated the OpenAI censured the requests because it violates the terms of use of OpenAI. \cite{ChatGPT_url}
The  finishhas finished on the prompt. If the value is  the prompt was executed without faults so that the response is valid. If the value is  the output would esceed the maximum token limit, so that the output will be incomplete. The value indicated the OpenAI censured the requests because it violates the terms of use of OpenAI. 
----
CURR LINE \subsection{Advantages and Challenges on using Large Language Models}\label{sec:llm_challenges}
[
  'Response',
  '',
  'The response of the ChatGPT is displayed in listing ',
  'undefined Example query to ChatGPT  \\cite{ChatGPT_urlundefined},',
  'label=lst:chatgpt_api_response,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter2/chatgpt_api_response.json',
  '',
  'In the lower part of the listing (l. 13-19), meta information is provided. This includes the response generation time (l. 12), the used model for the response (l. 14), an id for the response (l. 13).',
  '',
  'In order for clients to calculate the costs of using ChatGPT, each response also includes how many tokens have been used by the prompt (l. 18), by the response (l. 17), and the total number of tokens (l. 19).',
  '',
  'In the upper part of the response (l. 1-11), the actual response is provided. The response is an array of so-called choices (l. 2~-~11). Each choice consists of the actual message (l.~ 6-8), which itself consists of the message content (l. 7) and the role of  the content (in most cases this would be  assistant).',
  '',
  'The  finishhas finished on the prompt. If the value is  the prompt was executed without faults so that the response is valid. If the value is  the output would esceed the maximum token limit, so that the output will be incomplete. The value indicated the OpenAI censured the requests because it violates the terms of use of OpenAI. '
] [ 2, 3, 2, 0 ]
CURR LINE 
CURR LINE While using  \ac{LLM} for refactoring bring many advantages  and possibilities, using \ac{LLM}  successfully can be challenging. The advantages and disadvantages will be discussed in this subsection. A \textbf{traditional algorithm} as used in this section means any manual refactoring algorithm (e.~g.,~ refactoring data clumps by extracting a class, removing and introducing parameters in methods, and updating all references)
CURR LINE 
CURR LINE \subsubsection{Advantages}
[
  'Advantages and Challenges on using Large Language Models',
  '',
  'While using   successfully can be challenging. The advantages and disadvantages will be discussed in this subsection. A  {traditional algorithm} as used in this section means any manual refactoring algorithm (e.~g.,~ refactoring data clumps by extracting a class, removing and introducing parameters in methods, and updating all references)',
  ''
] [ 2, 3, 2, 1 ]
CURR LINE 
CURR LINE First of all, large language models are very flexible. A normal refactoring algorithm needs to consider many situations. For instance, an algorithm that modifies the method signature in a class might not work on an interface. A large language model does not need to be adapted to all edge cases but often finds a suitable solution to a problem because it is not restricted to a specific refactoring process,
CURR LINE 
CURR LINE Additionally, a \ac{LLM} is more similar to a human as it more  \enquote{creative}. While it is still a computer model and does not win the Turing-Test \cite{turing_test}, a \ac{LLM} can refactor code in a manner more closely as a human being would do. For instance, it can suggest class names that are related to the topic of the class, which a human being would also consider, while traditional algorithm would use placeholder names, concatenation of fields names or other simple name construction algorithms.
----
Additionally, a \acLLM is more similar to a human as it more   {creative}. While it is still a computer model and does not win the Turing-Test \cite{turing_test}, a \ac{LLM} can refactor code in a manner more closely as a human being would do. For instance, it can suggest class names that are related to the topic of the class, which a human being would also consider, while traditional algorithm would use placeholder names, concatenation of fields names or other simple name construction algorithms.
Additionally, a  While it is still a computer model and does not win the Turing-Test  a can refactor code in a manner more closely as a human being would do. For instance, it can suggest class names that are related to the topic of the class, which a human being would also consider, while traditional algorithm would use placeholder names, concatenation of fields names or other simple name construction algorithms.
----
CURR LINE 
CURR LINE Large language models are also extensible. For instance, if another programming language is used, an \ac{LLM} can be easily adapted while a traditional refactoring approach would require more effort to be language-agnostic.
CURR LINE 
CURR LINE Moreover, a \ac{LLM} can refactor the code in more ways than instructed. While  a model can be  specifically instructed to refactor data clumps, it might also correct formatting errors, spelling mistakes, or other code smells. While the focus of this master thesis will be on data clumps, other code smells are important too and might be more serious. Using a  \ac{LLM} allows developers to fix more code smells, without developing and testing more tools to refactor multiple code smells. As they are better to understand the context of the code than a traditional algorithm, the quality of the code can therefore be improved.
CURR LINE 
CURR LINE Furthermore, \ac{LLM} can adapt to the coding style of the source code. If for instance, the source code used the \enquote{snake\_case} or the \enquote{pascalCase} naming convention, the model can detect this convention and use it for its own refactoring (e.g. creating new methods, variables or classes). A traditional approach would need to be configured for each project to use the right convention so that the generated code might look more artificial as it does not fit to the rest of the code.
CURR LINE 
CURR LINE 
CURR LINE \subsubsection{Disadvantages}
[
  'Advantages',
  '',
  'First of all, large language models are very flexible. A normal refactoring algorithm needs to consider many situations. For instance, an algorithm that modifies the method signature in a class might not work on an interface. A large language model does not need to be adapted to all edge cases but often finds a suitable solution to a problem because it is not restricted to a specific refactoring process,',
  '',
  'Additionally, a  While it is still a computer model and does not win the Turing-Test  a can refactor code in a manner more closely as a human being would do. For instance, it can suggest class names that are related to the topic of the class, which a human being would also consider, while traditional algorithm would use placeholder names, concatenation of fields names or other simple name construction algorithms.',
  '',
  'Large language models are also extensible. For instance, if another programming language is used, an ',
  '',
  'Moreover, a allows developers to fix more code smells, without developing and testing more tools to refactor multiple code smells. As they are better to understand the context of the code than a traditional algorithm, the quality of the code can therefore be improved.',
  '',
  'Furthermore, or the naming convention, the model can detect this convention and use it for its own refactoring (e.g. creating new methods, variables or classes). A traditional approach would need to be configured for each project to use the right convention so that the generated code might look more artificial as it does not fit to the rest of the code.',
  '',
  ''
] [ 2, 3, 2, 2 ]
CURR LINE 
CURR LINE First of all \ac{LLM} are not trustworthy. They are often confident in their answers which nevertheless are wrong. This confidence can often be broken by asking subsequent questions which lead the \ac{LLM} to rethink the answer. however, doing this in an automatic way is challenging.
CURR LINE 
CURR LINE Additionally, \ac{LLM} use randomness in their answers which means that the same query can result in different replies. The factors influencing the reply are generally not known and should not be assumed. As a result, requirements regarding a specific output format, may be ignored by the model so that developers using a \ac{LLM} must always consider how to parse non-adhering output.
CURR LINE 
CURR LINE Furthermore, \ac{LLM}s are usually black boxes. They do not give hindsight on how they came to a specific reply. While they can explain their reasoning, it is not possible to check the exact thought process.
CURR LINE While a query can consists of multiple parts, conditions, or requirements, a \ac{LLM} will not always adhere to all of these. It may weight some requirements, ignore other or interpret them wrongly so that the result is unexpected. A \ac{LLM} may also come to an intermediate result that it will not show at the end even though the intermediate result was correct or requested. Also no sources of the information is provided.
CURR LINE 
CURR LINE Moreover, \ac{LLM}s do not have access to the latest information about a topic. They cannot access external sources like current news  and up-to-date documentation. Instead they employ a so-called cut-ff date. Only information before that cut-off date will be used. As of the time of writing this section, the cut-off date for ChatGPt is April  2023. However, the release was several months later.
CURR LINE 
CURR LINE There also security issues with using  \ac{LLM} like ChatGPT. If a model is asked to generate or refactor code, one cannot trust that the code is safe to use. As a result of the cut-off date, the code might use operations that are considered deprecated or even unsafe to use because security vulnerabilities have been detected in the meantime. As a result, the developer needs to verify whether the code is safe to use which is another burden.
CURR LINE 
CURR LINE Furthermore, it is not out of the question that a malicious attacker might change the query or the reply of a \ac{LLM}. Therefore, using such a model might be a feasible way to hack systems or create damage which is difficult to detect and prevent.
CURR LINE 
CURR LINE Lastly, also costs and capacity considerations needs to be observed. For large projects, a \ac{LLM} might be too costly because  the costs are often based on the input size. Therefore, the use of large language models should be adequately prepared so that as much costs as possible can be saved.
CURR LINE \subsection{Consideration while using large language models}
[
  'Disadvantages',
  '',
  'First of all to rethink the answer. however, doing this in an automatic way is challenging.',
  '',
  'Additionally, must always consider how to parse non-adhering output.',
  '',
  'Furthermore, ',
  'While a query can consists of multiple parts, conditions, or requirements, a may also come to an intermediate result that it will not show at the end even though the intermediate result was correct or requested. Also no sources of the information is provided.',
  '',
  'Moreover, ',
  '',
  'There also security issues with using  ',
  '',
  'Furthermore, it is not out of the question that a malicious attacker might change the query or the reply of a ',
  '',
  'Lastly, also costs and capacity considerations needs to be observed. For large projects, a '
] [ 2, 3, 3, 0 ]
CURR LINE In order to use a \ac{LLM} more effectively, the prompts and queries need to be modeled in a specific manner so that they are interpreted correctly. While deviating from these manners may still produce correct results, it nevertheless increases the risk of wrong results. The following tips were derived from the OpenAI documentation \cite{ChatGPT_url} and hence apply only to ChatGPT. Generally speaking. However, many recommendations will work for other \ac{LLM} too because they will help to make the prompt more clearly and prevent misunderstanding:
----
In order to use a \acLLM more effectively, the prompts and queries need to be modeled in a specific manner so that they are interpreted correctly. While deviating from these manners may still produce correct results, it nevertheless increases the risk of wrong results. The following tips were derived from the OpenAI documentation \cite{ChatGPT_url} and hence apply only to ChatGPT. Generally speaking. However, many recommendations will work for other \ac{LLM} too because they will help to make the prompt more clearly and prevent misunderstanding:
In order to use a and hence apply only to ChatGPT. Generally speaking. However, many recommendations will work for other too because they will help to make the prompt more clearly and prevent misunderstanding:
----
CURR LINE 
CURR LINE \subsubsection{Separate instruction and input}
[
  'Consideration while using large language models',
  'In order to use a and hence apply only to ChatGPT. Generally speaking. However, many recommendations will work for other too because they will help to make the prompt more clearly and prevent misunderstanding:',
  ''
] [ 2, 3, 3, 1 ]
CURR LINE Many queries to \ac{LLM} include an instruction and an input. For instance, a query to find and refactor data clumps could  provide the source code containing the possible data clumps \textbf{(input)}. The instruction could be the query \enquote{Find and refactor all data clumps in this source code}.
CURR LINE 
CURR LINE OpenAI recommends that the instructions and input be separated as distinctive as possible. It suggests enclosing the input in a block of \textit{"""} or \textit{\#\#\#} to mark what the input and what the instruction is clearly.
CURR LINE 
CURR LINE \subsubsection{Provide detailed context and how the model should respond}
[
  'Separate instruction and input',
  'Many queries to  The instruction could be the query  {Find and refactor all data clumps in this source code}.',
  '',
  'OpenAI recommends that the instructions and input be separated as distinctive as possible. It suggests enclosing the input in a block of  """ or to mark what the input and what the instruction is clearly.',
  ''
] [ 2, 3, 3, 2 ]
CURR LINE 
CURR LINE When generating a reply to a query, a language model will use the available context to process the query and generate an output that attempts satisfy the user's need. This means that every bit of relevant information can help the language model to generate a better response.
CURR LINE 
CURR LINE On the contrary, providing irrelevant information can increase the chance of wrong responses, so the creator of a query must always consider what to include in a query and what not.
CURR LINE 
CURR LINE In the context of data clump refactoring, the query should include the content of the source code and the programming language. However, files that cannot have data clumps (e.g., configuration files) should not be included.
CURR LINE 
CURR LINE An instruction for refactoring data clumps should state that only the refactored source code files should be returned without providing explanatory texts or other information, as they can hinder the parsing of the output.
CURR LINE 
CURR LINE When using the \enquote{gpt-4-1106-preview} or \enquote{gpt-3.5-turbo-1106} model of the OpenAI-\ac{API}, developers can force the model to response in \ac{JSON}. hence, the output can be made more predictable and easier to control. A request using this mode must include the term \enquote{\ac{JSON}}. It should however be noted, that the precise structure of the \ac{JSON} returned may still differ from the request.
CURR LINE 
CURR LINE \subsubsection{Politeness is not necessary}
[
  'Provide detailed context and how the model should respond',
  '',
  "When generating a reply to a query, a language model will use the available context to process the query and generate an output that attempts satisfy the user's need. This means that every bit of relevant information can help the language model to generate a better response.",
  '',
  'On the contrary, providing irrelevant information can increase the chance of wrong responses, so the creator of a query must always consider what to include in a query and what not.',
  '',
  'In the context of data clump refactoring, the query should include the content of the source code and the programming language. However, files that cannot have data clumps (e.g., configuration files) should not be included.',
  '',
  'An instruction for refactoring data clumps should state that only the refactored source code files should be returned without providing explanatory texts or other information, as they can hinder the parsing of the output.',
  '',
  'When using the  gpt-4-1106-preview or model of the OpenAI- developers can force the model to response in  hence, the output can be made more predictable and easier to control. A request using this mode must include the term . It should however be noted, that the precise structure of the returned may still differ from the request.',
  ''
] [ 2, 3, 3, 3 ]
CURR LINE 
CURR LINE Polite words like \enquote{please} or \enquote{thank you} are expected in human conversations. However, a  \ac{LLM} will ignore these words that are often referred to as \enquote{filler} words, or their effect will be marginal. Since costs are an important part of the usage of \ac{LLM}, these words are often unnecessary. \cite{bsharat2023principled}.
----
Polite words like  please or \enquote{thank you} are expected in human conversations. However, a  \ac{LLM} will ignore these words that are often referred to as \enquote{filler} words, or their effect will be marginal. Since costs are an important part of the usage of \ac{LLM}, these words are often unnecessary. \cite{bsharat2023principled}.
Polite words like  please or are expected in human conversations. However, a  will ignore these words that are often referred to as words, or their effect will be marginal. Since costs are an important part of the usage of  these words are often unnecessary. 
----
CURR LINE 
CURR LINE \subsubsection{Avoid using prohibition but suggest alternatives}
[
  'Politeness is not necessary',
  '',
  'Polite words like  please or are expected in human conversations. However, a  will ignore these words that are often referred to as words, or their effect will be marginal. Since costs are an important part of the usage of  these words are often unnecessary. ',
  ''
] [ 2, 3, 3, 4 ]
CURR LINE 
CURR LINE Providing prohibitions to a \ac{LLM} may sound like a valid approach to restrict certain outputs. However, they can degrade the quality of the results. For example, if a \ac{LLM} has found a solution that violates a prohibition, it does not know how to proceed.
CURR LINE 
CURR LINE In contrast, giving positive instructions, the \ac{LLM} can better determine how the user expects the output so that the quality is increased.
CURR LINE 
CURR LINE For instance, instructing a \ac{LLM} to not write informally can result to worse outputs than an instruction to use formal language because formal language is better defined than the negation of informal language. \cite{prompt_engineering_jonathan}
----
For instance, instructing a \acLLM to not write informally can result to worse outputs than an instruction to use formal language because formal language is better defined than the negation of informal language. \cite{prompt_engineering_jonathan}
For instance, instructing a 
----
CURR LINE 
CURR LINE \subsubsection{Provide rewards or punishments to the LLM}
[
  'Avoid using prohibition but suggest alternatives',
  '',
  'Providing prohibitions to a has found a solution that violates a prohibition, it does not know how to proceed.',
  '',
  'In contrast, giving positive instructions, the ',
  '',
  'For instance, instructing a ',
  ''
] [ 2, 3, 3, 5 ]
CURR LINE 
CURR LINE While a \ac{LLM} is not a human, its result can be influenced by rewards or punishments. For instance, the authors in \cite{bsharat2023principled} claim that promising the model a monetary amount, can lead to better results.
----
While a \acLLM is not a human, its result can be influenced by rewards or punishments. For instance, the authors in \cite{bsharat2023principled} claim that promising the model a monetary amount, can lead to better results.
While a claim that promising the model a monetary amount, can lead to better results.
----
CURR LINE 
CURR LINE Similarly, informing the model that certain results will be punished, can decrease the likelihood that these results will be returned. \cite{bsharat2023principled}
----
Similarly, informing the model that certain results will be punished, can decrease the likelihood that these results will be returned. \citebsharat2023principled
Similarly, informing the model that certain results will be punished, can decrease the likelihood that these results will be returned. 
----
CURR LINE \begin{comment}
found begin
CURR LINE \subsection{Cost reduction}
found end
CURR LINE 
found end
CURR LINE Many \ac{LLM} are not for free but must be paid based on usage or other factors. Even if a \ac{LLM} is free, there are many restrictions for the data to be processed by the model, so special care needs to be taken to reduce the data size as well as possible.  The cost might already be reduced by following the steps in section \ref{sec:prompt_engineering}. Nevertheless, there are other factors that will be outlined:
found end
CURR LINE \end{comment}
found end
CURR LINE 
CURR LINE \subsubsection{Chain-of-Thought Prompting}\label{sec:chain of thought}
[
  'Provide rewards or punishments to the LLM',
  '',
  'While a claim that promising the model a monetary amount, can lead to better results.',
  '',
  'Similarly, informing the model that certain results will be punished, can decrease the likelihood that these results will be returned. ',
  ''
] [ 2, 3, 3, 6 ]
CURR LINE 
CURR LINE Another approach to improve the results from \ac{LLM} like ChatGPT is to separate a query into interconnected sub queries that lead to a chain of thought. This can be compared to human thinking process because a human alone tries not to solve a problem at once but breaks it downs into simpler problem that are still connected to but easier to solve. \cite{Wei2022ChainOT}
----
Another approach to improve the results from \acLLM like ChatGPT is to separate a query into interconnected sub queries that lead to a chain of thought. This can be compared to human thinking process because a human alone tries not to solve a problem at once but breaks it downs into simpler problem that are still connected to but easier to solve. \cite{Wei2022ChainOT}
Another approach to improve the results from 
----
CURR LINE 
CURR LINE For instance, a query to find and refactor data clumps can be separate into a detection query and a refactoring query. These sub-queries can be further divided into a query for each file or for all files in a single directory. As a result, not one a single query needs but multiple queries.
CURR LINE 
CURR LINE This is useful if a human is reviewing the output from a \ac{LLM} since obvious errors can be spotted more easily if the task is divided into multiple  step. However, it is more challenging for an automated tool since it cannot find errors in the same way.
CURR LINE 
CURR LINE Furthermore, one should consider that each queries requires further overhead so that the performance might be impacted more negatively.
CURR LINE \section{Language Server Protocol} \label{sec:lsp}
[
  'Chain-of-Thought Prompting',
  '',
  'Another approach to improve the results from ',
  '',
  'For instance, a query to find and refactor data clumps can be separate into a detection query and a refactoring query. These sub-queries can be further divided into a query for each file or for all files in a single directory. As a result, not one a single query needs but multiple queries.',
  '',
  'This is useful if a human is reviewing the output from a ',
  '',
  'Furthermore, one should consider that each queries requires further overhead so that the performance might be impacted more negatively.'
] [ 2, 4, 0, 0 ]
CURR LINE The \ac{LSP}  \cite{lsp_website} is a protocol developed by Microsoft to create a language-independent interface for code-related operations.
----
The \acLSP  \cite{lsp_website} is a protocol developed by Microsoft to create a language-independent interface for code-related operations.
The is a protocol developed by Microsoft to create a language-independent interface for code-related operations.
----
CURR LINE The \ac{LSP}  describes a server and a client who communicate to each other using the JSON-Remote-Process-Call protocol \cite{json_rpc}.
----
The \acLSP  describes a server and a client who communicate to each other using the JSON-Remote-Process-Call protocol \cite{json_rpc}.
The 
----
CURR LINE 
CURR LINE In general, the client can be anything that works with source code but has no detailed knowledge of a specific programming language. For instance, an \ac{IDE}, an editor or a refactoring tool can be described as a client.
CURR LINE 
CURR LINE The client starts a server based on a programming language. The server has a inherent knowledge of one ore more programming languages and can provide source-code-related functionality. For instance, the server can rename a variable, find the usages of a method, or inform the client about compiler errors.
CURR LINE 
CURR LINE Initially, client and server share some information to set-up. This includes the path to the project that the server should load and information about what functionality each of them support. These functionalities are named \textit{Capabilities}. For example, the server can announce that it supports renaming variables while the client show error messages. Hence, server and client can interact in a manner such that no unsupported messages are transferred.
CURR LINE 
CURR LINE Figure \ref{fig:lsp_usage} illustrates how the \ac{LSP} works in practice after the initialization process.
CURR LINE \begin{figure}
[ '    \\caption{Example usage of the Language Server Protocol}' ]
0
CURR LINE \label{fig:lsp_usage}
CURR LINE \cite{lsp_website}
CURR LINE \end{figure}
CURR LINE 
CURR LINE After the server has successfully started by the client, a user can open a document (e.g. source code file). The request to open the document is submitted to the server. From now one, the server may not rely on the file system since that might be not the current version of the opened document.
CURR LINE 
CURR LINE The client can now inform the server about some changes (e.g. adding a new method). The server can in the meantime inform the client about syntactical errors which the client might show to the user of the client.
CURR LINE 
CURR LINE Afterwards, the client requests the definition of a method or variable, and the server returns a response with the requested data.
CURR LINE 
CURR LINE In the end, the client can save the document and notify the server that the document was closed which means that the physical file of the document represents the current version of the document again.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \section{Related Research}
[
  'Language Server Protocol',
  'The is a protocol developed by Microsoft to create a language-independent interface for code-related operations.',
  'The ',
  '',
  'In general, the client can be anything that works with source code but has no detailed knowledge of a specific programming language. For instance, an ',
  '',
  'The client starts a server based on a programming language. The server has a inherent knowledge of one ore more programming languages and can provide source-code-related functionality. For instance, the server can rename a variable, find the usages of a method, or inform the client about compiler errors.',
  '',
  'Initially, client and server share some information to set-up. This includes the path to the project that the server should load and information about what functionality each of them support. These functionalities are named  Capabilities. For example, the server can announce that it supports renaming variables while the client show error messages. Hence, server and client can interact in a manner such that no unsupported messages are transferred.',
  '',
  'Figure works in practice after the initialization process.',
  '',
  'After the server has successfully started by the client, a user can open a document (e.g. source code file). The request to open the document is submitted to the server. From now one, the server may not rely on the file system since that might be not the current version of the opened document.',
  '',
  'The client can now inform the server about some changes (e.g. adding a new method). The server can in the meantime inform the client about syntactical errors which the client might show to the user of the client.',
  '',
  'Afterwards, the client requests the definition of a method or variable, and the server returns a response with the requested data.',
  '',
  'In the end, the client can save the document and notify the server that the document was closed which means that the physical file of the document represents the current version of the document again.',
  '',
  '',
  ''
] [ 2, 5, 0, 0 ]
CURR LINE The problem of data clump detection and refactoring is addressed in multiple papers. Also the  use of large language models in software development is a fairly recent rearch topic. In this section, research to both topics will be outlined:
CURR LINE 
CURR LINE \subsection{Related to data clumps}
[
  'Related Research',
  'The problem of data clump detection and refactoring is addressed in multiple papers. Also the  use of large language models in software development is a fairly recent rearch topic. In this section, research to both topics will be outlined:',
  ''
] [ 2, 5, 1, 0 ]
CURR LINE Baumgartner et al.  developed a live code smell detection plugin for IntelliJ that can detect, report, and refactor data clumps without significantly impacting performance. However, the tool is semi-automatic, meaning the developer must still actively approve the data clump refactoring and suggest a suitable class name for the extracted class. \cite{BaumgartnerAP23}
----
Baumgartner et al.  developed a live code smell detection plugin for IntelliJ that can detect, report, and refactor data clumps without significantly impacting performance. However, the tool is semi-automatic, meaning the developer must still actively approve the data clump refactoring and suggest a suitable class name for the extracted class. \citeBaumgartnerAP23
Baumgartner et al.  developed a live code smell detection plugin for IntelliJ that can detect, report, and refactor data clumps without significantly impacting performance. However, the tool is semi-automatic, meaning the developer must still actively approve the data clump refactoring and suggest a suitable class name for the extracted class. 
----
CURR LINE 
CURR LINE As outlined in section \ref{sec:data_clump_def}, the definition of data clump by Fowler \cite{fowler2019refactoring} is somewhat ambiguous because no clear criteria to determine data clumps is established. Zang et al. \cite{zhangImprovingPrecisionFowler2008} creates a more algorithmic approach to determine whether a data clump exists. This approach is also explained in section \ref{sec:data_clump_def}. The authors also provide more precise definitions of other code smells like \enquote{message chains} or \enquote{speculative generality}. By interviewing four software development experts about the code smell definitions the authors developed, they find that their new data clump definition receives relatively more disagreement than other definitions, which the authors explain are the results of not covering edge cases in the definitions.
----
As outlined in section \refsec:data_clump_def, the definition of data clump by Fowler \cite{fowler2019refactoring} is somewhat ambiguous because no clear criteria to determine data clumps is established. Zang et al. \cite{zhangImprovingPrecisionFowler2008} creates a more algorithmic approach to determine whether a data clump exists. This approach is also explained in section \ref{sec:data_clump_def}. The authors also provide more precise definitions of other code smells like  {message chains} or \enquote{speculative generality}. By interviewing four software development experts about the code smell definitions the authors developed, they find that their new data clump definition receives relatively more disagreement than other definitions, which the authors explain are the results of not covering edge cases in the definitions.
As outlined in section is somewhat ambiguous because no clear criteria to determine data clumps is established. Zang et al. creates a more algorithmic approach to determine whether a data clump exists. This approach is also explained in section  The authors also provide more precise definitions of other code smells like  {message chains} or  By interviewing four software development experts about the code smell definitions the authors developed, they find that their new data clump definition receives relatively more disagreement than other definitions, which the authors explain are the results of not covering edge cases in the definitions.
----
CURR LINE 
CURR LINE 
CURR LINE Hall et al. analyzed the impact of code smells (including data clump) on the occurrence of faults in three open-source software projects. They find that data clumps have a mixed correlation to faults because, in two of the three projects analyzed, the correlation of data clumps per \ac{LOC} to detected faults is negative for two projects and positive for one project. This rejects their hypothesis that data clumps do not affect faults, and the authors suggest that the application domain and the development context need to be considered before the time-consuming refactoring of data clumps since their impact is not predictable.  \cite{hallCodeSmellsHave2014}
----
Hall et al. analyzed the impact of code smells (including data clump) on the occurrence of faults in three open-source software projects. They find that data clumps have a mixed correlation to faults because, in two of the three projects analyzed, the correlation of data clumps per \acLOC to detected faults is negative for two projects and positive for one project. This rejects their hypothesis that data clumps do not affect faults, and the authors suggest that the application domain and the development context need to be considered before the time-consuming refactoring of data clumps since their impact is not predictable.  \cite{hallCodeSmellsHave2014}
Hall et al. analyzed the impact of code smells (including data clump) on the occurrence of faults in three open-source software projects. They find that data clumps have a mixed correlation to faults because, in two of the three projects analyzed, the correlation of data clumps per 
----
CURR LINE 
CURR LINE \subsection{Related to large language models in software development}
[
  'Related to data clumps',
  'Baumgartner et al.  developed a live code smell detection plugin for IntelliJ that can detect, report, and refactor data clumps without significantly impacting performance. However, the tool is semi-automatic, meaning the developer must still actively approve the data clump refactoring and suggest a suitable class name for the extracted class. ',
  '',
  'As outlined in section is somewhat ambiguous because no clear criteria to determine data clumps is established. Zang et al. creates a more algorithmic approach to determine whether a data clump exists. This approach is also explained in section  The authors also provide more precise definitions of other code smells like  {message chains} or  By interviewing four software development experts about the code smell definitions the authors developed, they find that their new data clump definition receives relatively more disagreement than other definitions, which the authors explain are the results of not covering edge cases in the definitions.',
  '',
  '',
  'Hall et al. analyzed the impact of code smells (including data clump) on the occurrence of faults in three open-source software projects. They find that data clumps have a mixed correlation to faults because, in two of the three projects analyzed, the correlation of data clumps per ',
  ''
] [ 2, 5, 2, 0 ]
CURR LINE 
CURR LINE White et al. \cite{White2023ChatGPTPP} outline how ChatGPT can be used in software development to improve the worklflow of developers. This includes  exploration of requirements, removing ambiguity in technical specification, or describing source code. The authors suggest specific prompts to elicit a suitable response from ChatGPt.
----
White et al. \citeWhite2023ChatGPTPP outline how ChatGPT can be used in software development to improve the worklflow of developers. This includes  exploration of requirements, removing ambiguity in technical specification, or describing source code. The authors suggest specific prompts to elicit a suitable response from ChatGPt.
White et al. 
----
CURR LINE 
CURR LINE In case of refactoring, the author suggest that ChatGPT is able to refactor code with multiple prompts. For instance, ChatGPT can refactor based on a well known design pattern name, multiple examples on how to refactor the code, or a more lengthy requirements description. The exact success of each prompt is however dependent on how ChatGPT is trained and should be scrutinized manually.
CURR LINE 
CURR LINE Cao et al. \cite{cao2023study} focus on using ChatGPT for fixing deep learning programs. Those are programs that cannot be understood only by their source code alone, but are are largely influenced by underlying data like  neural networks etc. This makes finding faults more difficult. The study finds out that ChatGPT can find code smells and detect faults. Without giving instructions however, ChatGPT will tend to return code smells and outdated API calls while not finding other bugs.
----
Cao et al. \citecao2023study focus on using ChatGPT for fixing deep learning programs. Those are programs that cannot be understood only by their source code alone, but are are largely influenced by underlying data like  neural networks etc. This makes finding faults more difficult. The study finds out that ChatGPT can find code smells and detect faults. Without giving instructions however, ChatGPT will tend to return code smells and outdated API calls while not finding other bugs.
Cao et al. 
----
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \input{Main/konzeption}
input
masterthesis/Main/konzeption
CURR LINE \begingroup
CURR LINE \renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
CURR LINE \renewcommand{\clearpage}{}
CURR LINE \chapter{Concept}\label{chapter_conception}
[ '' ] [ 3, 0, 0, 0 ]
CURR LINE \endgroup
CURR LINE This chapter deals with the concept of the program. It provides information about the processing pipeline (\ref{sec:pipeline} and how the different parts of that pipeline can be modeled in a way that allows easy extension.
CURR LINE 
CURR LINE 
CURR LINE \hfill
CURR LINE \section{Pipeline}\label{sec:pipeline}
[
  'Concept',
  'This chapter deals with the concept of the program. It provides information about the processing pipeline (',
  '',
  ''
] [ 3, 1, 0, 0 ]
CURR LINE In order to find and refactor data clumps automatically, a particular sequence of steps \textbf{(pipeline)} has to be respected. Most steps of this sequence must be in a specific order because they rely on information extracted in a previous step, or the quality of the results (which might be needed by subsequent steps) would worsen.
CURR LINE 
CURR LINE Each step of the pipeline is performed by a  \textbf{Handler}. A handler might handle one or more steps.  Each handler has information about what steps of the pipeline it supports and  a handler can be registered to a pipeline step if it supports the particular step.
CURR LINE 
CURR LINE Since a service-based approach is used, a handler can be seen as a gateway to a different program or service that performs the specific functionality.  The handler and the respective service are tightly coupled. While a handler could deal with multiple services, it is not required by the design, and requires special care. Nevertheless, it recommendable to write the handler as abstract as possible.
CURR LINE \begin{figure}
[
  '    \\caption{Visualization of communication to services for one pipeline step}'
]
0
CURR LINE \label{fig:solver_gateway_service_overview}
CURR LINE \end{figure}
CURR LINE 
CURR LINE Figure \ref{fig:solver_gateway_service_overview} illustrates how the service-based approach works for one step:
CURR LINE 
CURR LINE \begin{enumerate}
found begin
CURR LINE \item The program developed in this master thesis \textbf{main program} executes one step (e.g. finding a suitable class name)
found end
CURR LINE \item The gateway registered to that step is used. It is provided with the current context containing the results from previous steps.
found end
CURR LINE \item The gateway starts or employs a specific service. This service might be dependent on the current step or might be constant. The gateway knows how to start or use the service and sends all relevant information to the service (e.g. via files, network streams etc.)
found end
CURR LINE \item The service processes the requests submitted by the gateway. For instance, ChatGPt can suggest a suitable class name. The service might be outside of the control of the developer. For instance, it might be programmed in a different programming language or the source code might not be available. At some point it will create a response and send it back to the gateway.
found end
CURR LINE \item The gateway reads the response and processed it. It then creates a new context based on the previous context and the response from the service.
found end
CURR LINE \item The new context is returned to the program and can be used for successive steps (e.~g. the suggested name can be used for creating a respective class)
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE Some parts of the pipeline are mandatory because the purpose of finding and refactoring data clumps would be defeated if these steps would not be executed. For instance, obtaining the source code of the project to analyze, finding data clumps, finding a suitable name, and the refactoring can be regarded as mandatory.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE A handler can also check whether it can be run in the given situation. For instance, a code obtaining handler can determine whether the project location does in fact exist or a validation handler for \textit{Gradle} can check whether the project is indeed \textit{Gradle}-based.
CURR LINE 
CURR LINE The program therefore acts as an orchestrator. It starts every service and controls them. The steps and services are temporally coupled. A step cannot be started unless all previous mandatory steps have successfully completed. In most cases, if a step was not successfully, further execution of the pipeline makes no sense so it can be aborted.
CURR LINE 
CURR LINE An alternative approach is called  choreography. Here each step would call the next step without using the central main program.
CURR LINE 
CURR LINE Both approaches have their advantages. In the choreography architecture, there is less coupling since a main program does not need to know which gateway handles with which step. Instead, each gateway need to know the next (and only the next) step. This can become challenging if steps are optional because the gateway does not know what service to call next. Additionally debugging, the process is more challenging as there is no central point that coordinates the pipeline.
CURR LINE 
CURR LINE On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each gateway is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but accessed using the internet or the local network which causes more overhead. \cite{orchestration_choreography}
----
On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each gateway is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but accessed using the internet or the local network which causes more overhead. \citeorchestration_choreography
On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each gateway is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but accessed using the internet or the local network which causes more overhead. 
----
CURR LINE 
CURR LINE In the following subsections, these steps will be outlined:
CURR LINE \subsubsection{Code obtaining}\label{sec:code_obtaining}
[
  'Pipeline',
  'In order to find and refactor data clumps automatically, a particular sequence of steps  (pipeline) has to be respected. Most steps of this sequence must be in a specific order because they rely on information extracted in a previous step, or the quality of the results (which might be needed by subsequent steps) would worsen.',
  '',
  'Each step of the pipeline is performed by a   Handler. A handler might handle one or more steps.  Each handler has information about what steps of the pipeline it supports and  a handler can be registered to a pipeline step if it supports the particular step.',
  '',
  'Since a service-based approach is used, a handler can be seen as a gateway to a different program or service that performs the specific functionality.  The handler and the respective service are tightly coupled. While a handler could deal with multiple services, it is not required by the design, and requires special care. Nevertheless, it recommendable to write the handler as abstract as possible.',
  '',
  'Figure ',
  '',
  '',
  '',
  '',
  'Some parts of the pipeline are mandatory because the purpose of finding and refactoring data clumps would be defeated if these steps would not be executed. For instance, obtaining the source code of the project to analyze, finding data clumps, finding a suitable name, and the refactoring can be regarded as mandatory.',
  '',
  '',
  '',
  'A handler can also check whether it can be run in the given situation. For instance, a code obtaining handler can determine whether the project location does in fact exist or a validation handler for  Gradle can check whether the project is indeed based.',
  '',
  'The program therefore acts as an orchestrator. It starts every service and controls them. The steps and services are temporally coupled. A step cannot be started unless all previous mandatory steps have successfully completed. In most cases, if a step was not successfully, further execution of the pipeline makes no sense so it can be aborted.',
  '',
  'An alternative approach is called  choreography. Here each step would call the next step without using the central main program.',
  '',
  'Both approaches have their advantages. In the choreography architecture, there is less coupling since a main program does not need to know which gateway handles with which step. Instead, each gateway need to know the next (and only the next) step. This can become challenging if steps are optional because the gateway does not know what service to call next. Additionally debugging, the process is more challenging as there is no central point that coordinates the pipeline.',
  '',
  'On the other hand, the orchestrator architecture is easier to control and debug. It easier to configure and to implement. Nevertheless, the coupling to each gateway is stronger because each gateway must be known by by the main program. Additionally, the main program is a single point of failure. If there is a problem with the main program, the data clump refactoring cannot proceed as every step requires coordination with the orchestrator. Also the overhead should be considered because the orchestrator will need to act multiple time to process responses and generate new requests for the next gateway. This becomes more important if services are not local but accessed using the internet or the local network which causes more overhead. ',
  '',
  'In the following subsections, these steps will be outlined:'
] [ 3, 1, 0, 1 ]
CURR LINE Data clumps can only be found if access to the source code is given. This requires that the source code is available in some location and that the location is known.
CURR LINE 
CURR LINE In most cases, this is trivial since a developer needing to find and fix data clumps should know where the source code is located.
CURR LINE 
CURR LINE However, there might be more complex cases. For instance, the source code could be located on a \ac{VCS} or must be downloaded elsewhere so that the newest changes can be considered. Therefore, defining a specific step for obtaining the source code before executing the rest of the pipeline is useful.
CURR LINE \subsubsection{Detecting data clumps}\label{subsec:chap3_data_clump_detection}
[
  'Code obtaining',
  'Data clumps can only be found if access to the source code is given. This requires that the source code is available in some location and that the location is known.',
  '',
  'In most cases, this is trivial since a developer needing to find and fix data clumps should know where the source code is located.',
  '',
  'However, there might be more complex cases. For instance, the source code could be located on a '
] [ 3, 1, 0, 2 ]
CURR LINE It is easily agreeable that data clumps can only be fixed if found previously. Therefore, one of the first steps of the pipeline must be the detection of data clumps.
CURR LINE 
CURR LINE The data clump detection process itself can be further divided into sub-steps.
CURR LINE \subsubsection{Filtering}\label{subsub:filtering_files}
[
  'Detecting data clumps',
  'It is easily agreeable that data clumps can only be fixed if found previously. Therefore, one of the first steps of the pipeline must be the detection of data clumps.',
  '',
  'The data clump detection process itself can be further divided into sub-steps.'
] [ 3, 1, 0, 3 ]
CURR LINE First of all, all files of the programming language need to be detected. Usually, there are specific file extensions (e.g., \textit{.java}) for a source code file, simplifying the finding of files. It is also common for software projects to have a root folder that contains all source code files. Therefore, one can recursively iterate through all files and directories of a given root folder with a matching extension.
CURR LINE 
CURR LINE However, not every source code file might be analyzed for data clumps. For instance, parts of the projects must not be refactored, or a re-factorization would be too time-consuming. As outlined in chapter \ref{sec:introduction}, the cost of detecting data clumps might need to be lowered. Therefore, filtering rules might be necessary to reduce the size of the relevant files for the subsequent steps.
CURR LINE 
CURR LINE \subsubsection{Extraction of AST}
[
  'Filtering',
  'First of all, all files of the programming language need to be detected. Usually, there are specific file extensions (e.g.,  .java) for a source code file, simplifying the finding of files. It is also common for software projects to have a root folder that contains all source code files. Therefore, one can recursively iterate through all files and directories of a given root folder with a matching extension.',
  '',
  'However, not every source code file might be analyzed for data clumps. For instance, parts of the projects must not be refactored, or a re-factorization would be too time-consuming. As outlined in chapter ',
  ''
] [ 3, 1, 0, 4 ]
CURR LINE Then,  the source code needs to be simplified as comments, the method bodies, and other language parts are irrelevant for detecting data clumps. This could be achieved by extracting a subset of the \ac{AST} from the source code, which includes all the relevant information to identify data clumps.
CURR LINE \subsubsection{Similarity detection}
[
  'Extraction of AST',
  'Then,  the source code needs to be simplified as comments, the method bodies, and other language parts are irrelevant for detecting data clumps. This could be achieved by extracting a subset of the '
] [ 3, 1, 0, 5 ]
CURR LINE 
CURR LINE The next step is finding pairs of method parameters and pairs of identical or at least similar fields. For this, the identifier and the data type can be compared. Unequal variable names might not rule out a similarity between two variables. For instance, typos may happen, or synonyms may be used. Also, the type might be different. For instance, the data type \textit{double} can be seen as a super-set of the datatype \textit{int} because every 32-bit integer can be converted to a \textit{double}.
CURR LINE 
CURR LINE From this, a graph can be constructed that visualizes the relationship of the several variables. A node represents a parameter or field value, while an edge exists if and only if a relationship between two variables is detected.
CURR LINE 
CURR LINE \subsubsection{Data clump detection}
[
  'Similarity detection',
  '',
  'The next step is finding pairs of method parameters and pairs of identical or at least similar fields. For this, the identifier and the data type can be compared. Unequal variable names might not rule out a similarity between two variables. For instance, typos may happen, or synonyms may be used. Also, the type might be different. For instance, the data type  double can be seen as a super-set of the datatype because every 32-bit integer can be converted to a ',
  '',
  'From this, a graph can be constructed that visualizes the relationship of the several variables. A node represents a parameter or field value, while an edge exists if and only if a relationship between two variables is detected.',
  ''
] [ 3, 1, 0, 6 ]
CURR LINE After similar variables have been detected, it must be determined whether a cluster of variables that are deemed similar is, in fact, a data clump. Here, the rules from section \ref{sec:data_clump_def} can be used. However, defining one's own rules can lead to better results, so flexibility is essential, too.
CURR LINE 
CURR LINE \subsubsection{Data clump filtering or prioritization} \label{subsub:filtering_data_clumps}
[
  'Data clump detection',
  'After similar variables have been detected, it must be determined whether a cluster of variables that are deemed similar is, in fact, a data clump. Here, the rules from section ',
  ''
] [ 3, 1, 0, 7 ]
CURR LINE In section \ref{subsub:filtering_files}, a filtering by files is performed. However, even if many data clumps were detected, they do not always need to be refactored. One can decide to ignore certain data clumps and refactor them later or even never.
CURR LINE 
CURR LINE There can be many reasons for this approach. For instance, parts of the source code that are infrequently updated might be too risky for refactoring since any automatic tool might cause bugs or invalid code that is hard to fix because the developers are unfamiliar with this part of the source code. On the contrary, a part of the source code that is updated more frequently might be in an intermittent state, so that refactoring is currently not recommendable. Also, as noted in section  \ref{subsub:filtering_files}, if a service like ChatGPT is used, the cost of transferring many source code files with data clumps might be too high.
CURR LINE 
CURR LINE In the case of field data clumps, we also have to consider the worst case, namely that at least one of the data clump fields is public. This is considered bad design as fields should always be encapsulated by getters and setters \cite{5680918}. Nevertheless, it is still prevalent \cite{5076631}.
----
In the case of field data clumps, we also have to consider the worst case, namely that at least one of the data clump fields is public. This is considered bad design as fields should always be encapsulated by getters and setters \cite5680918. Nevertheless, it is still prevalent \cite{5076631}.
In the case of field data clumps, we also have to consider the worst case, namely that at least one of the data clump fields is public. This is considered bad design as fields should always be encapsulated by getters and setters 
----
CURR LINE 
CURR LINE A subsequent refactoring of these fields is more complicated than that of less accessible fields because they might be used by external programs or libraries that are beyond the control of the developer, which might induce bugs or non-compilable software.
CURR LINE 
CURR LINE One can argue that refactoring should never affect the public interface since the above-mentioned issues can occur \cite{10.1145/1352678.1352681}.
----
One can argue that refactoring should never affect the public interface since the above-mentioned issues can occur \cite10.1145/1352678.1352681.
One can argue that refactoring should never affect the public interface since the above-mentioned issues can occur 
----
CURR LINE 
CURR LINE On the other hand, one can argue that those changes are still significant because they can improve readability, and the source code may be in the early stages to make such breaking changes feasible.
CURR LINE 
CURR LINE Therefore, filtering out certain data clumps considered not worthy of refactoring can be suggested.
CURR LINE 
CURR LINE \subsubsection{Name finding}\label{subsec:chap3_data_clump_name_finding}
[
  'Data clump filtering or prioritization',
  'In section ',
  '',
  'There can be many reasons for this approach. For instance, parts of the source code that are infrequently updated might be too risky for refactoring since any automatic tool might cause bugs or invalid code that is hard to fix because the developers are unfamiliar with this part of the source code. On the contrary, a part of the source code that is updated more frequently might be in an intermittent state, so that refactoring is currently not recommendable. Also, as noted in section  ',
  '',
  'In the case of field data clumps, we also have to consider the worst case, namely that at least one of the data clump fields is public. This is considered bad design as fields should always be encapsulated by getters and setters ',
  '',
  'A subsequent refactoring of these fields is more complicated than that of less accessible fields because they might be used by external programs or libraries that are beyond the control of the developer, which might induce bugs or non-compilable software.',
  '',
  'One can argue that refactoring should never affect the public interface since the above-mentioned issues can occur ',
  '',
  'On the other hand, one can argue that those changes are still significant because they can improve readability, and the source code may be in the early stages to make such breaking changes feasible.',
  '',
  'Therefore, filtering out certain data clumps considered not worthy of refactoring can be suggested.',
  ''
] [ 3, 1, 0, 8 ]
CURR LINE The next step of the pipeline is to find a suitable identifier of the extracted class.
CURR LINE 
CURR LINE There are some criteria for this identifier. It should be a valid name in the respective programming language so that, in general, only alphanumerical characters may be chosen. It should not conflict with other identifiers as it may lead to bugs or compiler errors, and it should be readable and, as best as feasible, represent the fields of the class.
CURR LINE 
CURR LINE Hence, the name finding is a complex process requiring domain knowledge and creativity if the resulting name is accepted.
CURR LINE Since English is the predominant language of identifiers, it will be assumed that only English identifiers will be used.
CURR LINE 
CURR LINE \subsubsection{Reference finding}
[
  'Name finding',
  'The next step of the pipeline is to find a suitable identifier of the extracted class.',
  '',
  'There are some criteria for this identifier. It should be a valid name in the respective programming language so that, in general, only alphanumerical characters may be chosen. It should not conflict with other identifiers as it may lead to bugs or compiler errors, and it should be readable and, as best as feasible, represent the fields of the class.',
  '',
  'Hence, the name finding is a complex process requiring domain knowledge and creativity if the resulting name is accepted.',
  'Since English is the predominant language of identifiers, it will be assumed that only English identifiers will be used.',
  ''
] [ 3, 1, 0, 9 ]
CURR LINE Having detected a data clump, there is still information missing to refactor it. In particular, successful re-factorization requires that every method call, method definition, variable usage or variable definition that is connected to the data clump is updated in a later step. These so-called \textbf{references} need to be found so that they can be updated.
CURR LINE 
CURR LINE Since the identifier of a the data clump variables (or method) is known, one could search for all occurrences of that identifier. This however is not enough because identifiers have a scope. A variable with the name \textit{var} can be declared as a field, as a parameter of a method or inside a block (e.~g. if-branch). All these names can refer to different variables although they have the same identifier. As a result, a textural replacement approach will not work always.
CURR LINE 
CURR LINE Therefore additional tools will be needed to parse the source code and use the rules of the programming language to find where the data clump items are indeed used.
CURR LINE \subsubsection{Class Extraction}\label{subsec:chap3_data_class_extraction}
[
  'Reference finding',
  'Having detected a data clump, there is still information missing to refactor it. In particular, successful re-factorization requires that every method call, method definition, variable usage or variable definition that is connected to the data clump is updated in a later step. These so-called  references need to be found so that they can be updated.',
  '',
  'Since the identifier of a the data clump variables (or method) is known, one could search for all occurrences of that identifier. This however is not enough because identifiers have a scope. A variable with the name  var can be declared as a field, as a parameter of a method or inside a block (e.~g. if-branch). All these names can refer to different variables although they have the same identifier. As a result, a textural replacement approach will not work always.',
  '',
  'Therefore additional tools will be needed to parse the source code and use the rules of the programming language to find where the data clump items are indeed used.'
] [ 3, 1, 0, 10 ]
CURR LINE After a class name has been found, a respective class can be added to the source code. This class will be used as a replacement for the data clump variables or parameters.
CURR LINE 
CURR LINE In contrast to the previous step of the pipeline, this process can be automated more straightforwardly because it does not require creativity and a general template can be used, that is:
CURR LINE \begin{itemize}
found begin
CURR LINE \item Create a private field for each variable of the data clump
found end
CURR LINE \item Create a public getter for each variable of the data clump. This getter may be named get\textit{Var} where \textit{Var} is replaced by the capitalized name of the variable. The method returns the value of the respective field
found end
CURR LINE \item Similarly, a setter set\textit{Var} may be created that has no return type, accepts a new value as a parameter, and sets the respective field.
found end
CURR LINE \item A constructor that initializes the fields with provided or default values may be created.
found end
CURR LINE \end{itemize}
found end
CURR LINE 
CURR LINE Additionally, methods for converting an object to a string or creating a hash code may be useful, but these methods are usually optional for refactoring data clumps.
CURR LINE 
CURR LINE 
CURR LINE \subsubsection{Refactoring}
[
  'Class Extraction',
  'After a class name has been found, a respective class can be added to the source code. This class will be used as a replacement for the data clump variables or parameters.',
  '',
  'In contrast to the previous step of the pipeline, this process can be automated more straightforwardly because it does not require creativity and a general template can be used, that is:',
  '',
  'Additionally, methods for converting an object to a string or creating a hash code may be useful, but these methods are usually optional for refactoring data clumps.',
  '',
  ''
] [ 3, 1, 0, 11 ]
CURR LINE After extracting a class, the next step is to refactor the source code to remove the data clumps. Here, again, we need to differentiate between method parameter data clumps as this impacts the access scope.
CURR LINE 
CURR LINE In case of a field data clump, the following transformations need to be applied.
CURR LINE 
CURR LINE \begin{enumerate}
found begin
CURR LINE \item For each class in which the same field data clump is detected, the data clump variables are replaced by an object of the extracted class
found end
CURR LINE \item A getter of that object is created that has the same access level as the highest access level of the data clump items
found end
CURR LINE \item If a data clump item is read, it must be replaced by a call of the respective getter of the extracted class
found end
CURR LINE \item Similarly, if a data clump item is written to, the assignment must be replaced by a setter
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE In the case of a method data clump, the general approach is similar to the case of field data clumps. An object of the extracted class replaces the data clump items, and all references to the data clump items are also updated. Here, the object of the extracted class is not a getter because it is only locally defined. Additionally, the calls to the refactored method must be updated. Instead of providing the arguments for the data clump items directly to the method, an intermediate object can be created that is filled with those values, and this object is provided to the method.
CURR LINE 
CURR LINE \subsubsection{Validation}
[
  'Refactoring',
  'After extracting a class, the next step is to refactor the source code to remove the data clumps. Here, again, we need to differentiate between method parameter data clumps as this impacts the access scope.',
  '',
  'In case of a field data clump, the following transformations need to be applied.',
  '',
  '',
  '',
  '',
  'In the case of a method data clump, the general approach is similar to the case of field data clumps. An object of the extracted class replaces the data clump items, and all references to the data clump items are also updated. Here, the object of the extracted class is not a getter because it is only locally defined. Additionally, the calls to the refactored method must be updated. Instead of providing the arguments for the data clump items directly to the method, an intermediate object can be created that is filled with those values, and this object is provided to the method.',
  ''
] [ 3, 1, 0, 12 ]
CURR LINE 
CURR LINE At the conclusion of the refactoring process, it is not certain that everything is perfect. This applies to manual refactoring and also automatic refactoring. For instance, there might be an undetected error, the refactoring leads to ambiguous method signatures, some reference are not up-to-date etc.
CURR LINE 
CURR LINE As a result, it is critical to test the project after refactoring so make sure that all requirements are still fulfilled. Testing includes checking the project for build errors (e.~g. invalid syntax, missing dependencies etc.) and unit test, where the functionality of the project is tested. Since those unit tests are supposed to run quick, they should be included in the validation check.
CURR LINE 
CURR LINE Many build systems like \textit{Gradle} or \textit{Maven} support simple commands to run all unit tests and build the project in a consistent manner.  Therefore, one can run such a command and check the exit code or output to determine whether the project compiles and passes unit tests.
CURR LINE 
CURR LINE Disadvantageously, this may only be effective if the project is correctly configured with a build system. This might not always be the case, and it is not trivial to construct an interface for a general build solution.
CURR LINE 
CURR LINE \subsubsection{Evaluation}
[
  'Validation',
  '',
  'At the conclusion of the refactoring process, it is not certain that everything is perfect. This applies to manual refactoring and also automatic refactoring. For instance, there might be an undetected error, the refactoring leads to ambiguous method signatures, some reference are not up-to-date etc.',
  '',
  'As a result, it is critical to test the project after refactoring so make sure that all requirements are still fulfilled. Testing includes checking the project for build errors (e.~g. invalid syntax, missing dependencies etc.) and unit test, where the functionality of the project is tested. Since those unit tests are supposed to run quick, they should be included in the validation check.',
  '',
  'Many build systems like  Gradle or support simple commands to run all unit tests and build the project in a consistent manner.  Therefore, one can run such a command and check the exit code or output to determine whether the project compiles and passes unit tests.',
  '',
  'Disadvantageously, this may only be effective if the project is correctly configured with a build system. This might not always be the case, and it is not trivial to construct an interface for a general build solution.',
  ''
] [ 3, 1, 0, 13 ]
CURR LINE 
CURR LINE In the last step, one might evalaute the effectiveness and efficiency  of the data clump refactoring. For instance, one could execute the data clump detection process again to determine how many data clumps are fixed in comparison to the first iteration.
CURR LINE 
CURR LINE Another metric for the evaluation could the running time. A huge running time of the refactoring can slow down the development process and cause the program not to be used again for future iterations. It could also be useful to anaylze the running time for each individual step so that handlers that require too much time or resources can be detected and later not be used.
CURR LINE 
CURR LINE \section{Context}
[
  'Evaluation',
  '',
  'In the last step, one might evalaute the effectiveness and efficiency  of the data clump refactoring. For instance, one could execute the data clump detection process again to determine how many data clumps are fixed in comparison to the first iteration.',
  '',
  'Another metric for the evaluation could the running time. A huge running time of the refactoring can slow down the development process and cause the program not to be used again for future iterations. It could also be useful to anaylze the running time for each individual step so that handlers that require too much time or resources can be detected and later not be used.',
  ''
] [ 3, 2, 0, 0 ]
CURR LINE In section \ref{sec:pipeline}, the several steps of the data clump pipeline are outlined. However, these steps do not include how the information generated by each step is stored and how successive steps might use this information.
CURR LINE 
CURR LINE Therefore, a concept to store intermediate information needs to be developed. Such information might be called \textbf{context}.
CURR LINE 
CURR LINE The context is a storage filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.
CURR LINE 
CURR LINE Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created.
CURR LINE 
CURR LINE \begin{figure}
[ '    \\caption{Visualization of context pipeline}' ]
0
CURR LINE \label{fig:context_pipeline}
CURR LINE \end{figure}
CURR LINE 
CURR LINE The context can be represented as a linked list. Each node of the list represents a part of the context. Figure \ref{fig:context_pipeline} illustrates how the context can be  represented.
CURR LINE 
CURR LINE Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a gateway needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. While this can create overhead, the size of the linked list is bounded by the size of the pipeline so that the overhead is limited.  The overhead would only be relevant if the number of steps for refactoring data clumps would be substantially more, which is not to be expected.
CURR LINE 
CURR LINE In figure \ref{fig:context_pipeline}, in the beginning, only the location of the project to analyze is stored, which is mandatory. The pipeline step executing the \textit{DataClumpDoctor} can use this context and append a new context. A name finding service like ChatGPT can then append a context with the names of the extracted classes. At the end, a service like IntelliJ can do the refactoring.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the names of the extracted class of each data clump must be stored in the context.
CURR LINE 
CURR LINE Such context can only work if the context data is in a specific format, which can be generated by one step and might be correctly interpreted by each successive step.
CURR LINE 
CURR LINE In the following, the context created or updated after each step will be explained:
CURR LINE 
CURR LINE \subsubsection{Code obtaining}
[
  'Context',
  'In section ',
  '',
  'Therefore, a concept to store intermediate information needs to be developed. Such information might be called  context.',
  '',
  'The context is a storage filled throughout the data clump refactoring pipeline. For instance, it might be stored in memory or a database if the context size is too large.',
  '',
  'Each step in the data clump refactoring pipeline can access the context and obtain such information as it may deem necessary for its use. A step may or may not update the context. For instance, if a service performs multiple step consecutively, it might not be able to store some information. To give a more concrete example, if a service can find data clumps and refactor them automatically, it might not be able to store the locations of each data clump so that respective context cannot be created.',
  '',
  '',
  'The context can be represented as a linked list. Each node of the list represents a part of the context. Figure ',
  '',
  'Each context is an extension of a previous context. In the beginning, there is no context and the code obtaining context is created. If a gateway needs to access information from a specific context, it needs to traverse the linked list until it finds the needed information. While this can create overhead, the size of the linked list is bounded by the size of the pipeline so that the overhead is limited.  The overhead would only be relevant if the number of steps for refactoring data clumps would be substantially more, which is not to be expected.',
  '',
  'In figure can use this context and append a new context. A name finding service like ChatGPT can then append a context with the names of the extracted classes. At the end, a service like IntelliJ can do the refactoring.',
  '',
  '',
  '',
  'To check whether a given pipeline is indeed executable, each step handler must describe how the context is updated when the the step is executed. Each step can also specify whether a specific type of context must exist. For instance, a step may require that the names of the extracted class of each data clump must be stored in the context.',
  '',
  'Such context can only work if the context data is in a specific format, which can be generated by one step and might be correctly interpreted by each successive step.',
  '',
  'In the following, the context created or updated after each step will be explained:',
  ''
] [ 3, 2, 0, 1 ]
CURR LINE The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.
CURR LINE 
CURR LINE Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files.
CURR LINE 
CURR LINE \subsubsection{Filtering}
[
  'Code obtaining',
  'The context after obtaining the source code of the project to analyze is usually the path to the project after it. In most cases, the project is defined in such a manner that there is a single base directory under which all files and directories of the project are located.',
  '',
  'Alternatively, one could store the list of all relevant file paths of the project. This requires more storage but makes it easier to filter out files.',
  ''
] [ 3, 2, 0, 2 ]
CURR LINE After filtering the files, the context can be a new path with all files considered relevant, requiring a deletion or moving file system operation. If a list of files is used, all irrelevant file paths can be deleted, which might be faster.
CURR LINE 
CURR LINE In most cases, however, the list of \textit{inclusion globs} and \textit{exclusion globs} are useful to be part of the filtering context. Globs are filenames that contain wildcard pattenr (e.~g. \textbf{*}). Each inclusion glob specifies which file must be included, and each exclusion glob specifies files that must be excluded. For instance, consider \textit{includeGlobs=[\enquote{*.java}]} and \textit{excludeGlobs=\enquote{*Main.java}}. In this case, all Java files are included, if however a file ends with \enquote{Main.java} it is excluded.
CURR LINE 
CURR LINE With this approach, the filtering context does not filter the files itself but provides the relevant information to the next contexts.
CURR LINE 
CURR LINE Since the filtering context does not know the next context, it cannot know how filtering can be performed. Some data clump detection tools allow filtering, some cannot do this easily. The disadvantage of this approach needs to be stressed explicitly. The filtering context is a very primitive context. In most cases it only load only data from configuration files which might be counterintuitive.
CURR LINE 
CURR LINE \subsubsection{Extraction of AST}
[
  'Filtering',
  'After filtering the files, the context can be a new path with all files considered relevant, requiring a deletion or moving file system operation. If a list of files is used, all irrelevant file paths can be deleted, which might be faster.',
  '',
  'In most cases, however, the list of  inclusion globs and are useful to be part of the filtering context. Globs are filenames that contain wildcard pattenr (e.~g.  {*}). Each inclusion glob specifies which file must be included, and each exclusion glob specifies files that must be excluded. For instance, consider } and . In this case, all Java files are included, if however a file ends with it is excluded.',
  '',
  'With this approach, the filtering context does not filter the files itself but provides the relevant information to the next contexts.',
  '',
  'Since the filtering context does not know the next context, it cannot know how filtering can be performed. Some data clump detection tools allow filtering, some cannot do this easily. The disadvantage of this approach needs to be stressed explicitly. The filtering context is a very primitive context. In most cases it only load only data from configuration files which might be counterintuitive.',
  ''
] [ 3, 2, 0, 3 ]
CURR LINE \subsubsection{Data clump detection and filtering}
[ 'Extraction of AST' ] [ 3, 2, 0, 4 ]
CURR LINE 
CURR LINE The format described in section \ref{sec:data_clump_format} can be used to store the detected data clump. While this format is relatively new, it contains all relevant information for storing data clump information and is extendable. However, there are other reasons for other data formats. For instance,  the data clump type context format might be too detailed, which can lead to storage or performance issues.
CURR LINE 
CURR LINE \subsubsection{Name finding}
[
  'Data clump detection and filtering',
  '',
  'The format described in section ',
  ''
] [ 3, 2, 0, 5 ]
CURR LINE To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database.
CURR LINE 
CURR LINE Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.
CURR LINE 
CURR LINE \subsubsection{Class extraction}
[
  'Name finding',
  'To store the determined names per found data clump, a dictionary can be used that maps a unique data clump to a name. For large instances, this could be done via a database.',
  '',
  'Since there could be multiple name suggestions for a given data clump, multiple names might be stored so that a correct name may be chosen later. However, this is not part of the modeled pipeline as the benefits do not outweigh the increased complexity.',
  ''
] [ 3, 2, 0, 6 ]
CURR LINE 
CURR LINE After a class extraction step, the created class can be stored somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project.
CURR LINE 
CURR LINE For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located.
CURR LINE 
CURR LINE Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used.
CURR LINE 
CURR LINE As an alternative, the complete class body could be stored instead of saving it directly to a file. This might advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step.
CURR LINE 
CURR LINE One might also argue that choosing the extracted class location should be separate step.
CURR LINE However, too many steps can be counterproductive as they increase the complexity and causes the individual steps to have too few responsibility.
CURR LINE 
CURR LINE Therefore, the class location is determined by the extractor which also saves the class content and stores the data in its context.
CURR LINE 
CURR LINE 
CURR LINE \subsubsection{Refactoring}
[
  'Class extraction',
  '',
  'After a class extraction step, the created class can be stored somewhere to be considered part of the project. The exact location can significantly impact the readability of the source code, as the location of files in a software project can help in understanding the project.',
  '',
  'For instance, the class could be where the data clump is initially found. This can lead to arbitrariness as the exact order of where and when data clumps are detected might not be predetermined. It should also be noted that there are always at least two parts of a data clump (e.g. two methods). As a result, if the two parts are located in separate directories, it is difficult to determine where the extracted class should be located.',
  '',
  'Alternatively, specific locations can be used to store all extracted classes. This, however, can also hinder readability as the extracted classes have no connections to the places where they are used.',
  '',
  'As an alternative, the complete class body could be stored instead of saving it directly to a file. This might advantageous if the class content should be formatted, refactored or otherwise modified in order to be valid. This also would deflect the responsibility of the class location from the class extractor step.',
  '',
  'One might also argue that choosing the extracted class location should be separate step.',
  'However, too many steps can be counterproductive as they increase the complexity and causes the individual steps to have too few responsibility.',
  '',
  'Therefore, the class location is determined by the extractor which also saves the class content and stores the data in its context.',
  '',
  ''
] [ 3, 2, 0, 7 ]
CURR LINE The context for the refactoring step  can be empty. While in previous steps, information has been gathered and analyzed, the refactoring step does not produce new information but applies the obtained information to refactor data clumps.
CURR LINE 
CURR LINE One could argue that the context should contain information about the location of the refactored source code. This is only useful if the refactored source code should not be stored at the same location as the original source code.  However, the code obtaining step (see \ref{sec:code_obtaining}
CURR LINE ) could handle this part by copying the project files to a specific location so this step is not necessary.
CURR LINE 
CURR LINE \subsubsection{Validation}
[
  'Refactoring',
  'The context for the refactoring step  can be empty. While in previous steps, information has been gathered and analyzed, the refactoring step does not produce new information but applies the obtained information to refactor data clumps.',
  '',
  'One could argue that the context should contain information about the location of the refactored source code. This is only useful if the refactored source code should not be stored at the same location as the original source code.  However, the code obtaining step (see ',
  ') could handle this part by copying the project files to a specific location so this step is not necessary.',
  ''
] [ 3, 2, 0, 8 ]
CURR LINE 
CURR LINE The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass).
CURR LINE 
CURR LINE In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases,, modern build tools like \textit{Gradle} already acquire these data so they can be easily obtained.
CURR LINE \begin{comment}
found begin
CURR LINE 
found end
CURR LINE \section{Pipeline execution example}
found end
CURR LINE Listing \ref{lst:math_stuff_java} will be used as the foundation to describe a detailed approach for fixing data clumps.
found end
CURR LINE 
found end
CURR LINE Looking at the parameters \textit{x}, \textit{y}, and \textit{z}, there are multiple suggestions for a class name. The most trivial one is a simple concatenation  (e.g., \textit{XYZ} or \textit{X\_Y\_Z}). However, this name is often not suitable as it can be hard to read and to understand its meaning.
found end
CURR LINE 
found end
CURR LINE A more suitable approach requires domain knowledge. It is common knowledge that the terms \textit{x}, \textit{y}, and \textit{z} are terms used in math to describe coordinates. Using this information, a fitting name could be \textit{Coordinate}. Simple methods would not have discovered this name as artificial or human intelligence is required.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE Since a suitable class name has been found, the following class can be created using previously obtained information.
found end
CURR LINE 
found end
CURR LINE Listing \ref{lst:coordinate_java} shows an example implementation of a coordinate class. This class contains fields, getters, and setters for all method parameters. There is also a constructor for initialization. This respects Fowler's opinion that the extracted class should not be a mere data class but a functional class that can be easily extended and modified.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \end{comment}
found end
CURR LINE \section{Pipeline execution order}
[
  'Validation',
  '',
  'The validation step context requires at least information about whether the validation is successful (i.~e. no compiler errors occur and all unit tests pass).',
  '',
  'In case of a failed validation, one might need more information. For instance, it is helpful to know which unit test fails or on which line the code fails to compile. In many cases,, modern build tools like  Gradle already acquire these data so they can be easily obtained.'
] [ 3, 3, 0, 0 ]
CURR LINE 
CURR LINE Another issue to analyze is the pipeline execution order. This should not be confused with the pipeline step execution order. The latter is simply the order of the different steps (e.~g. code obtaining before data clump detection).
CURR LINE 
CURR LINE The former order deals with how the steps are executed with respect to each detected data clump. Two approaches are considered here:
CURR LINE 
CURR LINE In the first approach, each data clump is   further processed immediately after detection. This means after a data clump has been detected, it is immediately given a suggested name, a class is extracted if necessary, and the refactoring is executed. In the following, this approach will be referred to as \textit{one-data-clump approach}
CURR LINE 
CURR LINE In the second approach, all data clumps are detected first. Then all, those data clumps are assigned a suitable name, and classes are extracted if necessary. At the end, each data clump is refactored. In the following, this approach will be referred to as \textit{one-step approach}
CURR LINE 
CURR LINE As a result, in the \textit{one-data-clump approach}, a data clump is processed by one step of the pipeline and then by the next while in the \textit{one-step approach} the first step has to process all data clumps first before the data clumps can be processed by the succeeding steps.
CURR LINE 
CURR LINE There are arguments for and against both approaches. However designing a system that support both approaches is challenging and out of scope for this master thesis so only one approach is used.
CURR LINE 
CURR LINE The \textit{one-data-clump approach} leads to quicker intermediate results so that if the tool to be developed is stopped during execution, it is more likely that some data clump are fixed so that some progress is being made.
CURR LINE 
CURR LINE Additionally, this approach is better for load balancing. Since many services,  might be used for refactoring data clumps and one data clump is processed at a time, each service gets a longer break before being used again. This is  especially important for services like ChatGPT where many requests in a short time period can lead to outages or huge costs.
CURR LINE 
CURR LINE On the other hand, the \textit{one-step-approach} is easier to implement using the tools already existing. For instance, the \textit{DataClumpDoctor} detects all data clumps at a time and therefore is more compatible with the \textit{one-step-approach}.
CURR LINE 
CURR LINE Moreover, configuration mistakes can be fixed more easily because in this approach, the software project will not be modified for some time (i.~e. data clumps will be detected or suitable names will be found). As a result, users of the tool are more likely to safely stop the tool if they discover a configuration mistakes. This might often occur in the very first moments after starting the tool.
CURR LINE 
CURR LINE Additionally, this approach can use the cache more efficiently. Since one service is being used  over a longer period of time,  relevant data for the service can be cached better in contrast to switching the service frequently  As a result, the performance can be increased.
CURR LINE 
CURR LINE 
CURR LINE \section{Integration of Large Language Services}
[
  'Pipeline execution order',
  '',
  'Another issue to analyze is the pipeline execution order. This should not be confused with the pipeline step execution order. The latter is simply the order of the different steps (e.~g. code obtaining before data clump detection).',
  '',
  'The former order deals with how the steps are executed with respect to each detected data clump. Two approaches are considered here:',
  '',
  'In the first approach, each data clump is   further processed immediately after detection. This means after a data clump has been detected, it is immediately given a suggested name, a class is extracted if necessary, and the refactoring is executed. In the following, this approach will be referred to as  one-data-clump approach',
  '',
  'In the second approach, all data clumps are detected first. Then all, those data clumps are assigned a suitable name, and classes are extracted if necessary. At the end, each data clump is refactored. In the following, this approach will be referred to as  one-step approach',
  '',
  'As a result, in the  one-data-clump approach, a data clump is processed by one step of the pipeline and then by the next while in the the first step has to process all data clumps first before the data clumps can be processed by the succeeding steps.',
  '',
  'There are arguments for and against both approaches. However designing a system that support both approaches is challenging and out of scope for this master thesis so only one approach is used.',
  '',
  'The  one-data-clump approach leads to quicker intermediate results so that if the tool to be developed is stopped during execution, it is more likely that some data clump are fixed so that some progress is being made.',
  '',
  'Additionally, this approach is better for load balancing. Since many services,  might be used for refactoring data clumps and one data clump is processed at a time, each service gets a longer break before being used again. This is  especially important for services like ChatGPT where many requests in a short time period can lead to outages or huge costs.',
  '',
  'On the other hand, the  one-step-approach is easier to implement using the tools already existing. For instance, the detects all data clumps at a time and therefore is more compatible with the ',
  '',
  'Moreover, configuration mistakes can be fixed more easily because in this approach, the software project will not be modified for some time (i.~e. data clumps will be detected or suitable names will be found). As a result, users of the tool are more likely to safely stop the tool if they discover a configuration mistakes. This might often occur in the very first moments after starting the tool.',
  '',
  'Additionally, this approach can use the cache more efficiently. Since one service is being used  over a longer period of time,  relevant data for the service can be cached better in contrast to switching the service frequently  As a result, the performance can be increased.',
  '',
  ''
] [ 3, 4, 0, 0 ]
CURR LINE 
CURR LINE Since \ac{LLM} like ChatGPT are a major part of this master thesis, the issue of integrating \ac{LLM} is another part of the concept. The following problems must be addressed
CURR LINE \begin{enumerate}
found begin
CURR LINE \item How should an interface to a \ac{LLM} look like?
found end
CURR LINE \item How should the conversation with a \ac{LLM} be performed?
found end
CURR LINE \item How are messages from and to the model structured?
found end
CURR LINE \item Where should instructions be stored and processed so that they can be sent to a \ac{LLM}
found end
CURR LINE \end{enumerate}
found end
CURR LINE Issue 1 and 2 will be addressed in section \ref{sec:llm_interface}. The issue 2 3 will be discussed in section \ref{sec:llm_msg_structure}. Lastly, issue 4 will be dealt with in section \ref{llm_msg_storage}.
CURR LINE \subsubsection{An interface for large language models}\label{sec:llm_interface}
[
  'Integration of Large Language Services',
  '',
  'Since is another part of the concept. The following problems must be addressed',
  'Issue 1 and 2 will be addressed in section  Lastly, issue 4 will be dealt with in section '
] [ 3, 4, 0, 1 ]
CURR LINE 
CURR LINE Since the market for large language model is constantly expanding in just a few years, designing a interface for the communication is challenging. As a result, only the core functionality can be modeled by an interface in order to keep compatibility and ease extendability.
CURR LINE 
CURR LINE An interface to a \ac{LLM} should support providing messages. These messages are provided by the user and can be a instruction (see  section \ref{llm_msg_storage}), data or any other relevant information.
CURR LINE 
CURR LINE Providing a message does not mean that the message is processed by the \ac{LLM} but is kept until further instruction. Thus, a user can prepare multiple messages before sending them to the model.
CURR LINE 
CURR LINE If the user decides to send the messages to the mode, another operation can be used. This operations send the accumulated message to the large language model and waits for the response, so that the operation is synchronous. While an asynchronous approach would also be feasible, in most cases the data clump detection and refactoring process cannot proceed without the relevant information from the model so that waiting is tolerable.
CURR LINE 
CURR LINE After sending and receiving the messages, the response from the model can be returned. Now the interface must deal with the messages it has accumulated. Since most models have no memory, the messages must be sent again if they are still relevant for future requests. However, storing and resending messages can cost more so that this should not be done always.
CURR LINE 
CURR LINE Therefore, the caller of the sending operation has the possibility to clear the previous messages after the \ac{LLM} has responded or keep them, and can therefore decide what to do.
CURR LINE 
CURR LINE \subsubsection{A message format for large language models} \label{sec:llm_msg_structure}
[
  'An interface for large language models',
  '',
  'Since the market for large language model is constantly expanding in just a few years, designing a interface for the communication is challenging. As a result, only the core functionality can be modeled by an interface in order to keep compatibility and ease extendability.',
  '',
  'An interface to a , data or any other relevant information.',
  '',
  'Providing a message does not mean that the message is processed by the ',
  '',
  'If the user decides to send the messages to the mode, another operation can be used. This operations send the accumulated message to the large language model and waits for the response, so that the operation is synchronous. While an asynchronous approach would also be feasible, in most cases the data clump detection and refactoring process cannot proceed without the relevant information from the model so that waiting is tolerable.',
  '',
  'After sending and receiving the messages, the response from the model can be returned. Now the interface must deal with the messages it has accumulated. Since most models have no memory, the messages must be sent again if they are still relevant for future requests. However, storing and resending messages can cost more so that this should not be done always.',
  '',
  'Therefore, the caller of the sending operation has the possibility to clear the previous messages after the ',
  ''
] [ 3, 4, 0, 2 ]
CURR LINE The structure of the messages to an \ac{LLM} is another issue to handle. Each model has its own requirements on how a request must be sent to it and how it will respond so that a general message structure must be developed. However there are similarities. Each model differentiate between requests by the user and the responses and represent the messages in a chronological manner, the most recent message is the message with the highest index.
CURR LINE 
CURR LINE As a result, a simple message format can be an array of message object. Each message object is either an input or an output message. Input messages are generated by the user while output messages come from the model. A \ac{LLM} may have specific terms for input and output messages (e.~g. assistant and user), however using these simpler terms helps to generalize the problem.
CURR LINE 
CURR LINE A message object may contain multiple messages as it uses a string array. This is useful if multiple messages have a connection and need to be sent at the same time. For instance, if a user wants to transfer the file contents of a project to a \ac{LLM}, he can transmit each file within a a single message object. This not only helps to improve the performance a little bit but allows for easier management of messages since messages are grouped by request.
CURR LINE 
CURR LINE \subsubsection{Storing instructions}\label{llm_msg_storage}
[
  'A message format for large language models',
  'The structure of the messages to an ',
  '',
  'As a result, a simple message format can be an array of message object. Each message object is either an input or an output message. Input messages are generated by the user while output messages come from the model. A ',
  '',
  'A message object may contain multiple messages as it uses a string array. This is useful if multiple messages have a connection and need to be sent at the same time. For instance, if a user wants to transfer the file contents of a project to a ',
  ''
] [ 3, 4, 0, 3 ]
CURR LINE 
CURR LINE Another issue that arises while using \ac{LLM} is the management of instructions.
CURR LINE 
CURR LINE An instruction is a resource or artifact. Similarly to resources like textures, 3D-models, sound data, images etc., they should be separated from the code \cite{separate_code_data}. As a result, separate text files for the instructions are better as they can be distributed and modified more easily, especially if the instruction prompt creation is done by other persons or entities.
----
An instruction is a resource or artifact. Similarly to resources like textures, 3D-models, sound data, images etc., they should be separated from the code \citeseparate_code_data. As a result, separate text files for the instructions are better as they can be distributed and modified more easily, especially if the instruction prompt creation is done by other persons or entities.
An instruction is a resource or artifact. Similarly to resources like textures, 3D-models, sound data, images etc., they should be separated from the code 
----
CURR LINE 
CURR LINE An instruction is often not a single resource but a composition of many resources or other data. For instance, if an instruction contains an code example, it is reasonable to split the code example into a different file to reduce the size of a single file. This  also allows easier modification of the example with an IDE because the combination of instruction text and source code would lead to compiler errors.
CURR LINE 
CURR LINE As a result, an instruction resource may need to hold references to other files (e.~g. source code) or reference to other data.
CURR LINE For the \ac{LLM}, the instruction should be complete such that it contains the whole instruction with the content of all referenced files and other information.
CURR LINE 
CURR LINE As a result, two perspectives need to be taken into consideration. From a user perspective, an instruction file should be as modular as possible as explained above. From the perspective of a large language model, an instruction needs to be complete.
CURR LINE 
CURR LINE These two perspectives can be reconciled by a template model. The instruction file can be considered as a template. It does not contain the complete instruction that will be sent to a \ac{LLM} but a mixture of actual text and references.
CURR LINE 
CURR LINE When loading the instruction, all references must be correctly mapped with the correct content so that it can be sent to the model and be correctly interpreted.
CURR LINE 
CURR LINE Listing \ref{lst:nstruction_template} illustrates an example instruction file which is also used in section \ref{sec:initial_experiments} The instruction prompts the model that code files will be provided (l.~1) and that all data clumps in those source code files need to be detected (l.~4). It also informs the model that examples of data clump will be provided (l. 6 and 11-12) and describes how the response by the \ac{LLM} should be structured (l~7-9).
CURR LINE 
CURR LINE However, the examples and output format are not directly specified in the instruction file but are referenced. For instance the text \enquote{$\{output\_format\}$} will not be sent to the model but replaced by the actual output format that is stored somewhere else. The same applies to the examples. Also the specific programming language (e.~g. Java) is not directly defined by the instruction but will added when the instruction is sent to a large language model.
CURR LINE 
CURR LINE This allows for more flexibility since a single instruction files can be used for multiple programming languages and scenarios. However, it requires more configuration as outlined in section \ref{sec:config}
CURR LINE \begin{lstlisting}[caption={Instruction file example}, label={lst:nstruction_template}, captionpos=b, numbers=left, ]
found begin
CURR LINE I will provide you one or more ${programming_language}
found end
CURR LINE code files.
found end
CURR LINE Find all data clumps in the respective files.
found end
CURR LINE 
found end
CURR LINE Examples of data clump are provided below.
found end
CURR LINE Use the following JSON format for the output:
found end
CURR LINE ## JSON
found end
CURR LINE ${output_format}
found end
CURR LINE 
found end
CURR LINE ## Examples
found end
CURR LINE ${examples}
found end
CURR LINE \end{lstlisting}
found end
CURR LINE 
CURR LINE 
CURR LINE \hfill
CURR LINE 
CURR LINE \input{Main/umsetzung}
input
masterthesis/Main/umsetzung
CURR LINE 
CURR LINE \begingroup
CURR LINE \renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
CURR LINE \renewcommand{\clearpage}{}
CURR LINE \chapter{Implementation}\label{chapter:implementation}
[ '', '' ] [ 3, 0, 0, 0 ]
CURR LINE \endgroup
CURR LINE 
CURR LINE \section{Configuration}\label{sec:config}
[ 'Implementation', '' ] [ 3, 1, 0, 0 ]
CURR LINE An important aspect for the usability of the tool is the possibility to configure the tool for the user's need.
CURR LINE 
CURR LINE Since the goal of the tool is to allow the combination of multiple services and other tools in order to find and refactor data clumps, the user must be able to define which handler deals with which step (see section \ref{sec:pipeline}).
CURR LINE 
CURR LINE The configuration is provided by a \ac{JSON} file whose location needs to be provided to the tool via a command line argument. It might be argued that providing the configuration directly via the command line is better suited than a separate configuration file because they do not require the creations of files and are easier for users who start the tool just once. Nevertheless, configuration files are persistent and escpecially \ac{JSON} can be more easily structured so that they are easier to understand. As a result, only \ac{JSON} files will be used  for the configuration.
CURR LINE 
CURR LINE 
CURR LINE Listing \ref{lst:config} shows an example configuration file:
CURR LINE \begin{figure} [htbp!]
[ '\t\t\t[caption', '{ Example configuration file},' ]
0
CURR LINE label={lst:config},
CURR LINE captionpos=b, basicstyle=\footnotesize, tabsize=2, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter4/config.json}
CURR LINE \end{figure}
CURR LINE 
CURR LINE 
CURR LINE In the beginning, the programming language is defined (l.~1). Then, the user can specify which handler should handle which step (l.~3-26). In this example, there are two steps.
CURR LINE 
CURR LINE The first step (l.~4-9) establishes the project location and therefore the code obtaining context.
CURR LINE 
CURR LINE The second step (l.~ 10-26), deals with the detection of data clumps. Here a handler is used that allows detection and refactoring of data clumps via ChatGPT. This specific handler requires sub-handlers that perform intermediate task. For instance, the \textit{SimpleInstructionHandler} loads an instruction from a file path. The \textit{PairOfFileContent} handler submits pairs of file contents to ChatGPT (see section \ref{sec:initial_experiments}).
CURR LINE 
CURR LINE It should be noted that the order of the handlers in the configuration  does not matter because the execution order is constant and in most cases, each step requires the context of a previous step so that parallel execution or vice-versa execution of steps is not possible. Only in the case of usage finding and name finding would a parallel execution make sense because no step is dependent on the other.  However, this exception is not implemented.
CURR LINE 
CURR LINE Not all relevant objects are part of the pipeline. Some are outside of the pipeline and can be referenced by all handlers. Here, the large language model \ac{API} is initialized with ChatGPT(l.~ 28).
CURR LINE 
CURR LINE Each object that appears in the \ac{JSON} is instantiated using dependency injection. This means that the objects initially are registered at a central location but not fully instantiated until they are really needed. The general approach is as follows:
CURR LINE \begin{enumerate}
found begin
CURR LINE \item The main program loads the \ac{JSON} and iterates over all objects
found end
CURR LINE \item Each object has a category (e.~g. \textit{LanguageModelInterface} (see l.~ 28)) and a name (e.~g. ChatGPTInterface)
found end
CURR LINE \item The name of the object and it paremters are registered at the central dependency manager under the category given
found end
CURR LINE \item If a handler or other object needs another object of a specific category, it can ask the central manager. If the respective object has  yet not been created, it will be created. Otherwise, the already instantiated object will be returned.
found end
CURR LINE \end{enumerate}
found end
CURR LINE With this approach, a looser coupling can be achieved. The main program and the handlers does not need to know the exact details of the configuration but provide and receive them from a central dependency manager. Only the handler-specific configuration (e.~g. loading the project location  for the \textit{CodeObtaining} handler is still required and must be performed by each handler. These parameters are stored by the dependency manager.
CURR LINE 
CURR LINE 
CURR LINE \section{Prompt engineering ChatGPT for data clump detection}\label{sec:prompt_engineering}
[
  'Configuration',
  "An important aspect for the usability of the tool is the possibility to configure the tool for the user's need.",
  '',
  'Since the goal of the tool is to allow the combination of multiple services and other tools in order to find and refactor data clumps, the user must be able to define which handler deals with which step (see section ',
  '',
  'The configuration is provided by a can be more easily structured so that they are easier to understand. As a result, only files will be used  for the configuration.',
  '',
  '',
  'Listing ',
  'undefined Example configuration fileundefined,',
  'label=lst:config,',
  'captionpos=b, basicstyle=',
  'figures/chapter4/config.json',
  '',
  '',
  'In the beginning, the programming language is defined (l.~1). Then, the user can specify which handler should handle which step (l.~3-26). In this example, there are two steps.',
  '',
  'The first step (l.~4-9) establishes the project location and therefore the code obtaining context.',
  '',
  'The second step (l.~ 10-26), deals with the detection of data clumps. Here a handler is used that allows detection and refactoring of data clumps via ChatGPT. This specific handler requires sub-handlers that perform intermediate task. For instance, the  SimpleInstructionHandler loads an instruction from a file path. The handler submits pairs of file contents to ChatGPT (see section .',
  '',
  'It should be noted that the order of the handlers in the configuration  does not matter because the execution order is constant and in most cases, each step requires the context of a previous step so that parallel execution or vice-versa execution of steps is not possible. Only in the case of usage finding and name finding would a parallel execution make sense because no step is dependent on the other.  However, this exception is not implemented.',
  '',
  'Not all relevant objects are part of the pipeline. Some are outside of the pipeline and can be referenced by all handlers. Here, the large language model ',
  '',
  'Each object that appears in the ',
  'With this approach, a looser coupling can be achieved. The main program and the handlers does not need to know the exact details of the configuration but provide and receive them from a central dependency manager. Only the handler-specific configuration (e.~g. loading the project location  for the  CodeObtaining handler is still required and must be performed by each handler. These parameters are stored by the dependency manager.',
  '',
  ''
] [ 3, 2, 0, 0 ]
CURR LINE 
CURR LINE As outlined in  section \ref{sec:llm_challenges}, using \ac{LLM} can result in some challenges for refactoring code. Therefore, experiments are needed to test which input can lead to good results for finding or refactoring data clumps or both. These experiments should not be seen as a replacement for the full evaluation discussed in chapter \ref{chapter:eval}, but a mandatory prerequisite to save costs since the full evaluations needs to be performed on a larger set of projects.
CURR LINE 
CURR LINE The input project is a modified version of the source code discussed in \ref{sec:data_clump_def} which is fairly small but include at least three data clumps. Table \ref{tbl:javaTest_data_clumps} lists all data clumps which exists in this project. The names in the \textit{From} or \textit{To} column are in the format \textit{className.methodName} or \textit{className} depending on whether it is a field-to-field \textit{(f2f} or parameter-to-parameter \textit{(p2p)} data clump
CURR LINE 
CURR LINE \begin{table}[]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{c|c|c|c|c}
found end
CURR LINE Id &From & To & Type & Data clump variables  \\\hline
found end
CURR LINE 1 & MathStuff.printLength & MathStuff.printSum & p2p & x,y,z\\\hline
found end
CURR LINE 2 & MathStuff.printSum & MathSum.printMax & p2p & x,y,z\\\hline
found end
CURR LINE 3 & MathStuff.printLength & MathSum.printMax & p2p & x,y,z\\\hline
found end
CURR LINE 4 & MathStuff & MathUser & f2f & sign, mantissa, exponent\\\hline
found end
CURR LINE 5 & Library & MathUser & f2f & sign, mantissa, exponent\\\hline
found end
CURR LINE 6 & MathStuff & Library & f2f & sign, mantissa, exponent\\\hline
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{All data clumps in the test projects}
CURR LINE \label{tbl:javaTest_data_clumps}
CURR LINE \end{table}
CURR LINE 
CURR LINE In this section, the results of some of these experiments will be discussed.  As repeatedly noted, these experiments are not always reproducible and therefore are only to some extent helpful. In all cases, ChatGPT is asked to output in \ac{JSON} to have a consistent format.
CURR LINE 
CURR LINE In each experiment, different parameters of the input are adjusted. Each these parameters has an influence of the quality of the output:
CURR LINE 
CURR LINE \begin{itemize}
found begin
CURR LINE \item Instruction
found end
CURR LINE \item Data format
found end
CURR LINE \item Data size
found end
CURR LINE \item Temperature
found end
CURR LINE \item Follow-ups
found end
CURR LINE \end{itemize}
found end
CURR LINE 
CURR LINE \subsection{Instruction}\label{sec:llm_instruction}
[
  'Prompt engineering ChatGPT for data clump detection',
  '',
  'As outlined in  section can result in some challenges for refactoring code. Therefore, experiments are needed to test which input can lead to good results for finding or refactoring data clumps or both. These experiments should not be seen as a replacement for the full evaluation discussed in chapter  but a mandatory prerequisite to save costs since the full evaluations needs to be performed on a larger set of projects.',
  '',
  'The input project is a modified version of the source code discussed in lists all data clumps which exists in this project. The names in the  {From} or column are in the format or depending on whether it is a field-to-field or parameter-to-parameter data clump',
  '',
  '',
  'In this section, the results of some of these experiments will be discussed.  As repeatedly noted, these experiments are not always reproducible and therefore are only to some extent helpful. In all cases, ChatGPT is asked to output in ',
  '',
  'In each experiment, different parameters of the input are adjusted. Each these parameters has an influence of the quality of the output:',
  '',
  ''
] [ 3, 2, 1, 0 ]
CURR LINE The instruction is a main part of a prompt to ChatGPT or other \ac{LLM}. I tells the  \ac{LLM} what to do and how to respond. As a result, varying the instruction is an important part of prompt engineering.
CURR LINE 
CURR LINE \subsubsection{No definition approach}
[
  'Instruction',
  'The instruction is a main part of a prompt to ChatGPT or other what to do and how to respond. As a result, varying the instruction is an important part of prompt engineering.',
  ''
] [ 3, 2, 1, 1 ]
CURR LINE At first, it is useful whether ChatGPT can find data clumps without giving a specific definition. This means ChatGPT needs to deduce the definition of a data clump from its trained models which might not be the same as the definition used in section \ref{sec:data_clump_def}.
CURR LINE \begin{comment}
found begin
CURR LINE Nevertheless, the results are fairly good. ChatGPT ignores the inheritance between \textit{MathStuff} and \textit{BetterMathStuff} and does not output a data clump for each method in \textit{MathStuff}
found end
CURR LINE \end{comment}
found end
CURR LINE \subsubsection{Definition-based approach}
[
  'No definition approach',
  'At first, it is useful whether ChatGPT can find data clumps without giving a specific definition. This means ChatGPT needs to deduce the definition of a data clump from its trained models which might not be the same as the definition used in section '
] [ 3, 2, 1, 2 ]
CURR LINE In this approach, ChatGPT is provided a specific definition of data clumps and asked to apply this definition on the project. Therefore, a precise definition is elementary for this approach.
CURR LINE 
CURR LINE The following query was used:
CURR LINE A data clump exists if
CURR LINE \begin{enumerate}
found begin
CURR LINE \item  Two methods (in the same or in different classes) have at least 3 common parameters
found end
CURR LINE and one of those methods does not override the other,
found end
CURR LINE \item At least three fields in a class are common with the parameters of a method (in the same or in a different class), or
found end
CURR LINE \item Two different classes have at least three common fields
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE \subsubsection{Providing examples of data clumps to ChatGPT}
[
  'Definition-based approach',
  'In this approach, ChatGPT is provided a specific definition of data clumps and asked to apply this definition on the project. Therefore, a precise definition is elementary for this approach.',
  '',
  'The following query was used:',
  'A data clump exists if',
  ''
] [ 3, 2, 1, 3 ]
CURR LINE 
CURR LINE Another approach for including ChatGPT for data clump refactoring is example-based. Multiple examples of data clumps and how they can be refactored are presented to ChatGPT. In order to avoid any bias, each type of data clump is equally represented and field identifiers or parameters field identifiers are unique across the whole example. This results in a larger request size and thusly more costs. Nevertheless, if larger projects were to be used the size of the examples becomes negligible so that this approach has its reasons. The examples provided to ChatGPT can be found in the digital appendix under the path \enquote{data\_clump\_examples.java}
CURR LINE 
CURR LINE 
CURR LINE \subsection{Data format}
[
  'Providing examples of data clumps to ChatGPT',
  '',
  'Another approach for including ChatGPT for data clump refactoring is example-based. Multiple examples of data clumps and how they can be refactored are presented to ChatGPT. In order to avoid any bias, each type of data clump is equally represented and field identifiers or parameters field identifiers are unique across the whole example. This results in a larger request size and thusly more costs. Nevertheless, if larger projects were to be used the size of the examples becomes negligible so that this approach has its reasons. The examples provided to ChatGPT can be found in the digital appendix under the path  data',
  '',
  ''
] [ 3, 2, 2, 0 ]
CURR LINE 
CURR LINE After creating a instruction, ChatGPT cannot do much because it does not have access to the source code which must be provided to ChatGPT. Therefore, the next part of the prompt is some representation of the source code
CURR LINE 
CURR LINE The two approaches tested are providing the complete source code and providing the \ac{AST}.
CURR LINE 
CURR LINE Providing the complete source code is the most natural manner to give ChatGPT the relevant information. Since ChatGPT is language-agnostic, it can deal with many programming languages and therefore is able to process the source code files to find data clumps. However, source code files can be large as they can contain comments, method bodies or other parts of the programming language that are not relevant for detecting data clumps. This leads to higher costs.
CURR LINE 
CURR LINE On the other hand, the \ac{AST} can be provided. The \ac{AST} only contains a reduced structure of the source code and will therefore cost not as much. However, the \ac{AST} is usually not available but must be produced by other tools or ChatGPT (which means the source code has to be sent to the \ac{LLM} nevertheless). Additionally, there is no fixed formate for the \ac{AST} which might complicate parsing and processing the \ac{AST} or the results by ChatGPT.
CURR LINE 
CURR LINE \subsection{Data size}
[
  'Data format',
  '',
  'After creating a instruction, ChatGPT cannot do much because it does not have access to the source code which must be provided to ChatGPT. Therefore, the next part of the prompt is some representation of the source code',
  '',
  'The two approaches tested are providing the complete source code and providing the ',
  '',
  'Providing the complete source code is the most natural manner to give ChatGPT the relevant information. Since ChatGPT is language-agnostic, it can deal with many programming languages and therefore is able to process the source code files to find data clumps. However, source code files can be large as they can contain comments, method bodies or other parts of the programming language that are not relevant for detecting data clumps. This leads to higher costs.',
  '',
  'On the other hand, the only contains a reduced structure of the source code and will therefore cost not as much. However, the is usually not available but must be produced by other tools or ChatGPT (which means the source code has to be sent to the nevertheless). Additionally, there is no fixed formate for the which might complicate parsing and processing the or the results by ChatGPT.',
  ''
] [ 3, 2, 3, 0 ]
CURR LINE 
CURR LINE The size of the data is the amount of data that is submitted during a single request. Here also two approaches are tested.
CURR LINE 
CURR LINE In the first approach, the full project is submitted to ChatGPT to find data clump while in the second pairs of files are transmitted which each pair of file being a separate independent request.
CURR LINE 
CURR LINE The advantage of the first approach is that only one transaction needs to be done, and ChatGPT can find many data clumps in one shot. ChatGPT can therefore better understand the structure and provide better suggested name or find data clumps where the identifier names of the data clump variables are not identical. However,  a major issue is that many projects are too large, and therefore ChatGPT cannot provide the advantages just described.
CURR LINE 
CURR LINE In the second approach, pair of files are transmitted to ChatGPT with an instruction mentioned in \ref{sec:llm_instruction}
CURR LINE This means that every request only contains the instruction and the content of the two files. Therefore, ChatGPT can only find data clumps only in those two files and in no others as it has no memory of previous prompts. One might think that this approach is cheaper, however, if one still wants to find data clump in the whole software project, he has to to transmit $\binom{n}{2}=0.5*n*(n-1)$ where $n$ is the number of relevant files. Therefore, the cost can be much higher than transmitting each file only once as in the first approach.
CURR LINE 
CURR LINE Additionally, each file could be sent again separated in order to find data clumps that exists in a single file and might not be detected if two files are provided so that the total number of files would be $0.5*n*(n-1)+n$.
CURR LINE 
CURR LINE 
CURR LINE \subsection{Requesting follow-ups}
[
  'Data size',
  '',
  'The size of the data is the amount of data that is submitted during a single request. Here also two approaches are tested.',
  '',
  'In the first approach, the full project is submitted to ChatGPT to find data clump while in the second pairs of files are transmitted which each pair of file being a separate independent request.',
  '',
  'The advantage of the first approach is that only one transaction needs to be done, and ChatGPT can find many data clumps in one shot. ChatGPT can therefore better understand the structure and provide better suggested name or find data clumps where the identifier names of the data clump variables are not identical. However,  a major issue is that many projects are too large, and therefore ChatGPT cannot provide the advantages just described.',
  '',
  'In the second approach, pair of files are transmitted to ChatGPT with an instruction mentioned in ',
  'This means that every request only contains the instruction and the content of the two files. Therefore, ChatGPT can only find data clumps only in those two files and in no others as it has no memory of previous prompts. One might think that this approach is cheaper, however, if one still wants to find data clump in the whole software project, he has to to transmit $0.5*n*(n-1)$ where $n$ is the number of relevant files. Therefore, the cost can be much higher than transmitting each file only once as in the first approach.',
  '',
  'Additionally, each file could be sent again separated in order to find data clumps that exists in a single file and might not be detected if two files are provided so that the total number of files would be $0.5*n*(n-1)+n$.',
  '',
  ''
] [ 3, 2, 4, 0 ]
CURR LINE 
CURR LINE As outlined in section \ref{sec:chain of thought},  one prompt might no be enough to refactor data clumps. This might also apply for the simpler task of detecting data clumps.
CURR LINE 
CURR LINE Instead of expecting that a single request will detect all data clumps, another approach would be to repeat the same requests multiple times and aggregate the results. These follow-up prompts result in ChatGPT to re-analyze the code so that more data clumps can be found.
CURR LINE 
CURR LINE The issue  with follow-up prompts is how often should a prompt be repeated. Each prompt repeat requires that the conversation history to be sent again because ChatGPT is stateless. This leads to higher resource usage and costs. There are multiple approach to implement follow up.
CURR LINE \begin{description}
found begin
CURR LINE \item[Full project follow-up] The whole project (all relevant files) is sent to  ChatGPT again with an instruction to find more data clumps. As a result, more data is sent per  request while the number of request can be reduced. As a  result, ChatGPt might find additional data clumps that it has not found during the previous prompts.
found end
CURR LINE \item [Tuple-based follow-up] A tuple of two files in the project (i.~e. the content of those files) is sent again to ChatGPT with the instruction to find more data clumps. If the goal is to data clumps between tuples of all files, $0.5*n*(n-1)$ file tuples need to uploaded so that many requests will needed. However, this can help ChatGPT to focus on a smaller part of the software project to analyze in order to find more data clumps.
found end
CURR LINE 
found end
CURR LINE \subsection{Results of the experiments}\label{sec:initial_experiments}
found end
CURR LINE 
found end
CURR LINE \end{description}
found end
CURR LINE \section{Implementation of steps}
[
  'Requesting follow-ups',
  '',
  'As outlined in section ',
  '',
  'Instead of expecting that a single request will detect all data clumps, another approach would be to repeat the same requests multiple times and aggregate the results. These follow-up prompts result in ChatGPT to re-analyze the code so that more data clumps can be found.',
  '',
  'The issue  with follow-up prompts is how often should a prompt be repeated. Each prompt repeat requires that the conversation history to be sent again because ChatGPT is stateless. This leads to higher resource usage and costs. There are multiple approach to implement follow up.'
] [ 3, 3, 0, 0 ]
CURR LINE While in section \ref{sec:pipeline} a general approach to find and refactor data clumps is given, the concrete implementation with the various tools is left out. In the following subsections, the implementations with the tools are described. In the case of \textit{code obtaining}, \textit{filtering}, only basic approaches are used so that they will not be detailed further.
CURR LINE 
CURR LINE 
CURR LINE \subsection{Data clump doctor}
[
  'Implementation of steps',
  'While in section   only basic approaches are used so that they will not be detailed further.',
  '',
  ''
] [ 3, 3, 1, 0 ]
CURR LINE 
CURR LINE The  \textit{Data Clump Doctor} is NodeJS command line tool developed by Nils Baumgartner in preparation of this master thesis. It employs \textit{PMD} to find all classes, methods, and fields in a Java project to generate an \ac{AST}. This syntax tree is saved as a \ac{JSON}.
CURR LINE 
CURR LINE In a second step, the generated \ac{AST} can be loaded again to find data clumps. This multi-step approach results in better performance since the detection of data clump requires many nested for-loops so that any reduction of data size has a measurable effect on the performance.
CURR LINE 
CURR LINE All detected data clumps are reported in the format specified in section \ref{sec:data_clump_format}.
CURR LINE 
CURR LINE Since the general tool is developed in Typescript, the \textit{Data Clump Doctor} can easily be integrated.
CURR LINE 
CURR LINE \begin{comment}
found begin
CURR LINE \subsection{ChatGPT}
found end
CURR LINE ChatGPT is another approach to detect data clumps as it can process code easily and report data clumps in any format the user wants. It also supports many programming languages that other tools do not provide.
found end
CURR LINE 
found end
CURR LINE However, ChatGPT has a limited context size, so that processing large projects is  either too costly or simply not possible.
found end
CURR LINE 
found end
CURR LINE Additionally, giving ChatGPT the right instructions to find data clumps can be challenging. While ChatGPT can define and find some data clumps without further context, it is better to give it a precise definition to work with. The following definition leads to good result, however it cannot be guaranteed that this will work forever:
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE A data clump exists
found end
CURR LINE \begin{enumerate}
found end
CURR LINE 
found end
CURR LINE \item if at least three fields also exists in another class
found end
CURR LINE \item if at least three fields also exists as method parameters in some method
found end
CURR LINE \item if two methods have at least three common parameters
found end
CURR LINE \end{enumerate}
found end
CURR LINE \end{comment}
CURR LINE 
CURR LINE \subsection{IntelliJ}
[
  'Data clump doctor',
  '',
  'The   Data Clump Doctor is NodeJS command line tool developed by Nils Baumgartner in preparation of this master thesis. It employs to find all classes, methods, and fields in a Java project to generate an  This syntax tree is saved as a ',
  '',
  'In a second step, the generated ',
  '',
  'All detected data clumps are reported in the format specified in section ',
  '',
  'Since the general tool is developed in Typescript, the  Data Clump Doctor can easily be integrated.',
  '',
  ''
] [ 3, 3, 2, 0 ]
CURR LINE The Program Structure Interface provided by IntelliJ is an \ac{API} to analyze projects that can be loaded by IntelliJ. As a result, the various classes, methods etc. can be obtained which allows for the detection of data clumps. Like the ChatGPT approach, it can be easily extended to refactor the data clumps.
CURR LINE 
CURR LINE In order to use this API, an instance of IntelliJ must be started. To reduce loading times and improve the performance, IntelliJ can be started in a headless mode so that no GUI is initialized. Nevertheless, IntelliJ requires many resources and much overhead so that  the initialization  needs some time.
CURR LINE 
CURR LINE Another problem during the development of the tool is the issue of correctly loading projects. While a project that has no IntelliJ meta data files can be loaded, finding references of a method or field can lead to undefined behavior. Sometimes all references are correctly found, sometimes only a subset of the references are found, and sometimes no even no references are detected. The reasons for this are difficult to determine and the documentation is scarce, so the the PSI approach seems to be only suitable for projects created via IntelliJ or correctly initialized by IntelliJ with the required meta data. Gradle and maven projects are therefore not suitable for the full refactoring step.
CURR LINE 
CURR LINE \subsubsection{Data clump Refactoring}
[
  'IntelliJ',
  'The Program Structure Interface provided by IntelliJ is an ',
  '',
  'In order to use this API, an instance of IntelliJ must be started. To reduce loading times and improve the performance, IntelliJ can be started in a headless mode so that no GUI is initialized. Nevertheless, IntelliJ requires many resources and much overhead so that  the initialization  needs some time.',
  '',
  'Another problem during the development of the tool is the issue of correctly loading projects. While a project that has no IntelliJ meta data files can be loaded, finding references of a method or field can lead to undefined behavior. Sometimes all references are correctly found, sometimes only a subset of the references are found, and sometimes no even no references are detected. The reasons for this are difficult to determine and the documentation is scarce, so the the PSI approach seems to be only suitable for projects created via IntelliJ or correctly initialized by IntelliJ with the required meta data. Gradle and maven projects are therefore not suitable for the full refactoring step.',
  ''
] [ 3, 3, 2, 1 ]
CURR LINE 
CURR LINE IntelliJ proves to be a effective way to refactor data clump because of it easy access to the syntax tree.
CURR LINE 
CURR LINE There are two ways of using IntelliJ for the refactoring of data clumps.
CURR LINE 
CURR LINE In the first way, IntelliJ performs the refactoring with libraries that are already included. Noteworthy are the \textit{IntroduceParameterProcessor} and teh \textit{ExtractFieldsProcessor}. Both classes can perform all necessary steps to refactor data clumps. The former is used to refactor parameters in a method, while the latter is used for data clumps with fields. They can be combined in case of parameters-to-field data clumps. The problem is that -- as already noted-- IntelliJ only works correctly if the software project is an IntelliJ project which is not always teh case.
CURR LINE 
CURR LINE In case of a non-IntelliJ project like Gradle or Maven, a different approach is needed. Here, the \ac{LSP} as described in section \ref{sec:lsp} is used to find all references of the methods, fields, and variables that are part of the data clump. For instance, the \ac{LSP} find all references of a method parameter that is part of a data clump. These references are grouped into four categories.
CURR LINE \begin{enumerate}
found begin
CURR LINE \item A field is declared
found end
CURR LINE \item A variable (field or method parameter) is read from or assigned to
found end
CURR LINE 
found end
CURR LINE \item A method is declared or overridden
found end
CURR LINE \item A method is called
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE This information can be obtained by \ac{LSP} and is provided to IntellJ which does not have the capability to obtain the data in case of non-supported project.
CURR LINE 
CURR LINE IntelliJ can then perform the refactoring with the information it can obtain even from non-supported projects and the information from \ac{LSP}. For instance, the data from \ac{LSP} informas IntelliJ where a particular method call is located, and IntelliJ can parse the specific source code part so that refactoring is easier.
CURR LINE 
CURR LINE Now, IntelliJ can perform the refactoring. Deepending of the category of a reference determined in the previous step, IntelliJ needs to perform distinct step.
CURR LINE \begin{enumerate}
found begin
CURR LINE \item In case of a declared field, the field can be deleted because it is part of a field data clump. Now it can be determined whether the class contains already the new field replaces the fields of the data clump.
found end
CURR LINE \item If a method is declared, IntelliJ can modify the signature of the method. This can be the original method or an overridden one. IntelliJ needs to remove the parameters that are part of the data clump and add a new method parameter which replaces the method parameters of the data clump.
found end
CURR LINE \item If a variable is used it can be replaced by a getter or setter call. For instance if the variable \textit{var} is read, and the name of the extracted class is \textit{Object}, any reading of the variable can be replaced by  \textit{object.getVar()} where \textit{object} is a variable of type \textit{Object} and \textit{getVar} is the getter of \textit{var}. Similarly, an assignment can be replaces by the setter.
found end
CURR LINE \item If a method is used, several substeps are needed.
found end
CURR LINE \begin{enumerate}
found end
CURR LINE \item First, for each argument provided in a method call it is determined whether the argument is connected to a data clump variable (i.~e. it provides a value to a parameter that is part of a data clump)
found end
CURR LINE \item The position of those argument is stored and a reference to the argument is stored for further processing.
found end
CURR LINE \item Since the extracted class is known and already existing, a matching constructor  is determined that supports all arguments to the data clump variables of the method call.
found end
CURR LINE \item For each argument to a data clump variable, the argument is inserted into the matching position of the constructor, and the argument  is deleted in the original method call.
found end
CURR LINE \item the constructor call is added at the position of the method  where the parameter of the extracted class is expected.
found end
CURR LINE 
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE \end{enumerate}
CURR LINE 
CURR LINE This approach requires some coordination. For instance, the order of operation is important. Method and field declarations must be updated first because they are needed to successfully perform the refactoring of variable usages and method calls.
CURR LINE 
CURR LINE Another aspect where the order of the operation matters is the hierarchy in abstract syntax tree. Consider the assignemt \textit{x=x+1}. In an abstract syntax tree, the reading of the value \textit{x} and the addition of 1 is executed first. It is also at a deeper level of the tree than the assignment. If the assignment were to be replaced by a setter, the syntax tree of the reading expression can be out of sync because it is not linked to the original assignment. Therefore, it is important to refactor parts of a code that have higher depth in the syntax tree before parts of the code with lower depths.
CURR LINE %\input{Main/main}
CURR LINE 
CURR LINE 
CURR LINE \begingroup
CURR LINE \renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
CURR LINE \renewcommand{\clearpage}{}
CURR LINE \chapter{Evaluation}\label{chapter:eval}
[
  'Background',
  '',
  '%Multiple input files for larger chapters are also possible',
  '',
  '',
  '',
  '%',
  '',
  ''
] [ 3, 0, 0, 0 ]
CURR LINE \endgroup
CURR LINE \input{Evaluation/eval}
input
masterthesis/Evaluation/eval
CURR LINE 
CURR LINE \begin{comment}
found begin
CURR LINE \newcommand{\checkpmd}{\textit{Checkstyle} und \textit{PMD} }
found end
CURR LINE \newcommand{\doceval}{\textit{DocEvaluator} }
found end
CURR LINE In diesem Kapitel soll das Programm evaluiert werden, um zu prüfen, ob der \doceval eine gute heuristische Aussage über den Stand der Dokumentationsqualität treffen kann und in der Praxis auch einsetzbar ist. Dazu wird der \textit{DocEvaluator} mit \textit{Checkstyle} und \textit{PMD} verglichen, indem drei exemplarische Open-Source-Projekte mit allen drei Programmen analysiert werden und die Treffergenauigkeit und Geschwindigkeit der Programme verglichen werden. Zunächst müssen diese drei Projekte ausgewählt werden, was in Kapitel \ref{chapter:choosing_project} erläutert wird. Anschließend wird in Kapitel \ref{chapter:quality} beschrieben, wie die Evaluation der Treffergenauigkeit durchgeführt wird. In Kapitel \ref{chapter:speed} wird die Geschwindigkeitsevaluation durchgeführt. Zum Abschluss der Evaluation werden die Ergebnisse der Evaluation in Kapitel \ref{chapter:eval_conclusion} resümiert.
found end
CURR LINE 
found end
CURR LINE \clearpage
found end
CURR LINE 
found end
CURR LINE \section{Wahl der zu analysierenden Projekte}\label{chapter:choosing_project}
found end
CURR LINE Zur Durchführung der Evaluation wurden verschiedene Softwareprojekte aus GitHub heruntergeladen. Grundsätzlich kann der Vergleich mit jedem Java-Projekt durchgeführt werden, dennoch wurden einige Bedingungen festgelegt, die bei der Auswahl der Projekte eine wichtige Rolle spielen. Diese Bedingungen werden in der folgenden Auflistung präsentiert:
found end
CURR LINE 
found end
CURR LINE \begin{enumerate}
found end
CURR LINE \item \label{enum:size} Die Projekte müssen mindestens einen Umfang von 10~000 \ac{LOC} haben
found end
CURR LINE \item \label{enum:already_cited} Die Projekte müssen bereits in einer in dieser Bachelorarbeit zitierten Quelle in puncto Dokumentationsqualität analysiert worden sein
found end
CURR LINE \item \label{enum:parsing_error}  Die Projekte sollen möglichst wenige Parsing-Fehler beim \doceval produzieren. Bei mehr als zwei Fehlern wird ein Projekt nicht betrachtet. Können jedoch Fehler folgenlos behoben werden, so kann das Projekt dennoch betrachtet werden
found end
CURR LINE \item Das größte Projekt sollte mindestens zehnmal so groß sein wie das kleinste Projekt
found end
CURR LINE \end{enumerate}
found end
CURR LINE 
CURR LINE Die \ac{LOC} sollen nur als sehr grobe Heuristik der Größe eines Softwareprojektes verstanden werden und enthalten nur den reinen Code ohne Kommentare. Dabei wird heuristisch vermutet, dass mit steigender \ac{LOC} die Anzahl der Komponenten in einem Softwareprojekt steigt, wobei all diese Komponenten fehlerhaft (oder gar nicht) dokumentiert sein können. Somit dienen die \ac{LOC} als approximative Schätzung der zu erwartenden Fehler.
CURR LINE 
CURR LINE Durch die Bedingung in Nr. \ref{enum:size} wird sichergestellt, dass eine ausreichende Anzahl an Fehlern in der Dokumentation zu erwarten ist, um eine gute Analyse der Dokumentationsqualität  und eine aussagekräftige Bewertung der Geschwindigkeit zu ermöglichen. Durch die Bedingung in Nr.  \ref{enum:already_cited} werden nur Projekte in Betracht gezogen, die bereits in der wissenschaftlichen Literatur berücksichtigt wurden und daher (zumindest für den jeweiligen Autor der Quelle) geeignet für eine Analyse der Dokumentation sind. Mit der Bedingung in Nr. \ref{enum:parsing_error} wird eine Verzerrung zugunsten oder zuungunsten des \textit{DocEvaluators} vermieden, denn durch Parsing-Fehler können Komponenten falsch analysiert werden, die von den anderen Tools richtig analysiert werden. Indirekt wird dadurch gefordert, dass ein Projekt keine neueren Java-Funktionen verwendet, da  der \textit{DocEvaluator} nur Java bis Version 8 unterstützt. Die letzte Bedingung ist besonders für die Bewertung der Geschwindigkeit relevant, da so geprüft werden kann, ob der \doceval auch bei größeren Projekten noch in annehmbarer Zeit ein Ergebnis berechnen kann.
----
Durch die Bedingung in Nr. \refenum:size wird sichergestellt, dass eine ausreichende Anzahl an Fehlern in der Dokumentation zu erwarten ist, um eine gute Analyse der Dokumentationsqualität  und eine aussagekräftige Bewertung der Geschwindigkeit zu ermöglichen. Durch die Bedingung in Nr.  \ref{enum:already_cited} werden nur Projekte in Betracht gezogen, die bereits in der wissenschaftlichen Literatur berücksichtigt wurden und daher (zumindest für den jeweiligen Autor der Quelle) geeignet für eine Analyse der Dokumentation sind. Mit der Bedingung in Nr. \ref{enum:parsing_error} wird eine Verzerrung zugunsten oder zuungunsten des  {DocEvaluators} vermieden, denn durch Parsing-Fehler können Komponenten falsch analysiert werden, die von den anderen Tools richtig analysiert werden. Indirekt wird dadurch gefordert, dass ein Projekt keine neueren Java-Funktionen verwendet, da  der \textit{DocEvaluator} nur Java bis Version 8 unterstützt. Die letzte Bedingung ist besonders für die Bewertung der Geschwindigkeit relevant, da so geprüft werden kann, ob der \doceval auch bei größeren Projekten noch in annehmbarer Zeit ein Ergebnis berechnen kann.
Durch die Bedingung in Nr. werden nur Projekte in Betracht gezogen, die bereits in der wissenschaftlichen Literatur berücksichtigt wurden und daher (zumindest für den jeweiligen Autor der Quelle) geeignet für eine Analyse der Dokumentation sind. Mit der Bedingung in Nr. wird eine Verzerrung zugunsten oder zuungunsten des  {DocEvaluators} vermieden, denn durch Parsing-Fehler können Komponenten falsch analysiert werden, die von den anderen Tools richtig analysiert werden. Indirekt wird dadurch gefordert, dass ein Projekt keine neueren Java-Funktionen verwendet, da  der nur Java bis Version 8 unterstützt. Die letzte Bedingung ist besonders für die Bewertung der Geschwindigkeit relevant, da so geprüft werden kann, ob der 
----
CURR LINE 
CURR LINE \subsubsection{Analysierte Projekte}\label{chapter:eval_projects}
[
  '',
  '',
  '',
  'Die die Anzahl der Komponenten in einem Softwareprojekt steigt, wobei all diese Komponenten fehlerhaft (oder gar nicht) dokumentiert sein können. Somit dienen die als approximative Schätzung der zu erwartenden Fehler.',
  '',
  'Durch die Bedingung in Nr. werden nur Projekte in Betracht gezogen, die bereits in der wissenschaftlichen Literatur berücksichtigt wurden und daher (zumindest für den jeweiligen Autor der Quelle) geeignet für eine Analyse der Dokumentation sind. Mit der Bedingung in Nr. wird eine Verzerrung zugunsten oder zuungunsten des  {DocEvaluators} vermieden, denn durch Parsing-Fehler können Komponenten falsch analysiert werden, die von den anderen Tools richtig analysiert werden. Indirekt wird dadurch gefordert, dass ein Projekt keine neueren Java-Funktionen verwendet, da  der nur Java bis Version 8 unterstützt. Die letzte Bedingung ist besonders für die Bewertung der Geschwindigkeit relevant, da so geprüft werden kann, ob der ',
  ''
] [ 3, 0, 0, 1 ]
CURR LINE Die folgende Auflistung zeigt die gewählten Projekte für die Evaluation. Die Quellen verweisen auf das Verzeichnis in GitHub, das von den drei Tools analysiert werden soll.  In runden Klammern dahinter befinden sich die  (auf Zehntausenderstelle gerundeten) \ac{LOC}.
CURR LINE \begin{itemize}
found begin
CURR LINE \item Log4J Version 1 \cite{log4j} (20~000)
found end
CURR LINE \item ArgoUML \cite{argouml} (17~000)
found end
CURR LINE \item Eclipse \ac{JDT} \cite{eclipsejdt} (400~000)
found end
CURR LINE \end{itemize}
found end
CURR LINE 
CURR LINE ArgoUML und Eclipse wurden in \cite[S.~74] {AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner} bewertet, wobei hier nur Eclipse \ac{JDT} betrachtet werden soll, da das gesamte Eclipse-Projekt zu umfassend ist. Log4J wurde in \cite[S.~267] {@tComment:TestingJavadocCommentstoDetectComment-CodeInconsistencies} betrachtet.
----
ArgoUML und Eclipse wurden in \cite[S.~74] AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner bewertet, wobei hier nur Eclipse \ac{JDT} betrachtet werden soll, da das gesamte Eclipse-Projekt zu umfassend ist. Log4J wurde in \cite[S.~267] {@tComment:TestingJavadocCommentstoDetectComment-CodeInconsistencies} betrachtet.
ArgoUML und Eclipse wurden in betrachtet werden soll, da das gesamte Eclipse-Projekt zu umfassend ist. Log4J wurde in betrachtet.
----
CURR LINE 
CURR LINE Bei allen Projekten traten zunächst Parsing-Fehler auf. Dies lag allerdings in den meisten Fällen daran, dass auskommentierte Methoden noch ein Javadoc-Präfix hatten. Dies kann vom \doceval nicht korrekt verarbeitet werden. Da diese Methoden offensichtlich nicht verwendet werden und keinen Einfluss auf die Qualität der Dokumentation haben können, wurden sie ersatzlos entfernt. Bei einem Fehler in Eclipse \ac{JDT} konnte der \doceval eine Datei mit mehr als 3000 Codezeilen nicht verarbeiten. Diese Datei wurde bei der Evaluation von allen Programmen ignoriert.
CURR LINE 
CURR LINE \section{Analyse der Qualität}\label{chapter:quality}
[
  'Analysierte Projekte',
  'Die folgende Auflistung zeigt die gewählten Projekte für die Evaluation. Die Quellen verweisen auf das Verzeichnis in GitHub, das von den drei Tools analysiert werden soll.  In runden Klammern dahinter befinden sich die  (auf Zehntausenderstelle gerundeten) ',
  '',
  'ArgoUML und Eclipse wurden in betrachtet werden soll, da das gesamte Eclipse-Projekt zu umfassend ist. Log4J wurde in betrachtet.',
  '',
  'Bei allen Projekten traten zunächst Parsing-Fehler auf. Dies lag allerdings in den meisten Fällen daran, dass auskommentierte Methoden noch ein Javadoc-Präfix hatten. Dies kann vom ',
  ''
] [ 3, 1, 0, 0 ]
CURR LINE Durch die Evaluation der Qualität soll geprüft werden, ob der \doceval trotz des in Kapitel \ref{chapter_conception}
CURR LINE beschriebenen abstrakten Formates eine Java-Datei richtig parsen kann und alle für die Dokumentation relevanten Informationen korrekt extrahieren kann. Zur Durchführung der Evaluation muss zunächst definiert werden, welche Funktionen der einzelnen Programme miteinander verglichen werden können, da die Programme unterschiedliche Aspekte der Dokumentation überprüfen und die Darstellung der Ergebnisse im Vergleich zum \textit{DocEvaluator} abweicht.
CURR LINE 
CURR LINE \checkpmd können als fehlersuchend bezeichnet werden. Sie prüfen die einzelnen Komponenten eines Programms und finden Abweichungen von vorher definierten Regeln. Eine solche Regel kann beispielsweise sein, dass jede öffentliche Methode dokumentiert sein muss, dass bestimmte Wörter nicht verwendet werden dürfen oder dass die Syntax der Dokumentation gültig sein muss. Damit sind sie vergleichbar mit den Metriken aus den Kapiteln \ref{chapter:metrics_coverage}  und \ref{chapter:metrics_errors}, welche ebenfalls bestimmte Fehler suchen und bei einem Verstoß gegen die Regeln eine Warnmeldung ausgeben. Allerdings berechnen \checkpmd keine Metriken, sondern finden nur die besagten Verstöße gegen die definierten Regeln. Somit kann ein Entwickler sehen, dasś ein Projekt beispielsweise 100 Verstöße gegen die Dokumentationsrichtlinien hat, erfährt aber nicht, ob die Anzahl der Verstöße unter Berücksichtigung der Projektgröße schwerwiegend ist und erhält keine normierte Bewertung, die dem Entwickler bei der Beurteilung der Dokumentationsqualität hilft.
CURR LINE 
CURR LINE Im Gegensatz dazu verwendet der \doceval Metriken, die stets einen Wert von 0 bis 100 zurückgeben, sodass ein Entwickler weiß, dass ein hoher Wert für eine hohe Qualität steht. Außerdem kann der \doceval auch die Semantik des Kommentars heuristisch prüfen, um zu erfahren, ob der Kommentar verständlich ist und nicht redundant ist (vgl. Kapitel \ref{chapter:metrics_semantic}). Nichtsdestotrotz gibt der \doceval auch Warnmeldungen aus, wenn er bestimmte Komponenten schlechter bewerten muss.
CURR LINE 
CURR LINE Aus diesen Gründen wird die Evaluation nicht mit den Metriken an sich, sondern mit den Warnmeldungen der jeweiligen Tools durchgeführt. Jedes Programm wird mit einem Verweis auf das zu analysierende Projekt aufgerufen und die Ausgaben der Programme werden in separaten Dateien umgeleitet.  Diese Dateien dienen dann als Grundlage für die spätere Evaluation.
CURR LINE 
CURR LINE \subsubsection{Durchführung der Evaluation}
[
  'Analyse der Qualität',
  'Durch die Evaluation der Qualität soll geprüft werden, ob der ',
  'beschriebenen abstrakten Formates eine Java-Datei richtig parsen kann und alle für die Dokumentation relevanten Informationen korrekt extrahieren kann. Zur Durchführung der Evaluation muss zunächst definiert werden, welche Funktionen der einzelnen Programme miteinander verglichen werden können, da die Programme unterschiedliche Aspekte der Dokumentation überprüfen und die Darstellung der Ergebnisse im Vergleich zum  DocEvaluator abweicht.',
  '',
  '',
  'Im Gegensatz dazu verwendet der ',
  '',
  'Aus diesen Gründen wird die Evaluation nicht mit den Metriken an sich, sondern mit den Warnmeldungen der jeweiligen Tools durchgeführt. Jedes Programm wird mit einem Verweis auf das zu analysierende Projekt aufgerufen und die Ausgaben der Programme werden in separaten Dateien umgeleitet.  Diese Dateien dienen dann als Grundlage für die spätere Evaluation.',
  ''
] [ 3, 1, 0, 1 ]
CURR LINE Wie im vorherigen Absatz beschrieben, dienen die Logdateien der drei Programme als Datengrundlage für die Evaluation. Jede Zeile in diesen Logdateien enthält mindestens den Dateipfad des gefundenen Fehlers, die Zeilennummer des Fehlers und einen Fehlercode als Zeichenkette.  Wenn die drei Programme einen übereinstimmenden Fehler finden, sollte es in allen drei Logdateien einen Eintrag geben, bei dem der Dateipfad identisch ist, die Zeilennummer innerhalb eines gewissen Intervalls identisch ist und die der Fehlercode identisch ist. Die Zeilennummer muss nicht zwingend identisch sein, da  \checkpmd die exakte Zeile des Fehlers ausgeben, während der \doceval nur ein Intervall ausgibt, in dem der Fehler liegt. Beispielsweise wird von \checkpmd bei einem unzulässigen Wort die exakte Zeile des Wortes genannt. Da die drei Tools unterschiedliche Fehlercodes ausgeben, werden diese so kategorisiert, dass Verstöße, welche von mehr als einem Tool erkannt werden, einen eigenen Fehlercode erhalten. Alle anderen Verstöße erhalten einen allgemeinen programmspezifischen Fehlercode, der somit keine näheren Informationen über die Art des Fehlers hergibt.
CURR LINE 
CURR LINE Basierend auf diesen Vorbereitungen kann nun für jeden Fehler jedes Programm gefunden werden, das diesen Fehler entdeckt hat. Beispielsweise kann ein Fehler von allen drei Programmen, nur von \textit{Checkstyle} oder ausschließlich von \textit{PMD} und \doceval gefunden werden. Mathematisch gesehen kann die Potenzmenge der Menge \textit{\{Checkstyle, PMD, DocEvaluator\}} genommen werden, wodurch alle möglichen Kombinationen an Programmen entstehen, die einen bestimmten Fehler erkennen können. Durch Zählen der Fehler pro Programmkombination lässt sich bewerten, welche Programme besonders viele Fehler finden, die andere Programme nicht erkennen und ob alle drei Programme viele gemeinsame Fehler finden.
CURR LINE 
CURR LINE 
CURR LINE \subsubsection{Auswahl der Regeln}
[
  'Durchführung der Evaluation',
  'Wie im vorherigen Absatz beschrieben, dienen die Logdateien der drei Programme als Datengrundlage für die Evaluation. Jede Zeile in diesen Logdateien enthält mindestens den Dateipfad des gefundenen Fehlers, die Zeilennummer des Fehlers und einen Fehlercode als Zeichenkette.  Wenn die drei Programme einen übereinstimmenden Fehler finden, sollte es in allen drei Logdateien einen Eintrag geben, bei dem der Dateipfad identisch ist, die Zeilennummer innerhalb eines gewissen Intervalls identisch ist und die der Fehlercode identisch ist. Die Zeilennummer muss nicht zwingend identisch sein, da  ',
  '',
  'Basierend auf diesen Vorbereitungen kann nun für jeden Fehler jedes Programm gefunden werden, das diesen Fehler entdeckt hat. Beispielsweise kann ein Fehler von allen drei Programmen, nur von  Checkstyle oder ausschließlich von und  genommen werden, wodurch alle möglichen Kombinationen an Programmen entstehen, die einen bestimmten Fehler erkennen können. Durch Zählen der Fehler pro Programmkombination lässt sich bewerten, welche Programme besonders viele Fehler finden, die andere Programme nicht erkennen und ob alle drei Programme viele gemeinsame Fehler finden.',
  '',
  ''
] [ 3, 1, 0, 2 ]
CURR LINE Zum Vergleich der drei Programme müssen zunächst Regeln festgelegt werden, bei denen die Programme eine Warnmeldung ausgeben. Bei \checkpmd{} erfolgt die Konfiguration über  Extensible-Markup-Language-Dateien. Beim \doceval erfolgt die Konfiguration über das in Kapitel \ref{chapter:conf} beschriebene \ac{JSON}-Format. Bei der Auswahl der Regeln muss beachtet werden, dass \checkpmd auch andere Fehler wie z.~B. komplexe Methoden finden können. Diese sind in diesem Kontext nicht relevant und werden ignoriert. Außerdem können die drei Programme zum Teil unterschiedliche Fehler finden, da beispielsweise \textit{PMD} (anders als \textit{Checkstyle}) den Inhalt der Dokumentation auf das Auftauchen bestimmter Begriffe prüfen kann.  \textit{Checkstyle} kann aber dafür (anders als \textit{PMD}) prüfen, ob die Block-Tags in der richtigen Reihenfolge definiert werden. Die letztere Regel wird vom \doceval in der ausgelieferten Fassung nicht geprüft. Alle drei Programme können jedoch prüfen, ob eine Komponente dokumentiert ist oder nicht. Tabelle \ref{tab:inters_rules} vergleicht die Überschneidungen der Regeln, welche die drei Programme anwenden können. Dabei stehen (sowohl hier als auch im Rest des Kapitels 5) die Abkürzungen \textit{CS} für \textit{Checkstyle} und \textit{DE} für \textit{DocEvaluator}.
CURR LINE 
CURR LINE \begin{table}[]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{m{4.5cm}|m{4.5cm}|m{4.5cm}}
found end
CURR LINE \textbf{CS} $\cap$ \textbf{DE}  & \textbf{PMD} $\cap$ \textbf{DE} & \textbf{PMD} $\cap$ \textbf{DE} $\cap$  \textbf{CS}  \\\hline
found end
CURR LINE \begin{itemize}
found end
CURR LINE \item Komponente dokumentiert
found end
CURR LINE \item Methode vollständig dokumentiert
found end
CURR LINE \item Fehler in Javadoc
found end
CURR LINE \end{itemize}
found end
CURR LINE &
CURR LINE \begin{itemize}
found begin
CURR LINE \item  Komponente dokumentiert
found end
CURR LINE 
found end
CURR LINE \item Bestimmte Wörter in Kommentar verbieten
found end
CURR LINE \end{itemize}
found end
CURR LINE &
CURR LINE \begin{itemize}
found begin
CURR LINE \item  Komponente dokumentiert
found end
CURR LINE 
found end
CURR LINE \end{itemize}
found end
CURR LINE \\\hline
CURR LINE \end{tabular}
CURR LINE \caption{Überschneidungen der Regeln der drei Programme}
CURR LINE \label{tab:inters_rules}
CURR LINE \end{table}
CURR LINE 
CURR LINE Aus der Tabelle lässt sich entnehmen, dass der \doceval mit \textit{Checkstyle} die meisten Übereinstimmungen hat. Beide Programme können die Vollständigkeit der Methodendokumentation und typische Fehler in Javadoc-Kommentaren (wie z.~B. fehlerhaftes HTML) finden. PMD und der \doceval können bestimmte Wörter in der Dokumentation bemängeln, die in einer Dokumentation vermieden werden sollten. Alle drei Programme können das Vorhandensein der Dokumentation überprüfen. Allerdings ignoriert \textit{PMD} alle privaten Komponenten außer Felder. Vor allem bei Checkstyle gibt es zudem viele Regeln, die von den anderen beiden Programmen nicht gefunden werden und auf der Webseite \cite{checkstyle_doc_metrics} aufgelistet werden.
----
Aus der Tabelle lässt sich entnehmen, dass der \doceval mit  Checkstyle die meisten Übereinstimmungen hat. Beide Programme können die Vollständigkeit der Methodendokumentation und typische Fehler in Javadoc-Kommentaren (wie z.~B. fehlerhaftes HTML) finden. PMD und der \doceval können bestimmte Wörter in der Dokumentation bemängeln, die in einer Dokumentation vermieden werden sollten. Alle drei Programme können das Vorhandensein der Dokumentation überprüfen. Allerdings ignoriert \textit{PMD} alle privaten Komponenten außer Felder. Vor allem bei Checkstyle gibt es zudem viele Regeln, die von den anderen beiden Programmen nicht gefunden werden und auf der Webseite \cite{checkstyle_doc_metrics} aufgelistet werden.
Aus der Tabelle lässt sich entnehmen, dass der alle privaten Komponenten außer Felder. Vor allem bei Checkstyle gibt es zudem viele Regeln, die von den anderen beiden Programmen nicht gefunden werden und auf der Webseite aufgelistet werden.
----
CURR LINE 
CURR LINE Da ein Vergleich der drei Tools somit nur eingeschränkt möglich ist, wird sich die qualitative Evaluation nur auf den Kernbereich beschränken. Es werden also nur die Regeln verwendet, die von mindestens zwei Tools unterstützt werden. Dies sind alle Regeln in Tabelle \ref{tab:inters_rules}. Da somit nur relativ leichte Fehler gefunden werden, können die Ergebnisse der Evaluation dazu verwendet werden, um die Parsing-Qualität zu ermitteln, denn wenn solche grundlegenden Fehler (wie z.~B. das Nichtvorhandensein der Dokumentation) nicht gefunden werden, besteht eine erhebliche Chance, dass eine Java-Datei falsch interpretiert wird.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \subsection{Ergebnisse}
[
  'Auswahl der Regeln',
  'Zum Vergleich der drei Programme müssen zunächst Regeln festgelegt werden, bei denen die Programme eine Warnmeldung ausgeben. Bei beschriebene Format. Bei der Auswahl der Regeln muss beachtet werden, dass (anders als  den Inhalt der Dokumentation auf das Auftauchen bestimmter Begriffe prüfen kann.  kann aber dafür (anders als  prüfen, ob die Block-Tags in der richtigen Reihenfolge definiert werden. Die letztere Regel wird vom vergleicht die Überschneidungen der Regeln, welche die drei Programme anwenden können. Dabei stehen (sowohl hier als auch im Rest des Kapitels 5) die Abkürzungen für und für ',
  '',
  '&',
  '&',
  '',
  'Aus der Tabelle lässt sich entnehmen, dass der alle privaten Komponenten außer Felder. Vor allem bei Checkstyle gibt es zudem viele Regeln, die von den anderen beiden Programmen nicht gefunden werden und auf der Webseite aufgelistet werden.',
  '',
  'Da ein Vergleich der drei Tools somit nur eingeschränkt möglich ist, wird sich die qualitative Evaluation nur auf den Kernbereich beschränken. Es werden also nur die Regeln verwendet, die von mindestens zwei Tools unterstützt werden. Dies sind alle Regeln in Tabelle ',
  '',
  '',
  ''
] [ 3, 1, 1, 0 ]
CURR LINE 
CURR LINE Tabelle \ref{tab:eval_results} listet die Anzahl der gefundenen Fehler (gemäß Tabelle \ref{tab:inters_rules}) auf. In den Spalten werden die Fehler nach dem Projekt gruppiert. Die ersten drei Zeilen beschreiben, wie viele Fehler die einzelnen Tools pro Projekt gefunden haben. So hat  der \textit{DocEvaluator} 1710 Fehler in \textit{Log4J} gefunden.  In den übrigen Zeilen wird aufgeführt, welche Kombinationen der Tools wie viele Fehler gefunden haben. Die Zeile mit der ersten Spalte \enquote{\{PMD, DE\}} beschreibt beispielsweise, dass \textit{PMD} und der  \textit{DocEvaluator} (aber nicht \textit{Checkstyle}) 41 Fehler bei \textit{Log4J}, 264 Fehler bei \textit{ArgoUML} und 253 Fehler bei \textit{Eclipse \ac{JDT}} gefunden haben.
CURR LINE \sisetup{group-minimum-digits=4,table-number-alignment =center,table-format=5.0}
CURR LINE \begin{table}[]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{c|S|S|S}
found end
CURR LINE & {Log4J} & {ArgoUML} & {Eclipse \ac{JDT}} \\ \hline
found end
CURR LINE |DE|            & 1710 & 10054  & 17380      \\ \hline
found end
CURR LINE |CS|            & 1590 & 9961   & 17638      \\ \hline
found end
CURR LINE |PMD|           & 1008 & 9051   & 12702      \\ \hline\hline
found end
CURR LINE \{DE\}          & 108   & 124     & 555         \\ \hline
found end
CURR LINE \{CS\}          & 26    & 285     & 273         \\ \hline
found end
CURR LINE \{PMD\}         & 86    & 377     & 298         \\ \hline
found end
CURR LINE \{PMD, DE\}     & 41    & 264     & 253         \\ \hline
found end
CURR LINE \{CS, DE\}      & 683   & 1266    & 5214       \\ \hline
found end
CURR LINE \{PMD, CS\}     & 3     & 10      & 793         \\ \hline
found end
CURR LINE \{PMD, CS, DE\} & 878   & 8400   & 11358      \\ \hline
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{Anzahl der gefundenen Fehler pro Projekt}
CURR LINE \label{tab:eval_results}
CURR LINE \end{table}
CURR LINE 
CURR LINE Aus der Tabelle ist ersichtlich, dass stets über 50~\% aller Fehler von allen drei Tools gefunden werden. Bei \textit{Eclipse \ac{JDT}} und \textit{Log4J} werden mehr als ein Viertel der Fehler von der Kombination  \textit{DocEvaluator} und \textit{Checkstyle} gefunden. Beim Projekt \enquote{ArgoUML} liegt diese Quote bei weniger als 15~\%.  Weniger als 10~\% der Fehler werden nur von einem Tool erkannt.
CURR LINE 
CURR LINE Die Überschneidungen der Fehler lassen sich auch mit Venn-Diagrammen darstellen.  Die Abbildungen \ref{fig:log4j_venn}, \ref{fig:argo_venn} und  \ref{fig:eclipse_venn} zeigen für jedes Projekt die Überschneidungen der gefundenen Fehler. Die Abbildung \ref{fig:legend_venn} ist die Legende dieser drei Abbildungen:
CURR LINE 
CURR LINE 
CURR LINE \begin{figure}[ht!]
[ '    \\caption{Venn-Diagramm: Log4J}' ]
0
CURR LINE \label{fig:log4j_venn}
CURR LINE \end{subfigure}
CURR LINE \hfill
CURR LINE \begin{subfigure}[b]{0.4\textwidth}
found begin
CURR LINE \centering
found end
CURR LINE \includesvg[scale=0.7,width=\textwidth]{figures/chapter5/argo.svg}
found end
CURR LINE \caption{Venn-Diagramm: ArgoUML}
found end
CURR LINE \label{fig:argo_venn}
found end
CURR LINE \end{subfigure}
found end
CURR LINE \hspace{10cm}
CURR LINE \begin{subfigure}[b]{0.4\textwidth}
found begin
CURR LINE \centering
found end
CURR LINE \includesvg[scale=0.7,width=\textwidth]{figures/chapter5/eclipse.svg}
found end
CURR LINE \caption{Venn-Diagramm: Eclipse \ac{JDT}}
found end
CURR LINE \label{fig:eclipse_venn}
found end
CURR LINE \end{subfigure}
found end
CURR LINE \hspace{3.4cm}
CURR LINE \begin{subfigure}[b]{0.25\textwidth}
found begin
CURR LINE \centering
found end
CURR LINE \includesvg[width=1\textwidth]{figures/chapter5/legende_venn.svg}
found end
CURR LINE \vspace{0.3cm}
found end
CURR LINE \caption{Legende der Venn-Diagramme}
found end
CURR LINE \label{fig:legend_venn}
found end
CURR LINE \end{subfigure}
found end
CURR LINE \end{figure}
CURR LINE 
CURR LINE Auch hier zeigt der graue Bereich visuell, dass die drei verglichenen Tools viele Fehler gemeinsam finden.  Dies ist vor allem bei \textit{ArgoUML} deutlich, da der graue Kreis fast alle anderen Kreise großflächig überdeckt. Bei den anderen beiden Projekten ist zudem eine große Überschneidung von \textit{Checkstyle} und \doceval (hellbraun) zu erkennen. Bei \textit{Log4J} zeigt das Diagramm einen im Vergleich zu den anderen Projekten größeren Bereich (dunkelblau) an Fehlern, die nur von \textit{PMD} gefunden werden. Auch der Bereich der exklusiv vom \textit{DocEvaluator} gefundenen Fehler (rot) ist bei \textit{Log4J} größer. Die nur vom \textit{DocEvaluator} und \textit{PMD} gefundenen Fehler (lila) sind bei  \textit{Eclipse \ac{JDT}} nicht zu erkennen, bei den anderen Venn-Diagrammen allerdings schon. Dafür scheint es bei  \textit{Eclipse \ac{JDT}} relativ viele Fehler zu geben, die nur von \checkpmd gefunden werden, da der hellblaue Abschnitt nur dort sichtbar ist. Außerdem gibt es bei  \textit{Eclipse \ac{JDT}} mehr Fehler, die nur von \textit{Checkstyle} gefunden werden, da der grüne Bereich größer ist.
CURR LINE 
CURR LINE Um die Trefferrate mathematisch auszudrücken, kann die Formel
CURR LINE \begin{equation}\label{eq1}
found begin
CURR LINE 1-\frac{\text{\{DE\}}}{|\text{DE}|}
found end
CURR LINE \end{equation} verwendet werden. Diese gibt in Prozent an, wie viele Fehler, die vom \doceval gefunden werden, auch von den anderen beiden Tools gefunden werden. Demgegenüber kann auch ermittelt werden, wie viele Fehler von \textit{Checkstyle} oder \textit{PMD} gefunden wurden, die auch vom \doceval erkannt wurden:
found end
CURR LINE 
CURR LINE \begin{equation}\label{eq2}
found begin
CURR LINE 1-\frac{\text{\{CS\}}+\text{\{PMD\}}+\text{\{CS,PMD\}}}{|\text{PMD}|+|\text{CS}|}
found end
CURR LINE \end{equation}
found end
CURR LINE 
CURR LINE Tabelle \ref{tab:hit_rate} zeigt basierend auf den genannten Formeln (\ref{eq1} und \ref{eq2}) die Treffergenauigkeit des \textit{DocEvaluators} für jedes analysierte Projekt:
CURR LINE \begin{table}[]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{c|c|c|c}
found end
CURR LINE Formel & Log4J & ArgoUML & Eclipse \ac{JDT} \\ \hline
found end
CURR LINE \ref{eq1} &   93,68~\% &	98,77~\% &	96,81~\% \\\hline
found end
CURR LINE \ref{eq2} & 95,57~\% &	96,47~\% &	95,50~\% \\\hline
found end
CURR LINE 
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{Trefferrate des \textit{DocEvaluators} gemäß den Formeln \ref{eq1} und \ref{eq2}}
CURR LINE \label{tab:hit_rate}
CURR LINE \end{table}
CURR LINE Es ist klar erkennbar, dass die Trefferrate unabhängig von der Formel und dem analysierten Projekt größer als 90~\% ist, sodass die meisten Fehler, die vom \doceval gefunden werden, von mindestens einem anderen Tool gefunden werden. Zudem werden die meisten Fehler, die von \textit{Checkstyle} oder \textit{PMD} gefunden werden, auch vom \doceval gefunden.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \subsection{Bewertung der Qualitätsevaluation}
[
  'Ergebnisse',
  '',
  'Tabelle  auf. In den Spalten werden die Fehler nach dem Projekt gruppiert. Die ersten drei Zeilen beschreiben, wie viele Fehler die einzelnen Tools pro Projekt gefunden haben. So hat  der  {DocEvaluator} 1710 Fehler in gefunden.  In den übrigen Zeilen wird aufgeführt, welche Kombinationen der Tools wie viele Fehler gefunden haben. Die Zeile mit der ersten Spalte  { beschreibt beispielsweise, dass und der  (aber nicht  41 Fehler bei  264 Fehler bei und 253 Fehler bei  gefunden haben.',
  '',
  'Aus der Tabelle ist ersichtlich, dass stets über 50~und werden mehr als ein Viertel der Fehler von der Kombination  und gefunden. Beim Projekt  {ArgoUML} liegt diese Quote bei weniger als 15~',
  '',
  'Die Überschneidungen der Fehler lassen sich auch mit Venn-Diagrammen darstellen.  Die Abbildungen und  zeigen für jedes Projekt die Überschneidungen der gefundenen Fehler. Die Abbildung ist die Legende dieser drei Abbildungen:',
  '',
  '',
  '',
  'Auch hier zeigt der graue Bereich visuell, dass die drei verglichenen Tools viele Fehler gemeinsam finden.  Dies ist vor allem bei  ArgoUML deutlich, da der graue Kreis fast alle anderen Kreise großflächig überdeckt. Bei den anderen beiden Projekten ist zudem eine große Überschneidung von und zeigt das Diagramm einen im Vergleich zu den anderen Projekten größeren Bereich (dunkelblau) an Fehlern, die nur von gefunden werden. Auch der Bereich der exklusiv vom gefundenen Fehler (rot) ist bei größer. Die nur vom und gefundenen Fehler (lila) sind bei   nicht zu erkennen, bei den anderen Venn-Diagrammen allerdings schon. Dafür scheint es bei   relativ viele Fehler zu geben, die nur von  mehr Fehler, die nur von gefunden werden, da der grüne Bereich größer ist.',
  '',
  'Um die Trefferrate mathematisch auszudrücken, kann die Formel',
  '',
  '',
  'Tabelle und  die Treffergenauigkeit des  {DocEvaluators} für jedes analysierte Projekt:',
  'Es ist klar erkennbar, dass die Trefferrate unabhängig von der Formel und dem analysierten Projekt größer als 90~gefunden werden, auch vom ',
  '',
  '',
  '',
  '',
  '',
  ''
] [ 3, 1, 2, 0 ]
CURR LINE Insgesamt zeigt die Evaluation der Qualität, dass der \doceval eine hohe Abdeckung mit \checkpmd hat und somit die meisten von diesen Tools gefundenen Fehler auch findet. Somit ist das abstrakte Format zur Repräsentation einer Quellcodedatei geeignet, um die meisten Aspekte, welche für die Dokumentation relevant sind, zu beschreiben. Allerdings wurde diese Evaluation auf größere Projekte (über 10~000 \ac{LOC}) beschränkt, sodass nicht geprüft wurde, ob der \doceval auch bei kleineren Projekten eine genaue Einschätzung der Dokumentationsqualität gibt. Tendenziell wird die Trefferrate kleiner sein, da durch die geringere Größe jeder nicht gefundene Fehler ein höheres Gewicht hat.
CURR LINE 
CURR LINE Während der Evaluation wurde geprüft, warum einige Fehler nicht vom \doceval gefunden wurden. In einigen Fällen waren dies einfache Fehler, die bereits behoben sind, sodass dadurch die Trefferrate erhöht wurde. In anderen Fällen gibt es größere strukturelle Probleme, die nicht mehr leicht behebbar sind. Einige dieser Fehler werden im Folgenden präsentiert:
CURR LINE \subsubsection{Fehler durch verschiedene Zeilennummerierung}
[
  'Bewertung der Qualitätsevaluation',
  'Insgesamt zeigt die Evaluation der Qualität, dass der ',
  '',
  'Während der Evaluation wurde geprüft, warum einige Fehler nicht vom '
] [ 3, 1, 2, 1 ]
CURR LINE 
CURR LINE Einige Fehler sind keine Fehler des \textit{DocEvaluators} an sich, sondern sind in der Methodik der Evaluation begründet. Um gemeinsame Fehler zu finden, müssen die gefundene Fehler pro Tool abgeglichen werden, wozu die Zeilennummer elementar ist. Allerdings gibt es Unterschiede bei der Festlegung der Zeilennummer. Der \doceval verwendet in seiner Logausgabe ein Zeilennummerintervall, der mit der Zeile des Bezeichners der Komponente endet und dessen Anfang durch Subtraktion der Anzahl der Zeilen der Dokumentation von der Zeile des Bezeichners definiert wird. Die anderen Tools verwenden die exakte Zeile eines Fehlers. In den meisten Fällen ist dies kein Problem, da diese Zeilennummer von \checkpmd innerhalb des vom \doceval beschriebenen Intervalls liegen muss. Listing \ref{lst:multiline_method} zeigt ein Beispiel, wo es problematisch wird.
CURR LINE 
CURR LINE \begin{figure}[ht!]
[ '\t\t\t[caption', '{Methode auf viele Zeilen verteilt},' ]
0
CURR LINE label={lst:multiline_method},
CURR LINE captionpos=b,language=java, basicstyle=\footnotesize, tabsize=1, showstringspaces=false,  numbers=left]
CURR LINE {figures/chapter5/multiline_method.java}
CURR LINE \end{figure}
CURR LINE Besonders an dieser Methode ist, dass die Parameter auf verschiedenen Zeilen verteilt ist. Während \textit{Checkstyle} bei einem undokumentierten Parameter die exakte Zeile eines nicht dokumentierten Parameters ausgibt (z.~B. Z. 5), würde der \doceval die Zeilen 1 bis 4 ausgeben, da die Dokumentation bei Zeile 1 beginnt und der Bezeichner der Komponente in Zeile 4 definiert ist. Ähnlich problematisch ist es, wenn zwischen der Dokumentation und dem Bezeichner noch Annotationen stehen, sodass der Bezeichner um eine Zeile nach unten rutscht.
CURR LINE 
CURR LINE \subsubsection{Klassen in Methoden}
[
  'Fehler durch verschiedene Zeilennummerierung',
  '',
  'Einige Fehler sind keine Fehler des  DocEvaluators an sich, sondern sind in der Methodik der Evaluation begründet. Um gemeinsame Fehler zu finden, müssen die gefundene Fehler pro Tool abgeglichen werden, wozu die Zeilennummer elementar ist. Allerdings gibt es Unterschiede bei der Festlegung der Zeilennummer. Der zeigt ein Beispiel, wo es problematisch wird.',
  '',
  'undefinedMethode auf viele Zeilen verteiltundefined,',
  'label=lst:multiline_method,',
  'captionpos=b,language=java, basicstyle=',
  'figures/chapter5/multiline_method.java',
  'Besonders an dieser Methode ist, dass die Parameter auf verschiedenen Zeilen verteilt ist. Während  Checkstyle bei einem undokumentierten Parameter die exakte Zeile eines nicht dokumentierten Parameters ausgibt (z.~B. Z. 5), würde der ',
  ''
] [ 3, 1, 2, 2 ]
CURR LINE 
CURR LINE In Java können Klassen in Methoden deklariert werden bzw. anonyme Klassen direkt instantiiert werden. Diese Klassen können ebenfalls Javadoc besitzen, werden allerdings vom \doceval ignoriert, da der \doceval jeglichen Code in Methoden nur unstrukturiert als Zeichenkette speichert und nicht weiterverarbeitet. Die anderen beiden Tools prüfen auch diese Klassen, sodass sie entsprechend einige Fehler finden, die der \doceval nicht mehr finden kann.
CURR LINE 
CURR LINE 
CURR LINE \subsubsection{Fehler in einzeiligen Kommentaren bei PMD}
[
  'Klassen in Methoden',
  '',
  'In Java können Klassen in Methoden deklariert werden bzw. anonyme Klassen direkt instantiiert werden. Diese Klassen können ebenfalls Javadoc besitzen, werden allerdings vom ',
  '',
  ''
] [ 3, 1, 2, 3 ]
CURR LINE Anders als der \doceval und \textit{Checkstyle} berücksichtigt \textit{PMD} auch einzeilige Kommentare. Wenn ein einzeiliger Kommentar ein unzulässiges Wort enthält, so würde \textit{PMD} einen Fehler melden, aber der \doceval nicht, da er nur Javadoc-Kommentare prüft. Dadurch kommt es zu einer Verzerrung und der Anteil der alleinig von \textit{PMD} gefundenen Fehler wird überschätzt.
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \section{Analyse der Geschwindigkeit} \label{chapter:speed}
[
  'Fehler in einzeiligen Kommentaren bei PMD',
  'Anders als der auch einzeilige Kommentare. Wenn ein einzeiliger Kommentar ein unzulässiges Wort enthält, so würde einen Fehler melden, aber der gefundenen Fehler wird überschätzt.',
  '',
  '',
  ''
] [ 3, 2, 0, 0 ]
CURR LINE In diesem Abschnitt wird der \doceval mit \checkpmd bezüglich der Geschwindigkeit verglichen. Damit soll geprüft werden, ob das Tool nicht nur eine ausreichende Qualität besitzt, sondern auch in einer angemessenen Zeit ein Ergebnis liefert. Dies ist im \ac{CI/CD}-Kontext wichtig, da bei einer langen Laufzeit des Tools  die Bereitstellung eines geprüften Softwareprojektes verzögert wird und somit die Produktivität reduziert wird.
CURR LINE 
CURR LINE \subsubsection{Durchführung der Geschwindigkeitsevaluation}
[ 'Analyse der Geschwindigkeit', 'In diesem Abschnitt wird der ', '' ] [ 3, 2, 0, 1 ]
CURR LINE Zur Durchführung der Evaluation der Geschwindigkeit analysieren die drei Tools die in Kapitel \ref{chapter:eval_projects} genannten Projekte. Damit jedes Programm fair behandelt wird und eine ungefähr gleiche Menge an Analysen durchführen kann, werden die Regeln so beschränkt, dass nur noch das Vorhandensein von Dokumentation geprüft wird. So wird verhindert, dass beispielsweise der \textit{DocEvaluator} und \textit{Checkstyle} die Dokumentation von Methodenparameter überprüfen, während \textit{PMD} dies ignoriert. Bei jeder Analyse wird die Zeit gemessen, die vom Start eines Tools bis zu dessen Beendigung vergehen.
CURR LINE 
CURR LINE Die Ausgabe jedes Tools wird auf \enquote{dev/null} umgeleitet, sodass jegliche Ausgabe ignoriert wird. Dadurch können Schwankungen unberücksichtigt bleiben, die bei der Verwendung von Eingabe- und Ausgabegeräten auftreten. So sind die Ergebnisse näher an der tatsächlichen Verarbeitungsgeschwindigkeit.  Nachteilhaft an diesem Vorgehen ist, dass die Tools im Praxiseinsatz eine Ausgabe produzieren müssen, um überhaupt dem Entwickler helfen zu können, sodass dieser wichtige Aspekt hier ignoriert wird.
CURR LINE 
CURR LINE Die Analyse jedes Projektes mit jedem Tool wird zehnmal durchgeführt, um Schwankungen durch Hintergrundprozesse oder andere Einflussfaktoren auszugleichen. Die Evaluation der Geschwindigkeit wird auf einem Laptop mit dem Prozessor \enquote{i7-1165G7} mit 16 GB Arbeitsspeicher durchgeführt. Dabei wurden alle Programme auf dem Computer geschlossen und die Berechnungen wurden ohne grafische Benutzeroberfläche durchgeführt, um Schwankungen in der Laufzeit zu minimieren.
CURR LINE \subsection{Ergebnisse}\label{chapter:eval_speed_result}
[
  'Durchführung der Geschwindigkeitsevaluation',
  'Zur Durchführung der Evaluation der Geschwindigkeit analysieren die drei Tools die in Kapitel und die Dokumentation von Methodenparameter überprüfen, während dies ignoriert. Bei jeder Analyse wird die Zeit gemessen, die vom Start eines Tools bis zu dessen Beendigung vergehen.',
  '',
  'Die Ausgabe jedes Tools wird auf  dev/null umgeleitet, sodass jegliche Ausgabe ignoriert wird. Dadurch können Schwankungen unberücksichtigt bleiben, die bei der Verwendung von Eingabe- und Ausgabegeräten auftreten. So sind die Ergebnisse näher an der tatsächlichen Verarbeitungsgeschwindigkeit.  Nachteilhaft an diesem Vorgehen ist, dass die Tools im Praxiseinsatz eine Ausgabe produzieren müssen, um überhaupt dem Entwickler helfen zu können, sodass dieser wichtige Aspekt hier ignoriert wird.',
  '',
  'Die Analyse jedes Projektes mit jedem Tool wird zehnmal durchgeführt, um Schwankungen durch Hintergrundprozesse oder andere Einflussfaktoren auszugleichen. Die Evaluation der Geschwindigkeit wird auf einem Laptop mit dem Prozessor  i7-1165G7 mit 16 GB Arbeitsspeicher durchgeführt. Dabei wurden alle Programme auf dem Computer geschlossen und die Berechnungen wurden ohne grafische Benutzeroberfläche durchgeführt, um Schwankungen in der Laufzeit zu minimieren.'
] [ 3, 2, 1, 0 ]
CURR LINE Die Tabellen \ref{tab:median_speed} und \ref{tab:std_speed} zeigen den Median und die Standardabweichung der benötigten Durchlaufzeit pro Tool und Projekt in Sekunden. Die Abbildungen \ref{fig:log4j_box}, \ref{fig:argo_box} und \ref{fig:eclipse_box} visualisieren den Inhalt der Tabellen als Boxplot.
CURR LINE 
CURR LINE Im Verzeichnis \enquote{speed\_eval} des digitalen Anhangs befinden sich die Rohdaten der Geschwindigkeitsmessung, gruppiert nach dem analysierten Projekt. Auch die Originaldateien der Boxplots befinden sich dort.
CURR LINE \sisetup{group-minimum-digits=4,table-number-alignment =center,table-format=2.3,output-decimal-marker = {,}}
CURR LINE \begin{table}[ht!]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{c|S|S|S}
found end
CURR LINE & {DE} & {CS} & {PMD}  \\\hline
found end
CURR LINE Log4J & 2.538 & 2.30 & 1.907\\\hline
found end
CURR LINE ArgoUML & 16.965 & 9.301 & 7.917 \\\hline
found end
CURR LINE Eclipse \ac{JDT} & 69.148 & 27.316 & 21.586
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{Median der Laufzeit in Sekunden}
CURR LINE \label{tab:median_speed}
CURR LINE \end{table}
CURR LINE 
CURR LINE \begin{table}[ht!]
found begin
CURR LINE \centering
found end
CURR LINE \begin{tabular}{c|S|S|S}
found end
CURR LINE & {DE} & {CS} & {PMD}  \\\hline
found end
CURR LINE Log4J & 0,077 &  0,174 &  0,068\\\hline
found end
CURR LINE ArgoUML & 0,197 &  0,083 & 0,157 \\\hline
found end
CURR LINE Eclipse \ac{JDT} & 1,594 & 0,265 & 0,270\\\hline
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{Standardabweichung der Laufzeit in Sekunden}
CURR LINE \label{tab:std_speed}
CURR LINE \end{table}
CURR LINE 
CURR LINE \begin{figure}
[ '    \\caption{Boxplot: Log4J}' ]
0
CURR LINE \label{fig:log4j_box}
CURR LINE \end{subfigure}
CURR LINE \hspace{0.01cm}
CURR LINE \begin{subfigure}[b]{0.49\textwidth}
found begin
CURR LINE \centering
found end
CURR LINE \includesvg[width=\textwidth]{figures/chapter5/argo_speed_boxplot.svg}
found end
CURR LINE \caption{Boxplot: ArgoUML}
found end
CURR LINE \label{fig:argo_box}
found end
CURR LINE \end{subfigure}
found end
CURR LINE 
CURR LINE \begin{subfigure}[b]{0.49\textwidth}
found begin
CURR LINE \centering
found end
CURR LINE \includesvg[,width=\textwidth]{figures/chapter5/eclipse_speed_boxplot.svg}
found end
CURR LINE \caption{Boxplot: Eclipse \ac{JDT} }
found end
CURR LINE \label{fig:eclipse_box}
found end
CURR LINE \end{subfigure}
found end
CURR LINE 
CURR LINE \end{figure}
CURR LINE 
CURR LINE 
CURR LINE Aus den Tabellen und Boxplot-Diagrammen wird ersichtlich, dass der \doceval im Durchschnitt länger benötigt, um die Projekte zu bewerten. Wie aus dem Boxplot-Diagramm \ref{fig:log4j_box} zu entnehmen ist, gab es nur bei Log4J einen Ausreißer, bei dem \textit{Checkstyle} mehr Zeit benötigt hat.   Ansonsten war \textit{Checkstyle} stets schneller als der \textit{DocEvaluator}. \textit{PMD} analysiert am schnellsten ein Projekt. Der Unterschied in der Laufzeit zwischen dem \doceval und den anderen Tools steigt stark  mit wachsender Größe des analysierten Projektes.  Bei dem kleinsten Projekt \textit{Log4J} benötigt der \doceval  durchschnittlich die 1,103-fache Zeit im Vergleich zu \textit{Checkstyle}. Bei dem größten Projekt \textit{Eclipse \ac{JDT}} benötigt der \doceval durchschnittlich 2,53-mal so viel Zeit wie \textit{Checkstyle}. Beim \doceval und \textit{PMD} steigt die Standardabweichung mit der Größe des Projektes, während dieser Trend bei \textit{Checkstyle} nicht so eindeutig ist.
CURR LINE 
CURR LINE Bei dem Boxplot-Diagramm \ref{fig:log4j_box} zu \textit{Log4J} ist auch erkennbar, dass Ausreißer der Laufzeit nach oben häufiger sind als nach unten, da der Median sich stets im unteren Bereich des Boxplot-Quartils befindet. Bei den anderen Projekten ist dies aufgrund des größeren Zeitabstandes  zwischen dem \doceval und den anderen Tools  und der daraus resultierenden Verkleinerung der einzelnen Boxplots  nicht so klar im Boxplot ersichtlich, allerdings stimmt diese Aussage größtenteils auch dort. Nur bei der Analyse von \textit{ArgoUML} durch den \doceval (Abbildung \ref{fig:argo_box}) scheinen Abweichungen nach unten häufiger zu sein.
CURR LINE 
CURR LINE \subsection{Bewertung der Ergebnisse}
[
  'Ergebnisse',
  'Die Tabellen zeigen den Median und die Standardabweichung der benötigten Durchlaufzeit pro Tool und Projekt in Sekunden. Die Abbildungen  und visualisieren den Inhalt der Tabellen als Boxplot.',
  '',
  'Im Verzeichnis  speed',
  '',
  '',
  '',
  '',
  '',
  '',
  'Aus den Tabellen und Boxplot-Diagrammen wird ersichtlich, dass der mehr Zeit benötigt hat.   Ansonsten war stets schneller als der  analysiert am schnellsten ein Projekt. Der Unterschied in der Laufzeit zwischen dem benötigt der  Bei dem größten Projekt  benötigt der  Beim steigt die Standardabweichung mit der Größe des Projektes, während dieser Trend bei nicht so eindeutig ist.',
  '',
  'Bei dem Boxplot-Diagramm ist auch erkennbar, dass Ausreißer der Laufzeit nach oben häufiger sind als nach unten, da der Median sich stets im unteren Bereich des Boxplot-Quartils befindet. Bei den anderen Projekten ist dies aufgrund des größeren Zeitabstandes  zwischen dem durch den  scheinen Abweichungen nach unten häufiger zu sein.',
  ''
] [ 3, 2, 2, 0 ]
CURR LINE Insgesamt zeigt die Evaluation der Geschwindigkeit, dass der \doceval langsamer arbeitet als die anderen Programmen. Allerdings  bleibt die Laufzeit auf einem angemessenen Niveau, da die Verarbeitung von dem größten Projekt \textit{Eclipse \ac{JDT}} mit 400~000 \ac{LOC} im Durchschnitt nur etwas mehr als einer Minute benötigt. Nichtsdestotrotz ignoriert diese Analyse, dass die Ausgabe von Ergebnissen über die Konsole hier nicht berücksichtigt wurde und nur eine einfache Metrik angewendet wurde. Bei einem Experiment mit aktivierter Ausgabe wurden ähnliche Ergebnisse produziert, insbesondere ist der \doceval weiterhin das langsamste Programm.
CURR LINE 
CURR LINE Für die schlechtere Laufzeit des \doceval im Vergleich zu \checkpmd lassen sich zwei Hauptargumente finden. So soll \textit{Node.Js}, welches die Plattform des Tools ist, langsamer sein als Java, in dem \checkpmd programmiert sind \cite{node_java_speed}.  Außerdem ist zu beachten, dass der \doceval Metriken berechnen soll und daher die Zwischenergebnisse aller Komponenten speichern muss, um daraus ein Gesamtergebnis mittels eines arithmetischen Mittelwerts oder eines anderen Algorithmus' berechnen zu können. Auch wenn dieses Gesamtergebnis bei der Laufzeitevaluation uninteressant ist, wird es dennoch berechnet, was zusätzliche Laufzeit benötigt. Dies erklärt auch die starke Steigerung des Zeitaufwands bei größeren Projekten.
----
Für die schlechtere Laufzeit des \doceval im Vergleich zu \checkpmd lassen sich zwei Hauptargumente finden. So soll  Node.Js, welches die Plattform des Tools ist, langsamer sein als Java, in dem \checkpmd programmiert sind \cite{node_java_speed}.  Außerdem ist zu beachten, dass der \doceval Metriken berechnen soll und daher die Zwischenergebnisse aller Komponenten speichern muss, um daraus ein Gesamtergebnis mittels eines arithmetischen Mittelwerts oder eines anderen Algorithmus' berechnen zu können. Auch wenn dieses Gesamtergebnis bei der Laufzeitevaluation uninteressant ist, wird es dennoch berechnet, was zusätzliche Laufzeit benötigt. Dies erklärt auch die starke Steigerung des Zeitaufwands bei größeren Projekten.
Für die schlechtere Laufzeit des   Außerdem ist zu beachten, dass der 
----
CURR LINE 
CURR LINE \section{Fazit der Evaluation}\label{chapter:eval_conclusion}
[
  'Bewertung der Ergebnisse',
  'Insgesamt zeigt die Evaluation der Geschwindigkeit, dass der mit 400~000 im Durchschnitt nur etwas mehr als einer Minute benötigt. Nichtsdestotrotz ignoriert diese Analyse, dass die Ausgabe von Ergebnissen über die Konsole hier nicht berücksichtigt wurde und nur eine einfache Metrik angewendet wurde. Bei einem Experiment mit aktivierter Ausgabe wurden ähnliche Ergebnisse produziert, insbesondere ist der ',
  '',
  'Für die schlechtere Laufzeit des   Außerdem ist zu beachten, dass der ',
  ''
] [ 3, 3, 0, 0 ]
CURR LINE 
CURR LINE Insgesamt zeigt die Evaluation sowohl bezüglich der Qualität als auch der Geschwindigkeit, dass der \doceval mit den anderen Tools mithalten kann. Zwar wird nicht jeder Fehler gefunden, aber die Trefferrate ist hoch und durch die Verwendung eines abstrakten Formates, das für mehrere Programmiersprachen geeignet ist, sind solche Abstriche nicht vermeidbar.
CURR LINE 
CURR LINE Auch bei der Geschwindigkeit zeigt sich, dass der \doceval zwar im Extremfall zweieinhalbmal langsamer ist als die anderen Tools, allerdings ist die Laufzeit mit knapp 70 Sekunden im Extremfall bei einem großen Projekt noch in einem (relativ) angemessenen Rahmen. Nichtsdestotrotz ist es eine Überlegung wert, den \doceval weiter zu optimieren, damit die Laufzeit verbessert wird.
CURR LINE \end{comment}
CURR LINE \begingroup
CURR LINE \renewcommand{\cleardoublepage}{} % TODO maybe removing this and next
CURR LINE \renewcommand{\clearpage}{}
CURR LINE \chapter{Conclusion}\label{chapter:conclusion}
[ 'Evaluation' ] [ 4, 0, 0, 0 ]
CURR LINE \endgroup
CURR LINE \label{sec:conclusion}
CURR LINE \input{Conclusion/conclusion}
input
masterthesis/Conclusion/conclusion
CURR LINE To conclude this master thesis, a modular approach for detecting and refactoring data clumps can have maqny advantages.  For instance, an extension to other programming languages, operating systems or build plattforms is easier so that the usability of such a tool is increased. Nevertheless, performance issues and configuration issues are some arguments against the modular method.
CURR LINE 
CURR LINE In this master thesis, only data clumps are considered and refactored. However, some developers might regard data clumps not as a serious code smell. Hence, extending the general design of the tool to other code smells would be an interesting research task as more code smells can be found and refactored without loosing the modular design. This requires for instance, that the \enquote{data clump detection} step needs to be more general (e.g. \enquote{code smell detection step}. Also, the \enquote{name finding} step might be removed or replaced with another suitable step.
CURR LINE 
CURR LINE As this master thesis, show ChatGPT can be veery helpful in some steps of the data clump detection process. However, it is not suitable for performing the whole process alone since the context size, the inherent randomness, and the costs are major limitations which cannot be ignored. Nevertheless, \ac{LLM} like ChatGPT can be a great help for developers to find and refactor code smells and their appropriate usage will be an important research topic.
CURR LINE 
CURR LINE While \ac{LLM} have been existing for some time, only the fairly recent release of ChatGPT have enabled developers around the world to use the advantages of such models in their daily developments. As a result, the impact of tools like ChatGPT on software development are part of current scientific research. This includes the need to deal with the challenges and limitations of ChatGPT while employing the undeniable potentials and powers of ChatGPT in the software development of the future.
CURR LINE \begin{comment}
found begin
CURR LINE Ziel dieser Bachelorarbeit war es, ein Tool zu Bewertung der Dokumentation zu entwickeln, das in einem \ac{CI/CD}-Prozess eingebunden werden kann und nicht auf einer Programmiersprache beschränkt ist. Dabei beschränkt sich diese Bachelorarbeit auf strukturierte Kommentaren wie z.~B. Javadoc. Dieses Ziel wurde im Großen und Ganzen erreicht.
found end
CURR LINE 
found end
CURR LINE Durch die allgemein gehaltene Klassenstruktur für das Parsing ist es möglich, Quellcode in anderen Programmiersprachen bewerten zu lassen. Allerdings muss dafür ein entsprechender Parser geschrieben werden, welcher den Quellcode in die vorgegebene Struktur transformiert. Dabei ist es natürlich nicht möglich, jedes Detail abzubilden, sondern es müssen Abstriche gemacht werden. Nichtsdestotrotz können auch sprachspezifische Eigenheiten berücksichtigt werden, indem eine entsprechende abgeleitete Klasse von \textit{ComponentMetaInformation} gebildet wird und diese sprachspezifischen Informationen dort gespeichert werden. Diese Daten können von einem geeigneten \textit{LanguageSpecificHelper} dazu verwendet werden, um sprachabhängige Details bei der Bewertung der Dokumentation zu berücksichtigen.
found end
CURR LINE 
found end
CURR LINE Auch das Parsen der strukturierten Kommentare erfolgt recht abstrakt, indem  die Informationen in den Beschreibungstexten unstrukturiert als Zeichenketten gespeichert werden, sodass die einzelnen Metriken diese Informationen weiterverarbeiten müssen. Da es Metriken gibt, die mit den einzelnen Wörtern ein eines Kommentars arbeiten und auch Metriken, welche die interne Struktur des Kommentars analysieren, wäre es ein mögliches Forschungsthema, wie diese zwei Darstellungen besser repräsentiert werden können.
found end
CURR LINE 
found end
CURR LINE Um das Tool konfigurierbar zu halten, wurde ein Konzept für eine Konfigurationsdatei im \ac{JSON}-Format vorgestellt, das alle wichtigen Informationen enthält. Die Konfiguration kann auch über GitHub Actions durchgeführt werden, indem passende Umgebungsvariablen gesetzt werden.  So kann das Tool sowohl als reguläres Programm auf einem lokalen System verwendet werden als auch mittels GitHub Actions in den \ac{CI/CD}-Prozess eingebunden werden. Durch die flexible Konfiguration kann ein Nutzer frei entscheiden, welche Metriken er für sinnvoll hält und wie er sie gewichten will. Dabei überschreibt die Konfiguration mittels GitHub Actions stets die Konfiguration in der \ac{JSON}-Datei. Außerdem können die Metriken selbst begrenzt konfiguriert werden. Eine Nutzung des Tools auf anderen \ac{CI/CD}-Plattformen, die mit GitHub Actions vergleichbar sind,  ist prinzipiell ebenfalls möglich, da die Konfiguration von dem  übrigen Programm entkoppelt ist.
found end
CURR LINE 
found end
CURR LINE Für das Tool wurden bereits einige Metriken entwickelt, welche verschiedene Bereiche der Dokumentation analysieren können. Leider war eine Evaluation der einzelnen Metriken nicht möglich, sodass weiterhin offenbleibt, welche Metriken in welchen Situationen valide Ergebnisse liefern und wie eine sinnvolle Gewichtung der Metriken aussehen kann.  Weitere Metriken lassen sich durch Einfügen einer neuen abgeleiteten Klasse von \textit{DocumentationAnalysisMetric} bilden. Schwierig kann im konkreten Einzelfall allerdings das Finden einer geeigneten Funktion werden, welche die Werte der Metrik in das vorgegebene Intervall von 0 bis 100 transformiert. Mögliche Ideen für weitere Metriken lassen sich in \cite{checkstyle_doc_metrics} finden. Auch die wissenschaftliche Literatur liefert weitere Metriken, die fortschrittliche Techniken im Bereich des \ac{NLP} nutzen und daher im Rahmen dieser Bachelorarbeit zu komplex waren. Beispielhaft sei hier das Tool \enquote{iComment} aus  \cite[S.~145ff.]{icomment} genannt, bei dem geprüft wurde, ob die Dokumentation mit dem Code auch konsistent ist und somit korrekt und aktuell ist. Diese Metriken können als Inspiration genommen werden, um die Dokumentationsqualität umfassender analysieren zu können, sodass Entwickler bei der Identifikation und Behebung von mangelhafter Softwaredokumentation  besser unterstützt werden.
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \end{comment}
found end
CURR LINE 
CURR LINE 
CURR LINE %Prints references
CURR LINE \printbibliography[title=Literature]
CURR LINE 
CURR LINE 
CURR LINE \clearpage
CURR LINE 
CURR LINE %Appendix (comes after bibliography)
CURR LINE \input{Misc/Appendix}
input
masterthesis/Misc/Appendix
CURR LINE 
CURR LINE \renewcommand\appendixpagename{Appendix}
CURR LINE \begin{appendices}
found begin
CURR LINE 
found end
CURR LINE \begin{comment}
found end
CURR LINE \chapter{Änderungen an der Parserdatei}\label{chapter:appendix_parser_changes}
found end
CURR LINE 
found end
CURR LINE \begin{table}[h!]
found end
CURR LINE \centering
found end
CURR LINE \begin{tabular}{m{0.75cm}|m{4cm}|m{10cm}}
found end
CURR LINE \textbf{Zeile} & \textbf{Änderung} & \textbf{Begründung} \\
found end
CURR LINE \hline
found end
CURR LINE 116 & Deklaration Kommentar & Hier wird ein mehrzeiliger Kommentar definiert, dies ist hier ein Alias für den Token \textit{JCOMMENT}\\
found end
CURR LINE \hline
found end
CURR LINE 127--128 & \textit{comment} als mögliches Präfix in Klassenmember & Hier wird dem Parser mitgeteilt, dass ein Bestandteil einer Klasse wie z. B. eine Methode einen Javadoc-Kommentar besitzen kann\\
found end
CURR LINE \hline
found end
CURR LINE 47 & \textit{comment} als mögliches Präfix vor Datentyp & Hier wird dem Parser mitgeteilt, dass ein Datentyp (Klasse, Schnittstelle etc. ) einen Javadoc-Kommentar haben kann \\
found end
CURR LINE \hline
found end
CURR LINE 404 & Zulassung von Javadoc in Methoden & Da Javadoc-Kommentare an beliebigen Stellen auftauchen können, auch wenn es nicht empfohlen wird und keinen Mehrwert bietet, wird hier sichergestellt, dass solche Kommentare nicht zu Warnungen oder Fehler von ANTLR4 führen. Diese Javadoc-Kommentare werden nichtsdestotrotz später ignoriert\\
found end
CURR LINE \hline
found end
CURR LINE 34, 38& Zulassung von Kommentaren vor Paketdeklarationen und Imports & Hier werden Kommentare auch vor Paketdeklarationen und Import-Statements erlaubt, was vor allem bei Klassen mit Urheberrechtsangabe sinnvoll ist\\
found end
CURR LINE \hline
found end
CURR LINE 105 & Zulassung von Kommentaren bei Enumerationen & Zwar werden Javadoc-Kommentare in Enumerationen mit diesem Tool nicht betrachtet, sie führen aber dennoch zu Warnungen und Fehlermeldungen. Daher werden sie hier zugelassen, aber später ignoriert \\
found end
CURR LINE \hline
found end
CURR LINE 82, 83 & Erzeugung eines separaten Knotens für \textit{Extends}- und \textit{Implements}-Deklarationen & In der originalen Version der Parserdatei wurde die Definition der Basisklasse bzw. der implementierten Schnittstellen direkt über die Tokens \textit{EXTENDS} bzw. \textit{IMPLEMENTS} gelöst. Dies wurde in einem neuen Knoten \textit{extendClass} bzw. \textit{implementInterfaces} ausgegliedert, um so das Parsing etwas zu vereinfachen  \\
found end
CURR LINE 
found end
CURR LINE \end{tabular}
found end
CURR LINE \caption{Änderungen an der Parserdatei}
CURR LINE \label{tab:parser_changes}
CURR LINE \end{table}
CURR LINE 
CURR LINE \chapter{UML-Diagramm: Parser}\label{appendix_parsing_uml}
[ '', '', '' ] [ 5, 0, 0, 0 ]
CURR LINE \begin{figure}[ht!]
[
  '    \\caption{UML-Diagramme aller Klassen, die relevant für das Parsen sind}'
]
0
CURR LINE \label{fig:uml_parsing}
CURR LINE \end{figure}
CURR LINE \chapter{UML-Diagramm: Metriken}\label{appendix_metrics_uml}
[ 'UML-Diagramm: Parser' ] [ 6, 0, 0, 0 ]
CURR LINE \begin{figure}[ht!]
[
  '    \\caption{UML-Diagramme aller Klassen, die relevant für die Metriken sind}'
]
0
CURR LINE \label{fig:uml_metrics}
CURR LINE \end{figure}
CURR LINE \chapter{Konfiguration des Tools}
[ 'UML-Diagramm: Metriken' ] [ 7, 0, 0, 0 ]
CURR LINE \begin{description}
found begin
CURR LINE \item[include]  Alle Dateien, die bei der Bewertung der Dokumentationsqualität berücksichtigt werden müssen
found end
CURR LINE \item[exclude]  Teilmenge von include, enthält Dateien, die nicht weiter betrachtet werden müssen
found end
CURR LINE \item[metrics]  Alle Metriken, die das Tool verwenden soll. Dies ist ein Array von Objekten mit der Struktur \enquote{(name,weight, unique\_name, params)}, wobei \textit{weight} das Gewicht der jeweiligen Metrik ist (Bei Algorithmen ohne Relevanz des Gewichts wird es ignoriert), \textit{name} der Name der Metrik und \textit{params} ein Objekt mit den Parametern der Metrik
found end
CURR LINE \item[absolute\_threshold] Mindestwert der Bewertung, die erreicht werden muss, sonst wird die Dokumentationsqualität nicht akzeptiert
found end
CURR LINE 
found end
CURR LINE \item[builder] Der Algorithmus/\textit{ResultBuilder}, der die einzelnen Ergebnisse verarbeitet.
found end
CURR LINE 
found end
CURR LINE \item[parser]  Kann verwendet, um die zu parsende Programmiersprache zu wählen. Dazu muss \textit{ParserFactory} angepasst werden
found end
CURR LINE 
found end
CURR LINE \item[path\_weights] Ein Array von Objekten der Struktur \enquote{(path,weight)}. Wird verwendet, um einzelne Pfade höher oder niedriger zu gewichtet
found end
CURR LINE 
found end
CURR LINE \item[component\_weights] Ein Array von Objekten der Struktur \enquote{(name,weight)}. Wird verwendet, um einzelne Komponenten höher oder niedriger zu gewichtet
found end
CURR LINE 
found end
CURR LINE \item[default\_path\_weight] Das Standardgewicht für eine Datei, wenn keine passende Gewichtung gefunden wurde
found end
CURR LINE 
found end
CURR LINE \item[default\_component\_weight] Das Standardgewicht einer Komponente, wenn keine passende Gewichtung gefunden wurde
found end
CURR LINE 
found end
CURR LINE \item[state\_manager] Kann verwendet werden, um festzulegen, wie das letzte Ergebnis der Dokumentationsqualität gespeichert werden soll. Weitere Möglichkeiten können durch Erweiterung der \textit{StateManagerFactory} hinzugefügt werden.
found end
CURR LINE 
found end
CURR LINE \item[relative\_threshold] Der maximale  relative Abstand zur letzten Dokumentationsqualität bevor eine Fehlermeldung geworfen wird.
found end
CURR LINE \item[builder\_params] Parameter für die \textit{MetricResultBuilder}. Diese wird aktuell nur von dem Squale-Builder (Kapitel \ref{chapter:squale}) genutzt
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE 
found end
CURR LINE \label{enum:tool_javadoc_conf}
found end
CURR LINE \end{description}
found end
CURR LINE \chapter{Implementierte Metriken}\label{appendix_metrics}
[ 'Konfiguration des Tools' ] [ 8, 0, 0, 0 ]
CURR LINE 
CURR LINE \begin{description}
found begin
CURR LINE 
found end
CURR LINE \item[Anteil dokumentierter Komponenten an allen Komponenten]
found end
CURR LINE \begin{description}
found end
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  simple\_comment
found end
CURR LINE \item [Klassenname] SimpleCommentPresentMetric
found end
CURR LINE \item[Beschreibung] Berechnet den Anteil der dokumentierten Komponenten an allen Komponenten, kann Getter und Setter ignorieren
found end
CURR LINE \item[Quellen] \cite[S. 5]{HowDocumentationEvolvesoverTime}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Anteil öffentlicher dokumentierter Komponenten]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  public\_members\_only
found end
CURR LINE \item [Klassenname] SimplePublicMembersOnlyMetric
found end
CURR LINE \item[Beschreibung] Berechnet den Anteil der öffentlichen dokumentierten Komponenten an allen öffentlichen Komponenten, kann Getter und Setter ignorieren
found end
CURR LINE \item[Quellen] \cite[S. 253]{JavadocViolationsandTheirEvolutioninOpen-SourceSoftware}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Bestrafung langer undokumentierter Methoden]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  large\_method\_commented
found end
CURR LINE \item [Klassenname] SimpleLargeMethodCommentedMetric
found end
CURR LINE \item[Beschreibung] Bestraft undokumentierte Methoden je nach ihrer Länge
found end
CURR LINE \item[Quellen] Eigene Idee
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Vollständigkeit der Dokumentation von Methoden]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  method\_fully\_documented
found end
CURR LINE \item [Klassenname] SimpleMethodDocumentationMetric
found end
CURR LINE \item[Beschreibung] Prüft, ob alle Methodenparameter und Rückgabewert dokumentiert sind
found end
CURR LINE \item[Quellen] \cite[S. 5]{HowDocumentationEvolvesoverTime}
found end
CURR LINE \end{description}
found end
CURR LINE \clearpage
CURR LINE \item[Anteil dokumentierter Methoden unter
CURR LINE Berücksichtigung der LOC]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  commented\_lines
found end
CURR LINE \item [Klassenname] CommentedLinesRatioMetric
found end
CURR LINE \item[Beschreibung]  Berechnet den Anteil der \ac{LOC} der dokumentierten Methoden an allen \ac{LOC} aller Methoden
found end
CURR LINE \item[Quellen] Eigene Idee
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Flesch-Score]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  flesch
found end
CURR LINE \item [Klassenname] FleschMetric
found end
CURR LINE \item[Beschreibung]   Berechnet den Flesch-Score des Kommentars und bewertet so, ob der Kommentar verständlich ist
found end
CURR LINE \item[Quellen] \cite[S. 72]{AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Kohärenz zwischen Kommentar und
CURR LINE Komponentenname]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  comment\_name\_coherence
found end
CURR LINE \item [Klassenname] CommentNameCoherenceMetric
found end
CURR LINE \item[Beschreibung]  Prüft, ob der Kommentar und der Name der dokumentierten Komponente sehr ähnlich sind oder keine Ähnlichkeit haben, arbeitet nur mit Methoden
found end
CURR LINE \item[Quellen] \cite[S. 86ff ]{Qualityanalysisofsourcecodecomments}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Verwendung bestimmter Wörter bestrafen]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  certain\_terms
found end
CURR LINE \item [Klassenname] CertainTermCountMetric
found end
CURR LINE \item[Beschreibung]  Bestraft das Vorkommen bestimmter Wörter (wie z.~B. Abkürzungen)
found end
CURR LINE \item[Quellen] Inspiriert von Verbot lateinischer Ausdrücke nach \cite{HowtoWriteDocCommentsfortheJavadocTool}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Bewertung der Formatierung]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  formatting\_good
found end
CURR LINE \item [Klassenname] FormattingGoodMetric
found end
CURR LINE \item[Beschreibung] Überprüft, ob korrekte Tags verwendet wurde, HTML-Tags geschlossen wurden und bei langen Methoden überhaupt eine Formatierung verwendet wurden
found end
CURR LINE \item[Quellen] Inspiriert von Regel in Checkstyle \cite{checkstyle_doc_metrics}
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \clearpage
CURR LINE \item[Rechtschreibfehler bestrafen]
CURR LINE 
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  spellling
found end
CURR LINE \item [Klassenname] SpellingMetric
found end
CURR LINE \item[Beschreibung]Sucht nach Rechtschreibfehlern und bestraft sie
found end
CURR LINE \item[Quellen] Eigene Idee
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE \item[Erwähnung von Randfällen bei Methodenparameter
CURR LINE und -rückgabewerte]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  edge\_case
found end
CURR LINE \item [Klassenname] EdgeCaseMetric
found end
CURR LINE \item[Beschreibung] Prüft, ob bei der Dokumentation von Parametern die Behandlung des Wertes \textit{null} erwähnt wird
found end
CURR LINE \item[Quellen] Inspiriert von Idee in  \cite{javadoc_coding_standards}. In \cite[S.~1ff.]{@tComment:TestingJavadocCommentstoDetectComment-CodeInconsistencies} wird ebenfalls auf einer ähnlichen Art und Weise die Erwähnung von Randfällen geprüft, dort aber auch, ob diese Angaben korrekt sind
found end
CURR LINE \end{description}
found end
CURR LINE 
CURR LINE 
CURR LINE \item[Gunning-Fog-Index]
CURR LINE \begin{description}
found begin
CURR LINE \item[]
found end
CURR LINE \item [Metrikname]  gunning\_fog
found end
CURR LINE \item [Klassenname] GunningFogMetric
found end
CURR LINE \item[Beschreibung] Berechnet den Gunning-Fog-Index des Kommentars und bewertet so, ob der Kommentar verständlich ist
found end
CURR LINE \item[Quellen] \cite[S. 71]{AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner}
found end
CURR LINE \end{description}
found end
CURR LINE \end{description}
CURR LINE 
CURR LINE \chapter{Bilder des Tools}\label{chapter:pictures_tool}
[
  'Implementierte Metriken',
  '',
  '',
  '',
  '',
  'Berücksichtigung der LOC]',
  '',
  '',
  'Komponentenname]',
  '',
  '',
  '',
  '',
  '',
  'und -rückgabewerte]',
  '',
  '',
  ''
] [ 9, 0, 0, 0 ]
CURR LINE In diesem Kapitel sind zwei Bilder des \textit{DoxEvaluators} abgedruckt, welche die zwei möglichen Ausgaben des Programms zeigen (Dokumentationsqualität ausreichend und nicht ausreichend):
CURR LINE \begin{figure}[htbp!]
[ '    \\caption{Foto vom Tool: Dokumentationsqualität ausreichend}' ]
0
CURR LINE \label{fig:passed}
CURR LINE \end{figure}
CURR LINE \begin{figure}[htbp!]
[ '    \\caption{Foto vom Tool: Dokumentationsqualität zu schlecht}' ]
0
CURR LINE \label{fig:absolute}
CURR LINE \end{figure}
CURR LINE \end{comment}
CURR LINE 
CURR LINE \end{appendices}
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE 
CURR LINE \input{Misc/Testimony}
input
masterthesis/Misc/Testimony
CURR LINE \chapter*{Erklärung zur selbstständigen Abfassung der Masterarbeit}
[ '' ] [ 5, 0, 0, 0 ]
CURR LINE 
CURR LINE Ich versichere, dass ich die eingereichte Masterarbeit selbstständig und ohne unerlaubte Hilfe verfasst habe. Anderer als der von mir angegebenen Hilfsmittel und Schriften habe ich mich nicht bedient. Alle wörtlich oder sinngemäß den Schriften anderer Autoren entnommenen Stellen habe ich kenntlich gemacht. \\
CURR LINE 
CURR LINE 
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \noindent
CURR LINE Osnabrück \today\\
CURR LINE 
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \bigskip
CURR LINE \noindent
CURR LINE Timo Schoemaker
CURR LINE \end{document}
CURR LINE 
CURR LINE 
